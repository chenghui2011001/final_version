#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage 3: MoEå¼•å…¥è®­ç»ƒ (ç¦ç”¨FiLMï¼Œå•å˜é‡éªŒè¯MoEè´¡çŒ®)

æŒ‰ç…§AETHERå·¥ç¨‹çº§ä»»åŠ¡æ‰§è¡Œæ¸…å•è¦æ±‚:
- ç›®æ ‡: éš”ç¦»è¯„ä¼°MoEå¯¹ç“¶é¢ˆè¡¨è¾¾çš„è´¡çŒ®
- æ¨¡å—: ä¿ç•™DualStream+GLAï¼›å¯ç”¨Micro-MoEï¼›ç¦ç”¨FiLMï¼›ç¦ç”¨ä¿¡é“æ¨¡æ‹Ÿ
- å•å˜é‡åŸåˆ™: é¿å…æ··æ·†MoEä¸CSI/FiLMçš„è´¡çŒ®
"""

from __future__ import annotations
import argparse
from pathlib import Path
from typing import Dict, Optional, Tuple, Any

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from tqdm.auto import tqdm

# ä½¿ç”¨ç®€åŒ–çš„æ¶æ„
import sys
import os
# Ensure final_version root is on sys.path; avoid inserting subdirs (e.g. models)
# to prevent shadowing top-level packages like 'utils'.
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from models.enhanced_aether_integration import AETHEREncoder, AETHERDecoder, create_aether_codec
from models.maybe_useless.aether_fargan_decoder import AETHERFARGANDecoder
from utils.real_data_loader import create_aether_data_loader
from training.pipeline.stages import StageConfig, get_stage_config
from training.pipeline.wave_loss import fargan_wave_losses
from training.losses import rate_loss, compute_layered_loss
from models.utils import validate_csi_config, extract_acoustic_priors
# ---- æ”¾åœ¨æ–‡ä»¶é¡¶éƒ¨åˆé€‚ä½ç½®ï¼ˆæˆ– train_one_epoch å†…éƒ¨å¼€å¤´ï¼‰----
def _sum_grad_norm(named_params, include_key=None, exclude_key=None):
    """å¯¹æŒ‡å®šå‚æ•°é›†åˆæ±‚æ¢¯åº¦èŒƒæ•°ä¹‹å’Œï¼ˆå…ˆå‡€åŒ– NaN/Infï¼‰ï¼Œè¿”å› (sum_norm, n_tensors)ã€‚"""
    total = 0.0
    n = 0
    for name, p in named_params:
        if p.grad is None:
            continue
        if include_key is not None and include_key not in name:
            continue
        if exclude_key is not None and exclude_key in name:
            continue
        g = p.grad.detach()
        # å…³é”®ï¼šå‡€åŒ– NaN/Infï¼Œé¿å…æ•´æ®µç»Ÿè®¡å˜ NaN
        g = torch.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)
        # ç”¨ float() å†å–èŒƒæ•°ï¼Œé¿å…åŠç²¾åº¦ä¸‹æº¢
        val = g.float().norm()
        # åŒé‡ä¿é™©ï¼šèŒƒæ•°æœ¬èº«è‹¥ä»éæœ‰é™ï¼Œå°±è·³è¿‡
        if torch.isfinite(val):
            total += val.item()
            n += 1
    return total, n

def _topk_grad_norm(named_params, k=5, include_key=None, exclude_key=None):
    """å¯é€‰ï¼šæ‰“å°æœ€å¤§æ¢¯åº¦èŒƒæ•°çš„è‹¥å¹²å‚æ•°ï¼Œä¾¿äºå®šä½å¼‚å¸¸ã€‚"""
    arr = []
    for name, p in named_params:
        if p.grad is None:
            continue
        if include_key is not None and include_key not in name:
            continue
        if exclude_key is not None and exclude_key in name:
            continue
        g = torch.nan_to_num(p.grad.detach(), nan=0.0, posinf=0.0, neginf=0.0)
        val = g.float().norm()
        if torch.isfinite(val):
            arr.append((val.item(), name))
    arr.sort(key=lambda x: x[0], reverse=True)
    return arr[:k]

class CharbonnierLoss(nn.Module):
    def __init__(self, eps: float = 1e-3):
        super().__init__()
        self.eps = eps
    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        # ç¡®ä¿epsä¸è¾“å…¥tensorçš„dtypeä¸€è‡´
        eps_tensor = torch.tensor(self.eps, dtype=x.dtype, device=x.device)
        return torch.mean(torch.sqrt((x - y) **2 + eps_tensor** 2))

class SimplifiedStage3Trainer:
    """Simplified Stage3 trainer focusing on MoE validation only."""
    def __init__(self, config: Dict[str, Any]):
        self.config = config

        # Stage3: å¼ºåˆ¶ç¦ç”¨FiLMï¼Œå¯ç”¨3ä¸“å®¶MoE (æ— CSIï¼Œä¸éœ€è¦LowSNRExpert)
        stage3_config = {
            "d_in": 36,
            "d_model": 128,
            "dz": 24,
            "d_csi": 10,
            "use_film": False,  # Stage3: ç¦ç”¨FiLM
            "use_moe": True,   # Stage3: å¯ç”¨MoE
            "use_quantization": False,
            "latent_bits": 4,
            "n_experts": 3,    # Stage3: åªç”¨3ä¸ªä¸“å®¶ (æ— CSIï¼Œè·³è¿‡LowSNRExpert)
            "topk": 2,         # Stage3: TOP-2è·¯ç”±
        }

        self.encoder, self.decoder = create_aether_codec(stage3_config)

    def get_moe_metrics(self) -> Dict[str, float]:
        """è·å–MoEä¸“å®¶åˆ©ç”¨ç‡æŒ‡æ ‡ã€‚"""
        metrics = {}
        if hasattr(self.encoder, 'moe') and self.encoder.moe is not None:
            try:
                expert_util = self.encoder.moe.get_expert_utilization()
                metrics['expert_usage_min'] = expert_util.min().item()
                metrics['expert_usage_max'] = expert_util.max().item()
                metrics['expert_entropy'] = -(expert_util * torch.log(expert_util + 1e-8)).sum().item()
                metrics['expert_balance'] = 1.0 - expert_util.std().item()
                # Store individual expert utilization rates
                for i, util in enumerate(expert_util):
                    metrics[f'expert_{i}_usage'] = util.item()
                # Store as formatted string for display
                usage_str = ', '.join([f'{util.item():.3f}' for util in expert_util])
                metrics['expert_usage_all'] = usage_str
            except (AttributeError, RuntimeError):
                # Fallback for MoE without utilization tracking
                metrics['expert_usage_min'] = 0.25  # Placeholder
                metrics['expert_usage_max'] = 0.25
                metrics['expert_entropy'] = 1.386  # log(4) for 4 experts
                metrics['expert_balance'] = 0.8
                metrics['expert_usage_all'] = '0.250, 0.250, 0.250, 0.250'
        return metrics


def train_one_epoch(
    encoder: nn.Module,
    decoder: nn.Module,
    loader,
    device: torch.device,
    optimizer: optim.Optimizer,
    stage_cfg: StageConfig,
    current_step: int,
    args: argparse.Namespace,
    epoch_idx: Optional[int] = None,
    scaler: Optional[torch.cuda.amp.GradScaler] = None, 
) -> Tuple[Dict[str, float], int]:
    """Train one epoch with MoE monitoring (encoder+decoder provided)."""
    encoder.train()
    decoder.train()
    use_fp16 = (scaler is not None) and scaler.is_enabled()
    epoch_metrics = {
        'total_loss': 0.0,
        'feature_loss': 0.0,
        'wave_loss': 0.0,
        'moe_loss': 0.0,
        'rate_loss': 0.0,
        'expert_entropy': 0.0,
        'expert_usage_min': 0.0,
        'expert_usage_max': 0.0,
    }
    total_samples = 0
    step = current_step

    progress = tqdm(
        enumerate(loader),
        total=len(loader),
        desc=f"Epoch {epoch_idx}/{args.epochs}" if epoch_idx is not None else "Train",
        dynamic_ncols=True,
        leave=False,
    )

    if not hasattr(train_one_epoch, '_gn_ema'):
        train_one_epoch._gn_ema = 0.0
    if not hasattr(train_one_epoch, '_wave_bp_ratio'):
        train_one_epoch._wave_bp_ratio = 0.0

    wave_char = CharbonnierLoss(eps=1e-3)  

    for batch_idx, batch in progress:

        global_step = step + batch_idx

        # Optional: token-level router warmup (use sample-level only for first N steps)
        warmup_tok = int(getattr(args, 'moe_token_warmup_steps', 0) or 0)
        if warmup_tok > 0 and hasattr(encoder, 'moe') and encoder.moe is not None:
            try:
                # CompatibleMicroMoE wrapper path
                encoder.moe.specialized_moe.use_token_level = (global_step >= warmup_tok)
            except Exception:
                # Legacy MicroMoE or other: ignore
                pass

        # Data to device
        x = batch['x'].to(device, non_blocking=True)  # Features [B, T, 36]
        y = batch['y'].to(device, non_blocking=True)  # Target features [B, T, 36]
        audio = batch['audio'].to(device, non_blocking=True)  # Target audio [B, L]

        # Stage3: ä½¿ç”¨å›ºå®šdummy CSIå‡å°‘è®¡ç®—å¼€é”€ (çœŸæ­£çš„å•å˜é‡éªŒè¯)
        batch_size = x.size(0)

        # æ¢¯åº¦ç´¯ç§¯ï¼šä»…åœ¨å¼€å§‹æ—¶æ¸…é›¶ï¼ˆåœ¨ç´¯ç§¯å¾ªç¯å†…éƒ¨å¤„ç†ï¼‰
        accum_steps = max(1, int(getattr(args, 'gradient_accumulation_steps', 1)))
        if batch_idx % accum_steps == 0:
            optimizer.zero_grad(set_to_none=True)

        # è®¡ç®—æ³¢å½¢å¯ç”¨ä¸çƒ­èº«ï¼šå…ˆä¸¥æ ¼æ‰§è¡Œ wave_start_stepï¼Œå†è¿›è¡Œ warmup æ‹‰èµ·
        start_step = int(getattr(args, 'wave_start_step', 0) or 0)
        warm_steps = int(getattr(args, 'wave_warmup_steps', 0) or 0)
        active_wave = (global_step >= start_step)
        if active_wave and warm_steps > 0:
            warm_ratio = min(1.0, max(0.0, (global_step - start_step) / float(warm_steps)))
        else:
            warm_ratio = 0.0

        # ç›®æ ‡åä¼ æ¯”ä¾‹ï¼šæ›´æ¸©å’Œçš„äºŒæ¬¡æ›²çº¿
        bp_target = warm_ratio ** 2
        # ä»…å½“æœ€è¿‘æ¢¯åº¦EMAè¾ƒä½æ—¶ï¼Œæ‰å…è®¸æ”¾å¼€åä¼ æ¯”ä¾‹ï¼ˆåªå¢ä¸å‡ï¼‰
        gn_ema = float(train_one_epoch._gn_ema)
        wave_bp_ratio = float(train_one_epoch._wave_bp_ratio)
        if gn_ema < 3.0:
            wave_bp_ratio = max(wave_bp_ratio, bp_target)
        # >>> ğŸ”§ FIX 3: ä¼˜åŒ–æ¢¯åº¦ä¼ æ’­æ§åˆ¶ï¼Œæé«˜æœ€å°ä¼ æ’­æ¯”ä¾‹ <<<
        # å¤§å¹…æé«˜æœ€å°æ¢¯åº¦ä¼ æ’­æ¯”ä¾‹ï¼Œç¡®ä¿fargan_coreèƒ½æ¥æ”¶åˆ°æœ‰æ•ˆæ¢¯åº¦ä¿¡å·
        min_bp = float(getattr(args, 'wave_min_bp', 0.5))  # ä»10%å¤§å¹…æé«˜åˆ°50%
        if active_wave:
            wave_bp_ratio = max(wave_bp_ratio, min_bp)   # ç¡®ä¿æœ€å°50%æ¢¯åº¦ä¼ æ’­

        # <<< DEBUG/SAFETY end <<<

        # ä¿®å¤æŸå¤±æƒé‡ï¼šæŒ‰ç”¨æˆ·è¦æ±‚è°ƒæ•´æƒé‡æ¯”ä¾‹ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        # feat_loss=0.1, wave_loss=0.7, moe_loss=0.2
        # æ³¨æ„ï¼šwave lossé€šå¸¸è¾ƒå¤§ï¼Œæ‰€ä»¥å®é™…æƒé‡è¦æ›´å°
        alpha_wave_eff = args.alpha_wave * 0.1  # åŸºç¡€æƒé‡ï¼š0.1ï¼Œå®é™…ä¼šåœ¨æŸå¤±ç»„åˆæ—¶ä¹˜ä»¥7
        # å†™å›çŠ¶æ€ï¼ˆæœ¬æ­¥ç”¨"æ—§EMA"ï¼Œåœ¨ç»“å°¾æ›´æ–°EMAï¼‰
        train_one_epoch._wave_bp_ratio = wave_bp_ratio

        # Forward pass (optional AMP autocast)
        # === æ­£ç¡®çš„AMPé…ç½®ï¼šå‰å‘ç”¨fp16ï¼ŒæŸå¤±è®¡ç®—ç”¨float32 ===
        amp_mode    = getattr(args, 'amp', 'none')
        # å…³é”®ä¿®å¤ï¼šAMPåœ¨éæ³¢å½¢é˜¶æ®µä¹Ÿå¯ç”¨ï¼Œåªæ˜¯ä¸è®¡ç®—æ³¢å½¢æŸå¤±
        amp_enabled = (device.type == 'cuda' and amp_mode in ('fp16', 'bf16'))
        amp_dtype   = torch.float16 if amp_mode == 'fp16' else torch.bfloat16
        # æŸå¤±è®¡ç®—å§‹ç»ˆç”¨float32ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        loss_dtype  = torch.float32

        # åˆ›å»ºå›ºå®šçš„dummy CSIï¼ˆå§‹ç»ˆç”¨float32ï¼Œé¿å…dtypeé—®é¢˜ï¼‰
        if not hasattr(train_one_epoch, '_dummy_csi_cache'):
            train_one_epoch._dummy_csi_cache = {
                'snr_db': torch.tensor(15.0, device=device, dtype=torch.float32),
                'fading_onehot': torch.zeros(8, device=device, dtype=torch.float32),
                'ber': torch.tensor(0.001, device=device, dtype=torch.float32)
            }
            train_one_epoch._dummy_csi_cache['fading_onehot'][0] = 1.0

        # å¤åˆ¶åˆ°å½“å‰batch_size
        csi_dict = {
            'snr_db': train_one_epoch._dummy_csi_cache['snr_db'].expand(batch_size),
            'fading_onehot': train_one_epoch._dummy_csi_cache['fading_onehot'].unsqueeze(0).expand(batch_size, -1),
            'ber': train_one_epoch._dummy_csi_cache['ber'].expand(batch_size)
        }

        # å‰å‘ä¼ æ’­ï¼šè¾“å…¥ä¿æŒåŸå§‹dtypeï¼Œåªåœ¨autocastå†…éƒ¨è‡ªåŠ¨è½¬æ¢
        with torch.cuda.amp.autocast(enabled=False):
            z, enc_logs = encoder(x, csi_dict=None, training_step=global_step)

        # 2) è§£ç å™¨ï¼šå¯ç”¨ AMPï¼ˆæ³¨æ„ï¼šä½ çš„ vocoder å†…éƒ¨å·²ç¦ç”¨ autocastï¼‰
        if amp_enabled:
            with torch.autocast(device_type='cuda', dtype=amp_dtype):
                feats = decoder(z, csi_dict=csi_dict, return_wave=False, target_len=None)
        else:
            feats = decoder(z, csi_dict=csi_dict, return_wave=False, target_len=None)

        # 3) ç»Ÿä¸€è½¬å› FP32ï¼Œé¿å…åç»­æ•°å€¼ä¸ç¨³
        z = z.float()
        feats = feats.float()

        # 4) enc_logs ä¸­çš„æ‰€æœ‰å¼ é‡ï¼ˆåŒ…å« dict/list/tuple åµŒå¥—ï¼‰ä¹Ÿè½¬æˆ FP32
        def _to_float32_inplace(obj):
            if isinstance(obj, torch.Tensor):
                return obj.float()
            if isinstance(obj, dict):
                for k, v in obj.items():
                    obj[k] = _to_float32_inplace(v)
                return obj
            if isinstance(obj, (list, tuple)):
                return type(obj)(_to_float32_inplace(v) for v in obj)
            return obj

        enc_logs = _to_float32_inplace(enc_logs)
        # ----------------------------------------------------------

        # ğŸ”§ FIX 1: ç¡®ä¿featsä¿æŒæ¢¯åº¦è¿æ¥ï¼Œé¿å…è®¡ç®—å›¾æ–­è£‚
        if not isinstance(feats, torch.Tensor) or not feats.requires_grad:
            # å°† feats ä¸ z çš„è®¡ç®—å›¾å»ºç«‹ä¾èµ–ï¼Œé¿å…è¢«ä¸Šæ¸¸ detach/no_grad æ–­å¼€
            feats = feats + 0.0 * z.sum()

        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿featså§‹ç»ˆä¿æŒæ¢¯åº¦
        if isinstance(feats, torch.Tensor) and not feats.requires_grad:
            feats = feats.detach().requires_grad_(True)
        # --- Wave / Vocoder branch: only after wave_start_step ---
        y_hat_audio = None  # é»˜è®¤ä¸ç®—æ³¢å½¢

        # ğŸ” è°ƒè¯•è¾“å‡ºï¼šæ£€æŸ¥vocoderè°ƒç”¨æ¡ä»¶
        wave_stride = int(getattr(args, 'wave_stride', 1) or 1)
        should_call_vocoder = active_wave and (batch_idx % wave_stride == 0)
        if batch_idx % 50 == 0:  # æ¯50ä¸ªbatchè¾“å‡ºä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
            tqdm.write(f"      ğŸ” Vocoder Debug: global_step={global_step}, active_wave={active_wave}")
            tqdm.write(f"         batch_idx={batch_idx}, wave_stride={wave_stride}, should_call={should_call_vocoder}")

        if should_call_vocoder:
            # ç°åœºæ„é€  vocoder æ¡ä»¶ï¼šfeats20 (20-d cepstrum) + period
            # å½“å‰è§£ç å™¨è¾“å‡º feats æ˜¯ FARGAN 36 ç»´ï¼šceps(18) + dnn_pitch(1, idx=18) + frame_corr(1) + lpc(16)

            # === 1) ğŸ”§ FIX 2: æ”¹è¿›teacher forcingç­–ç•¥ï¼Œç¡®ä¿æ¢¯åº¦è·¯å¾„æ¸…æ™° ===
            # é™ä½æœ€å¤§teacher forcingæ¯”ä¾‹ï¼Œå‡å°‘å¯¹GTçš„ä¾èµ–
            tf = max(0.0, min(0.3, 0.3 * (1.0 - warm_ratio)))  # ä»0.5é™ä½åˆ°0.3

            # ä½¿ç”¨detach()æ˜ç¡®æ§åˆ¶æ¢¯åº¦æµï¼šGTéƒ¨åˆ†ä¸å‚ä¸åä¼ ï¼Œé¢„æµ‹éƒ¨åˆ†ä¿æŒæ¢¯åº¦
            if tf > 0.0:
                feats_mix = tf * y.detach() + (1.0 - tf) * feats  # GTéƒ¨åˆ†æ˜¾å¼detach
            else:
                feats_mix = feats  # å®Œå…¨ä½¿ç”¨é¢„æµ‹ç‰¹å¾ï¼Œä¿æŒæ¢¯åº¦

            # â‘  å€’è°±ç»™ vocoderï¼šå–æ··åˆæµçš„å‰20ç»´ï¼Œç¡®ä¿æ¢¯åº¦è¿æ¥
            feats20 = feats_mix[..., :20].contiguous()      # [B,T,20] â† ä¿æŒæ¢¯åº¦è¿æ¥

            # â‘¡ å‘¨æœŸä¹Ÿç”¨é¢„æµ‹æµçš„ dnn_pitchï¼ˆç¬¬18ç»´ï¼‰
            pred_pitch = feats_mix[..., 18].float()         # [B,T]
            f0_hz = (2.0 ** (pred_pitch + 5.0)).clamp(50.0, 400.0)
            period = (16000.0 / f0_hz).clamp(32.0, 255.0).round().to(torch.long)


            # === 2) æ—¶é—´è½´æ¡¥æ¥ï¼šè‹¥ä¸Šæ¸¸æ˜¯ 50 Hzï¼ˆå¸¸è§ï¼šT_audioâ‰ˆ2*T_featï¼‰ï¼Œåˆ™ä¸Šé‡‡æ ·åˆ° 100 Hz ===
            T_feat = int(feats20.size(1))
            audio_frames = int(audio.size(-1) // 160)  # 100 Hz å¸§
            if audio_frames >= 2 * T_feat - 4:  # ç»éªŒé˜ˆï¼šå½“éŸ³é¢‘å¸§æ•°æ˜¾è‘—å¤§äºç‰¹å¾å¸§æ—¶ï¼Œè®¤å®šä¸º50â†’100
                feats20 = F.interpolate(feats20.permute(0, 2, 1), scale_factor=2.0,
                                        mode="linear", align_corners=False).permute(0, 2, 1).contiguous()
                period = F.interpolate(period.float().unsqueeze(1), scale_factor=2.0,
                                       mode="nearest").squeeze(1).to(torch.long).contiguous()
                T_feat = int(feats20.size(1))

            # === 3) å…ˆåœ¨â€œè¿›å…¥ FARGANCond ä¹‹å‰â€æŠŠæ—¶é—´ç»´å¯¹é½ï¼šfeatures ä¸ period å¿…é¡»åŒé•¿ ===
            T0 = min(T_feat, int(period.size(1)))
            if T0 <= 0:
                tqdm.write(f"      âŒ T0={T0} <= 0, T_feat={T_feat}, period.size(1)={period.size(1)}")
                y_hat_audio = None
            else:
                feats20 = feats20[:, :T0, :].contiguous()
                period  = period[:,  :T0   ].contiguous()

                # === 4) ä¸¥æ ¼æŒ‰ FARGANCond çš„å†…éƒ¨æ”¶ç¼©è®¡ç®—å¯ç”¨å¸§æ•° ===
                # FARGANCond.forward å†…éƒ¨ä¼šï¼š
                #   a) ä¸¢å‰ 2 å¸§ â†’ T-2
                #   b) ä¸å‘¨æœŸå¯¹é½ cat åï¼Œèµ° k=3 çš„ valid å·ç§¯ â†’ å†å‡ 2 å¸§ï¼Œæœ€ç»ˆæœ‰æ•ˆä¸º (T-4)
                # ä¸ºäº†è®© f/p åœ¨ cat å‰â€œæ—¶é—´ç»´å®Œå…¨ç›¸ç­‰â€ï¼Œæˆ‘ä»¬å¯¹å¤–éƒ¨è¾“å…¥ä¹Ÿç»Ÿä¸€ç»™â€œ_nb+4â€é•¿åº¦ï¼Œ
                # è¿™æ ·å†…éƒ¨ä¸¢ 2 å¸§åéƒ½ä¸º (_nb+2)ï¼Œå†ç» k=3 valid å·ç§¯ â†’ (_nb)ï¼ˆåˆšå¥½ç­‰äºç›®æ ‡ nbï¼‰ã€‚
                nb_pre = 2                                 # 2 å¸§é¢„çƒ­ï¼ˆä¸ Stage2 ä¸€è‡´ï¼‰
                pre = audio[..., : nb_pre * 160]           # é¢„çƒ­æ³¢å½¢
                cond_len   = max(0, T0 - 4)                # æ¡ä»¶åˆ†æ”¯æœ‰æ•ˆå¸§ï¼ˆTâ†’T-4ï¼‰
                period_len = max(0, int(period.size(1)) - 4)  # å‘¨æœŸä¹ŸæŒ‰ -4 ä¼°ç®—ï¼Œä»¥â€œç­‰é•¿ +4 è£•é‡â€ç­–ç•¥å–‚å…¥
                target_len = max(0, audio_frames - nb_pre) # å»æ‰é¢„çƒ­åçš„å¯åˆæˆå¸§
                nb_frames  = min(cond_len, period_len, target_len)

                # ä»¥ 5 å¸§ç²’åº¦å¯¹é½ï¼ˆFARGAN å†…æ ¸å¤šä»¥ 5 ä¸ºç²’åº¦ï¼‰
                nb_frames = (nb_frames // 5) * 5

                if nb_frames < 5:
                    tqdm.write(f"      âŒ nb_frames={nb_frames} < 5")
                    y_hat_audio = None
                else:
                    # â€œç­‰é•¿ +4 è£•é‡â€è£ç‰‡ï¼šä¸¤è€…éƒ½è£åˆ° nb+4ï¼Œç¡®ä¿è¿›å…¥ cond_net å‰æ—¶é—´ç»´å®Œå…¨ä¸€è‡´
                    feats20_vc = feats20[:, : nb_frames + 4, :].contiguous()
                    period_vc  = period[:,  : nb_frames + 4   ].contiguous()  # æ³¨æ„ï¼š+4ï¼ˆä¸æ˜¯ +2ï¼‰

                    def _call_vocoder_nb(_nb: int):
                        # â‘  å…ˆè£ç‰‡ï¼ˆç­‰é•¿ +4 è£•é‡ï¼‰
                        f = feats20_vc[:, : _nb + 4, :].contiguous()
                        p = period_vc[:,  : _nb + 4   ].clamp(32, 255).to(torch.long).contiguous()

                        # â‘¡ å†æ‰“å°ä¸€æ¬¡æ€§æ¡¥æ¥æ£€æŸ¥
                        if not hasattr(train_one_epoch, '_vc_checked'):
                            T_in = f.size(1)  # = _nb + 4
                            tqdm.write(f"[VocoderBridge] T_in={T_in}, will request nb={_nb}, "
                                    f"expect cond_len={T_in-4}, pre_frames={nb_pre}")
                            train_one_epoch._vc_checked = True

                        # â‘¢ ğŸ”§ FIX 5: ç»Ÿä¸€FARGANè°ƒç”¨ç­–ç•¥ï¼Œä¿æŒæ¢¯åº¦è¿æ¥
                        # ç§»é™¤ä¸å¿…è¦çš„autocastç¦ç”¨ï¼Œä¿æŒä¸å…¶ä»–éƒ¨åˆ†çš„ä¸€è‡´æ€§
                        y_audio, aux = decoder.fargan_core(f, p, _nb, pre=pre)

                        # ä¼˜åŒ–NaNå¤„ç†ï¼Œä¿æŒæ¢¯åº¦è¿æ¥è€Œéå®Œå…¨æ›¿æ¢
                        if torch.isnan(y_audio).any() or torch.isinf(y_audio).any():
                            # ä½¿ç”¨æ¡ä»¶æ›¿æ¢ä¿æŒæ¢¯åº¦ï¼Œè€Œétorch.nan_to_num
                            y_audio = torch.where(
                                torch.isnan(y_audio) | torch.isinf(y_audio),
                                torch.zeros_like(y_audio),
                                y_audio
                            )
                        return y_audio.float(), aux



                    # === 5) è‡ªé€‚åº”é‡è¯•ï¼ˆæœ€å¤š 4 æ¬¡ï¼‰ï¼šä»â€œåˆæ³• nbâ€ç›´æ¥èµ·æ­¥ï¼Œå¿…è¦æ—¶æŒ‰ 5 å¸§é€’å‡ ===
                    y_hat_audio = None
                    max_tries = 4
                    nb_try = max(5, (int(nb_frames) // 5) * 5)  # åˆæ³• nbï¼ˆå·²è€ƒè™‘ -4 æ”¶ç¼©ï¼‰

                    tqdm.write(f"      ğŸ”„ Starting vocoder retry loop with nb_try={nb_try}")
                    for try_idx in range(max_tries):
                        try:
                            if not hasattr(train_one_epoch, '_vc_checked'):
                                # è¿™é‡Œåœ¨ _call_vocoder_nb å¤–éƒ¨ï¼Œæ²¡æœ‰ _nb / f å¯ç”¨ï¼›
                                # ç›´æ¥ç”¨ feats20_vc çš„é•¿åº¦å’Œ nb_try
                                T_in = int(feats20_vc.size(1))  # é¢„æœŸ = nb_try + 4
                                tqdm.write(f"[VocoderBridge] T_in={T_in}, will request nb={nb_try}, "
                                        f"expect cond_len={T_in-4}, pre_frames={nb_pre}")
                                train_one_epoch._vc_checked = True


                            tqdm.write(f"      ğŸ¯ Calling vocoder with nb_try={nb_try}")
                            y_hat_audio, _ = _call_vocoder_nb(nb_try)
                            L = int(y_hat_audio.size(-1))
                            tqdm.write(f"      âœ… Vocoder returned audio length={L}")

                            # âœ… æ­£ç¡®çš„æœŸæœ›ï¼švocoder åªè¿”å›â€œé¢„æµ‹æ®µâ€ï¼Œä¸å« pre æ®µ
                            exp_len = max(0, (nb_try - nb_pre) * 160)

                            if L > exp_len:
                                y_hat_audio = y_hat_audio[..., :exp_len]
                            elif L < exp_len:
                                # ç”¨è¿”å›é•¿åº¦åæ¨æœ€å¯èƒ½çš„ nbï¼šframes_pred = L/160ï¼Œnb = pre + frames_pred
                                frames_pred = max(0, L // 160)
                                nb_est = frames_pred + nb_pre
                                # ä»¥ 5 å¸§ç²’åº¦å›é€€
                                nb_back = max(5, (nb_est // 5) * 5)
                                if nb_back < nb_try:
                                    nb_try = nb_back
                                    tqdm.write(f"  ğŸ” vocoder retry: nb_tryâ†’{nb_try}")
                                    y_hat_audio = None
                                    continue

                            break
                        except IndexError as ie:
                            # æç«¯è¶Šç•Œï¼ŒæŒ‰ 5 å¸§é€€
                            tqdm.write(f"      âŒ IndexError in vocoder (try {try_idx+1}/{max_tries}): {str(ie)[:100]}")
                            nb_new = max(5, nb_try - 5)
                            if nb_new >= nb_try:
                                nb_new = nb_try - 5
                            nb_try = nb_new
                            tqdm.write(f"  ğŸ” vocoder retry: nb_tryâ†’{nb_try}")
                        except Exception:
                            # å…¶å®ƒå¼‚å¸¸ä¿ç•™æŠ›å‡ºï¼Œä¾¿äºå®šä½
                            raise

                    if y_hat_audio is None:
                        tqdm.write("  âŒ VOCODER FAILURE: All retries failed, skipping wave loss for this batch.")
                        tqdm.write(f"      Final nb_try={nb_try}, nb_frames={nb_frames}, nb_pre={nb_pre}")

        # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
        if feats.size(1) != y.size(1):
            min_len = min(feats.size(1), y.size(1))
            feats = feats[:, :min_len, :]
            y = y[:, :min_len, :]


        # Stage1-like warm start: ignore first N frames for loss
        fs = max(0, int(getattr(args, 'preheat_frames', 0)))
        if fs > 0 and feats.size(1) > fs:
            feats_loss = feats[:, fs:, :]
            y_loss = y[:, fs:, :]
        else:
            feats_loss, y_loss = feats, y

        # === Fused Loss Computation - æŸå¤±è®¡ç®—ç”¨float32 ===

        # åˆå§‹åŒ–æ‰€æœ‰æŸå¤±ç»„ä»¶ï¼Œç”¨float32ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        l_feat = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_wave = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_moe  = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_rate = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_sem  = torch.tensor(0.0, device=device, dtype=loss_dtype)

        moe_metrics = {}

        # 1. Feature reconstruction lossï¼ˆè½¬æ¢ä¸ºfloat32è¿›è¡Œç¨³å®šè®¡ç®—ï¼‰
        feats_loss_safe = feats_loss.float()
        y_loss_safe     = y_loss.float()


        # æ£€æŸ¥ç‰¹å¾é‡å»ºè¾“å…¥æ˜¯å¦æœ‰å¼‚å¸¸
        if torch.isnan(feats_loss_safe).any() or torch.isinf(feats_loss_safe).any():
            tqdm.write(f"    ğŸš¨ NaN/Inf in decoded features before loss computation!")
            tqdm.write(f"      feats_loss range: [{feats_loss_safe.min().item():.6f}, {feats_loss_safe.max().item():.6f}]")
            feats_loss_safe = torch.where(torch.isnan(feats_loss_safe) | torch.isinf(feats_loss_safe),
                                        torch.zeros_like(feats_loss_safe), feats_loss_safe)

        if torch.isnan(y_loss_safe).any() or torch.isinf(y_loss_safe).any():
            tqdm.write(f"    ğŸš¨ NaN/Inf in target features before loss computation!")
            tqdm.write(f"      y_loss range: [{y_loss_safe.min().item():.6f}, {y_loss_safe.max().item():.6f}]")
            y_loss_safe = torch.where(torch.isnan(y_loss_safe) | torch.isinf(y_loss_safe),
                                    torch.zeros_like(y_loss_safe), y_loss_safe)

        l_feat = F.mse_loss(feats_loss_safe, y_loss_safe)

        # Add layered loss if enabled
        if stage_cfg.layered_enabled(global_step):
            layered_loss, _, _ = compute_layered_loss(
                feats, y,
                current_step=global_step,
                feature_spec_type='fargan'
            )
            l_feat = l_feat + layered_loss.float()

        # 2. Wave loss (ç»§æ‰¿Stage2æƒé‡è°ƒåº¦)
        if y_hat_audio is not None and wave_bp_ratio > 0.0:
            # å¯¹é½wave lossçš„å…³æ³¨åŒºåŸŸï¼šè£å‰ªæ‰å‰ fs å¸§å¯¹åº”çš„æ ·æœ¬æ•°
            wave_audio_target = audio
            wave_audio_pred = y_hat_audio
            if fs > 0:
                sample_off = fs * 160  # 16kHz, 10ms hop
                if y_hat_audio.size(-1) > sample_off:
                    wave_audio_pred = y_hat_audio[..., sample_off:]
                if audio.size(-1) > sample_off:
                    wave_audio_target = audio[..., sample_off:]

            # â‘  ä¿®å¤æ¢¯åº¦ä¼ æ’­ï¼šç§»é™¤detachæ“ä½œï¼Œé¿å…äººä¸ºå‰Šå¼±FARGANæ¢¯åº¦
            # é—®é¢˜åˆ†æï¼šdetach()å¯¼è‡´FARGANæ¢¯åº¦åªæœ‰50%ï¼Œé€ æˆæ¢¯åº¦ä¸å¹³è¡¡
            wave_audio_pred_bp = wave_audio_pred  # ç›´æ¥ä¼ æ’­ï¼Œä¸å†äººä¸ºå‰Šå¼±æ¢¯åº¦
            # â‘¡ è®¡ç®—wave lossçš„çª—å£å¤§å° - ä½¿ç”¨æ›´å¤§çš„åŸºç¡€çª—å£
            # ä¿®å¤ï¼šç¡®ä¿æœ€å°çª—å£è‡³å°‘80%ï¼Œé¿å…å› bp_ratioè¿‡å°è€Œå¯¼è‡´çª—å£å¤ªå°
            window_ratio = max(0.8, 0.6 + 0.4 * wave_bp_ratio)  # æœ€å°80%ï¼Œæœ€å¤§100%
            m = int(min(
                wave_audio_pred_bp.size(-1),
                window_ratio * wave_audio_pred_bp.size(-1)
            ))
            wave_pred_head = wave_audio_pred_bp[..., :m]
            wave_tgt_head  = wave_audio_target[..., :m]
            # â‘¢ ä½¿ç”¨ä¿®å¤åçš„FARGAN waveæŸå¤±å‡½æ•°ï¼Œç®€åŒ–å¤„ç†
            from training.pipeline.wave_loss import fargan_wave_losses
            # ç¡®ä¿periodç»´åº¦æ­£ç¡®å¯¹é½
            audio_frames = wave_pred_head.size(-1) // 160
            if period.size(1) > audio_frames:
                period_aligned = period[:, :audio_frames]
            else:
                # å¦‚æœperiodä¸å¤Ÿé•¿ï¼Œæ‰©å±•åˆ°æ‰€éœ€é•¿åº¦
                period_aligned = period.repeat(1, (audio_frames // period.size(1)) + 1)[:, :audio_frames]

            # å¼ºåˆ¶ä½¿ç”¨fp32è®¡ç®—FARGANæŸå¤±ï¼Œé¿å…bf16å¯¼è‡´çš„æ•°å€¼ä¸‹æº¢
            with torch.autocast(device_type='cuda', enabled=False):
                l_wave_raw, wave_details = fargan_wave_losses(
                    wave_pred_head.float(),
                    wave_tgt_head.float(),
                    period_aligned,
                    device=device
                )
            # â‘£ æœ‰æ•ˆæƒé‡
            l_wave = l_wave_raw * alpha_wave_eff

            # RMS gating (èåˆè®¡ç®—é¿å…é‡å¤GPUæ“ä½œ)
            pred_rms = torch.sqrt(torch.mean(wave_audio_pred_bp.float().pow(2), dim=-1) + 1e-8)
            pred_rms_db = 20.0 * torch.log10(pred_rms.mean() + 1e-8)
            gating_threshold = -35.0 + 7.0 * min(1.0, global_step / 1000.0)
            # if pred_rms_db < gating_threshold or global_step < 400:
            #     l_wave = l_wave * 0.3

        # 3. Rate loss (ä»…åœ¨éœ€è¦ç ç‡æ§åˆ¶æ—¶å¯ç”¨)
        l_rate = torch.tensor(0.0, device=device, dtype=loss_dtype)
        if args.enable_rate:
            l_rate = torch.tensor(0.0, device=device, dtype=loss_dtype)

            # Stage3æ— ç ç‡ç›®æ ‡æ—¶ï¼Œrate_lossä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸

        # 4. Semantic proxy loss (encoder semantic head vs acoustic priors)
        if isinstance(enc_logs, dict) and 'semantic_pred' in enc_logs and args.alpha_sem > 0:
            sem_pred = enc_logs['semantic_pred']  # [B,T,6]
            sem_pred_avg = sem_pred.mean(dim=1)   # [B,6]
            # ç¡®ä¿extract_acoustic_priorsçš„è¾“å…¥æ˜¯float32
            sem_target = extract_acoustic_priors(y.float())
            l_sem = args.alpha_sem * F.mse_loss(sem_pred_avg.float(), sem_target.float())


        # 5. MoE auxiliary losses â€”â€” ç›´æ¥è¿›å›¾ï¼Œå‚ä¸åä¼ 
        l_moe = torch.tensor(0.0, device=device, dtype=loss_dtype)
        moe_metrics = {}

        if isinstance(enc_logs, dict):
            # è®°å½•åˆ©ç”¨ç‡ï¼ˆåªåœ¨éœ€è¦æ—¶ï¼Œæ— æ¢¯åº¦ï¼‰
            util_interval = max(50, int(getattr(args, 'log_interval', 50)))
            if hasattr(encoder, 'moe') and encoder.moe is not None and (batch_idx == 0 or batch_idx % util_interval == 0):
                with torch.no_grad():
                    try:
                        expert_util = encoder.moe.get_expert_utilization()
                        moe_metrics['expert_usage_min'] = expert_util.min().item()
                        moe_metrics['expert_usage_max'] = expert_util.max().item()
                        moe_metrics['expert_entropy']   = -(expert_util * torch.log(expert_util + 1e-8)).sum().item()
                        moe_metrics['expert_usage_all'] = ', '.join([f'{u.item():.3f}' for u in expert_util])
                    except Exception:
                        pass

            # **å…³é”®ï¼šæŒ‰ CLI æƒé‡å¹¶å…¥ lossï¼ˆä¿æŒ float32ï¼Œå‚ä¸åä¼ ï¼‰**
            mb = enc_logs.get('moe_balance_loss', None)
            if isinstance(mb, torch.Tensor):
                l_moe = l_moe + float(getattr(args, 'moe_w', 0.05)) * mb.float()

            mt = enc_logs.get('moe_token_balance_loss', None)
            if isinstance(mt, torch.Tensor):
                l_moe = l_moe + float(getattr(args, 'moe_token_w', 0.02)) * mt.float()

            # å…¶ä»–å¯èƒ½çš„ MoE çº¦æŸï¼ˆå¦‚ä¸€è‡´æ€§ç­‰ï¼‰ï¼Œç»Ÿä¸€ç”¨è¾ƒå°æƒé‡
            for k, v in enc_logs.items():
                if not isinstance(v, torch.Tensor) or (k in ('moe_balance_loss','moe_token_balance_loss')):
                    continue
                if k.startswith('moe_') and v.requires_grad:
                    l_moe = l_moe + 0.05 * v.float()


        # 6. Fused total loss computation with alpha_feat anneal schedule
        if args.alpha_feat_start is not None:
            start = float(args.alpha_feat_start)
            end = float(args.alpha_feat_end)
            steps = max(1, int(args.alpha_feat_steps))
            ratio = min(1.0, max(0.0, global_step / steps))
            alpha_feat_now = (1 - ratio) * start + ratio * end
        else:
            alpha_feat_now = args.alpha_feat

        # èåˆæŸå¤±è®¡ç®— - æŒ‰ç”¨æˆ·è¦æ±‚è°ƒæ•´æƒé‡ï¼šfeat=0.1, wave=0.7, moe=0.2
        # æ‰€æœ‰æŸå¤±ç»„ä»¶éƒ½æ˜¯float32ï¼Œç›´æ¥ç›¸åŠ 
        alpha_feat_eff = 0.1  # ç‰¹å¾æŸå¤±æƒé‡ï¼š0.1
        alpha_wave_final = 0.7  # æ³¢å½¢æŸå¤±æƒé‡ï¼š0.7
        alpha_moe_eff = 0.2   # MoEæŸå¤±æƒé‡ï¼š0.2
        total_loss = alpha_feat_eff * l_feat + alpha_wave_final * l_wave + l_rate + l_sem + alpha_moe_eff * l_moe


        # ğŸš¨ å¢å¼ºçš„NaNæ£€æµ‹å’Œè¯Šæ–­ - åˆ†è§£æ£€æŸ¥æ¯ä¸ªæŸå¤±åˆ†é‡
        if torch.isnan(l_feat) or torch.isinf(l_feat):
            tqdm.write(f"      âš ï¸ NaN/Inf in l_feat: {l_feat.item()}")
        if torch.isnan(l_wave) or torch.isinf(l_wave):
            tqdm.write(f"      âš ï¸ NaN/Inf in l_wave: {l_wave.item()}")
        if torch.isnan(l_moe) or torch.isinf(l_moe):
            tqdm.write(f"      âš ï¸ NaN/Inf in l_moe: {l_moe.item()}")
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            tqdm.write(f"      âš ï¸ NaN/Inf in total_loss: {total_loss.item()}")
            tqdm.write(f"      Components: feat={l_feat.item():.4f}, wave={l_wave.item():.4f}, moe={l_moe.item():.4f}")

        def check_tensor(name, tensor):
            """æ£€æŸ¥tensorä¸­çš„å¼‚å¸¸å€¼"""
            if tensor is None:
                return f"{name}: None"
            if not isinstance(tensor, torch.Tensor):
                return f"{name}: not tensor ({type(tensor)})"
            has_nan = torch.isnan(tensor).any()
            has_inf = torch.isinf(tensor).any()
            min_val = tensor.min().item() if tensor.numel() > 0 else 0
            max_val = tensor.max().item() if tensor.numel() > 0 else 0
            return f"{name}: nan={has_nan}, inf={has_inf}, range=[{min_val:.3f}, {max_val:.3f}]"

        # è¯¦ç»†çš„NaNè¯Šæ–­
        if torch.isnan(total_loss) or torch.isinf(total_loss) or (args.debug_nan and batch_idx < 10):
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                tqdm.write(f"    ğŸš¨ NaN/Inf detected in total_loss: {total_loss.item() if not torch.isnan(total_loss) else 'NaN'}")
            elif args.debug_nan:
                tqdm.write(f"    ğŸ” Debug NaN mode - batch {batch_idx} diagnostics:")

            tqdm.write(f"      Loss components:")
            tqdm.write(f"        {check_tensor('feat', l_feat)}")
            tqdm.write(f"        {check_tensor('wave', l_wave)}")
            tqdm.write(f"        {check_tensor('rate', l_rate)}")
            tqdm.write(f"        {check_tensor('sem', l_sem)}")
            tqdm.write(f"        {check_tensor('moe', l_moe)}")

            # æ£€æŸ¥è¾“å…¥æ•°æ®
            tqdm.write(f"      Input data:")
            tqdm.write(f"        {check_tensor('features_x', x)}")
            tqdm.write(f"        {check_tensor('features_y', y)}")
            tqdm.write(f"        {check_tensor('audio', audio)}")

            # æ£€æŸ¥å‰å‘è¾“å‡º
            tqdm.write(f"      Forward outputs:")
            tqdm.write(f"        {check_tensor('latent_z', z)}")
            tqdm.write(f"        {check_tensor('decoded_feats', feats)}")
            if y_hat_audio is not None:
                tqdm.write(f"        {check_tensor('decoded_audio', y_hat_audio)}")

            # æ£€æŸ¥MoEç›¸å…³ä¿¡æ¯
            if encoder.use_moe and isinstance(enc_logs, dict):
                tqdm.write(f"      MoE logs:")
                for key, value in enc_logs.items():
                    if isinstance(value, torch.Tensor):
                        tqdm.write(f"        {check_tensor(key, value)}")

            # ä»…åœ¨çœŸæ­£çš„NaNæ—¶è¿›è¡Œæ¢å¤
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                # è·³è¿‡è¿™ä¸ªbatchï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªæœ‰æ•ˆloss
                if hasattr(train_one_epoch, '_last_valid_loss'):
                    total_loss = train_one_epoch._last_valid_loss.clone().detach().requires_grad_()
                    tqdm.write(f"      Recovery: using last valid loss {total_loss.item():.6f}")
                else:
                    # ä½¿ç”¨ä¿å®ˆçš„fallback loss
                    total_loss = torch.tensor(1.0, device=device, dtype=loss_dtype, requires_grad=True)
                    tqdm.write(f"      Recovery: using fallback loss {total_loss.item():.6f}")

        if not (torch.isnan(total_loss) or torch.isinf(total_loss)):
            # ä¿å­˜æœ‰æ•ˆlossç”¨äºæ¢å¤
            train_one_epoch._last_valid_loss = total_loss.clone().detach()

        # æ¢¯åº¦ç´¯ç§¯æ”¯æŒ
        accumulation_steps = getattr(args, 'gradient_accumulation_steps', 1)
        scaled_loss = total_loss / accumulation_steps
        # ---- Quick feature-only backprop debug every 50 steps ----
        do_feat_debug = (global_step % 50 == 0)
        if do_feat_debug:
            # æš‚å­˜ wave æƒé‡
            _old_wave = l_wave
            # åªç”¨ç‰¹å¾æŸå¤±åšä¸€æ¬¡æå°æ­¥å›ä¼ ï¼ˆä¸ step ä¼˜åŒ–å™¨ï¼‰
            (alpha_feat_now * l_feat).backward(retain_graph=True)

            # æ‰“å°è‡ªæ£€æ¢¯åº¦ï¼ˆç°åœ¨åº”è¯¥é 0 äº†ï¼‰
            gn_dec_dbg, n_dec_dbg = _sum_grad_norm(decoder.named_parameters(), exclude_key='fargan_core')
            gn_enc_dbg, n_enc_dbg = _sum_grad_norm(encoder.named_parameters())
            tqdm.write(f"[FEAT-ONLY DEBUG] dec(non-vc) grad={gn_dec_dbg:.3e} over {n_dec_dbg} | enc grad={gn_enc_dbg:.3e} over {n_enc_dbg}")
        # ---- end debug ----


        # è®¡ç®—å¤æ‚åº¦ç›‘æ§ï¼ˆå¯é€‰ï¼‰
        complexity_monitor_interval = 200
        if global_step % complexity_monitor_interval == 0 and hasattr(encoder, 'moe'):
            try:
                with torch.no_grad():
                    # ç»Ÿè®¡ä¸åŒpathwayæ¨¡å¼çš„è®¡ç®—é‡
                    pathway_stats = encoder.moe.get_performance_stats() if hasattr(encoder.moe, 'get_performance_stats') else {}
                    pathway_mode = pathway_stats.get('pathway_mode', 'unknown')
                    complexity_ratio = pathway_stats.get('complexity_ratio', 1.0)

                    # è®°å½•åˆ°epoch metricsç”¨äºåç»­åˆ†æ
                    if 'pathway_complexity_samples' not in epoch_metrics:
                        epoch_metrics['pathway_complexity_samples'] = 0
                        epoch_metrics['pathway_complexity_total'] = 0.0

                    epoch_metrics['pathway_complexity_samples'] += batch_size
                    epoch_metrics['pathway_complexity_total'] += complexity_ratio * batch_size

                    # æ¯200æ­¥æŠ¥å‘Šä¸€æ¬¡å¤æ‚åº¦çŠ¶æ€
                    if global_step % (complexity_monitor_interval * 5) == 0:
                        avg_complexity = epoch_metrics['pathway_complexity_total'] / max(1, epoch_metrics['pathway_complexity_samples'])
                        tqdm.write(f"    ğŸ“Š Complexity Monitor: avg={avg_complexity:.2f}x Stage1, mode={pathway_mode}")
            except Exception as e:
                pass  # å¤æ‚åº¦ç›‘æ§å¤±è´¥ä¸å½±å“è®­ç»ƒ

        # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
        accumulation_steps = getattr(args, 'gradient_accumulation_steps', 1)
        scaled_loss = total_loss / accumulation_steps

        # åä¼ ï¼šfp16 ç”¨ GradScalerï¼Œbf16/none ç›´æ¥ backward
        if use_fp16:
            scaler.scale(scaled_loss).backward()
        else:
            scaled_loss.backward()

        # åªæœ‰åœ¨ç´¯ç§¯è¾¹ç•Œæ‰åš unscale/clip/æ—¥å¿—/step
        if (batch_idx + 1) % accumulation_steps == 0:
            # å…ˆ unscale å† clipï¼ˆfp16 ä¸“ç”¨ï¼‰
            if use_fp16:
                scaler.unscale_(optimizer)

            # ï¼ˆæŠŠä½ åŸæ¥â€œå¯¹ parametrizations ä¸ fargan_core çš„æ‰‹åŠ¨ clampâ€ç§»åˆ°è¿™é‡Œæ¥ï¼Œ
            #  ç¡®ä¿åœ¨ unscale ä¹‹åã€clip ä¹‹å‰æ‰§è¡Œï¼‰
            with torch.no_grad():
                for name, param in decoder.named_parameters():
                    if param.grad is None:
                        continue
                    if 'parametrizations' in name:
                        param.grad.clamp_(-0.1, 0.1)
                    elif 'fargan_core' in name:
                        param.grad.clamp_(-1.0, 1.0)

            # å…ˆåšä¸€æ¬¡ NaN/Inf æ¸…æ´—ï¼ˆå°±åœ°ï¼‰
            with torch.no_grad():
                for p in list(encoder.parameters()) + list(decoder.parameters()):
                    if p.grad is not None:
                        p.grad.nan_to_num_(0.0, posinf=0.0, neginf=0.0)

            # å…¨æ¨¡å‹ clipï¼ˆå»ºè®® 1.0 æ›´ç¨³ï¼‰
            total_norm = torch.nn.utils.clip_grad_norm_(
                list(encoder.parameters()) + list(decoder.parameters()), max_norm=1.0
            )

            # åœ¨ step ä¹‹å‰æ‰“å°æ¢¯åº¦ç»Ÿè®¡ï¼ˆæ­¤æ—¶å·²ç» unscale/clip å®Œæˆï¼Œæ•°å€¼å¯ä¿¡ï¼‰
            gn_vc, n_vc  = _sum_grad_norm(decoder.named_parameters(), include_key='fargan_core')
            gn_dec, n_dec = _sum_grad_norm(decoder.named_parameters(), exclude_key='fargan_core')
            gn_enc, n_enc = _sum_grad_norm(encoder.named_parameters())
            tqdm.write(f"[GRAD] fargan_core grad-norm sum={gn_vc:.3e} over {n_vc} tensors")
            tqdm.write(f"[GRAD] decoder(non-vocoder) grad-norm sum={gn_dec:.3e} over {n_dec} tensors")
            tqdm.write(f"[GRAD] encoder grad-norm sum={gn_enc:.3e} over {n_enc} tensors")

            # ä¿å­˜fargan_coreæ¢¯åº¦çŠ¶æ€ç”¨äºåç»­æ£€æŸ¥
            train_one_epoch._last_fargan_grad_norm = gn_vc

            # éæœ‰é™æ€»ä½“èŒƒæ•°ï¼šåšä¸€æ¬¡æ·±åº¦æ¸…æ´—å¹¶è·³è¿‡æœ¬æ­¥
            if not torch.isfinite(total_norm):
                tqdm.write("    ğŸš¨ Non-finite grad norm detected. Sanitizing & skipping this step.")
                with torch.no_grad():
                    for p in list(encoder.parameters()) + list(decoder.parameters()):
                        if p.grad is not None:
                            p.grad.nan_to_num_(0.0, posinf=0.0, neginf=0.0)
                optimizer.zero_grad(set_to_none=True)
            else:
                # æ­£å¸¸æ›´æ–°ï¼šfp16 ç”¨ scaler.step/updateï¼Œå…¶ä»–ç›´æ¥ step
                if use_fp16:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad(set_to_none=True)

            # æ›´æ–°æ¢¯åº¦ 
            with torch.no_grad():
                tn = float(total_norm) if torch.isfinite(total_norm) else 10.0
                beta = 0.98
                train_one_epoch._gn_ema = beta * train_one_epoch._gn_ema + (1.0 - beta) * tn



        # ä¸ºmetricsè®°å½•ä½¿ç”¨åŸå§‹lossï¼ˆæœªç¼©æ”¾ï¼‰
        loss_for_metrics = total_loss

        # Update metrics
        batch_size = x.size(0)
        total_samples += batch_size

        epoch_metrics['total_loss'] += loss_for_metrics.item() * batch_size
        epoch_metrics['feature_loss'] += l_feat.item() * batch_size
        epoch_metrics['wave_loss'] += l_wave.item() * batch_size
        epoch_metrics['moe_loss'] += l_moe.item() * batch_size
        epoch_metrics['rate_loss'] += l_rate.item() * batch_size
        if isinstance(l_sem, (int, float)):
            sem_item = l_sem
        else:
            sem_item = l_sem.item()
        epoch_metrics.setdefault('semantic_loss', 0.0)
        epoch_metrics['semantic_loss'] += sem_item * batch_size

        # MoE metrics
        for key in ['expert_entropy', 'expert_usage_min', 'expert_usage_max']:
            if key in moe_metrics:
                epoch_metrics[key] += moe_metrics[key] * batch_size

        # Individual expert metrics - åŠ¨æ€ä¸“å®¶æ•°é‡
        n_experts = getattr(encoder.moe, 'n_experts', 3) if hasattr(encoder, 'moe') else 3
        for i in range(n_experts):  # åŠ¨æ€ä¸“å®¶æ•°é‡
            expert_key = f'expert_{i}_usage'
            if expert_key in moe_metrics:
                if expert_key not in epoch_metrics:
                    epoch_metrics[expert_key] = 0.0
                epoch_metrics[expert_key] += moe_metrics[expert_key] * batch_size

        # ç›´æµé€šè·¯æ€§èƒ½ç›‘æ§
        if hasattr(encoder, 'moe') and hasattr(encoder.moe, 'get_performance_stats'):
            pathway_stats = encoder.moe.get_performance_stats()
            for key, value in pathway_stats.items():
                pathway_key = f'pathway_{key}'
                if pathway_key not in epoch_metrics:
                    epoch_metrics[pathway_key] = 0.0
                if isinstance(value, (int, float)):
                    epoch_metrics[pathway_key] += value * batch_size
                elif isinstance(value, list) and len(value) > 0:
                    # å¯¹äºåˆ—è¡¨ç±»å‹ï¼Œè®¡ç®—å¹³å‡å€¼
                    avg_value = sum(value) / len(value)
                    epoch_metrics[pathway_key] += avg_value * batch_size

            # åˆ†ç¦»æŸå¤±è®¡ç®—å’ŒEMAæ›´æ–°
            if hasattr(encoder.moe, 'get_separated_outputs'):
                direct_output, expert_output = encoder.moe.get_separated_outputs()
                if direct_output is not None and expert_output is not None:
                    # è®¡ç®—åˆ†ç¦»æŸå¤±ï¼šç›´æµvsä¸“å®¶çš„å†…åœ¨å·®å¼‚ï¼ˆL2è·ç¦»ï¼‰
                    # è¿™é‡Œæ¯”è¾ƒçš„æ˜¯ä¸¤ç§å¤„ç†æ–¹å¼çš„å·®å¼‚ï¼Œè€Œä¸æ˜¯ä¸åŸå§‹è¾“å…¥çš„é‡å»ºè¯¯å·®
                    pathway_diff = F.mse_loss(direct_output, expert_output)

                    # ä½¿ç”¨pathway_diffä½œä¸ºæ€§èƒ½æŒ‡æ ‡ï¼šå€¼è¶Šå¤§è¯´æ˜ä¸¤ç§æ–¹æ³•å·®å¼‚è¶Šå¤§
                    # ç†æƒ³æƒ…å†µä¸‹ä¸“å®¶ç³»ç»Ÿåº”è¯¥äº§ç”Ÿä¸ç›´æµä¸åŒä½†æ›´ä¼˜çš„ç‰¹å¾
                    direct_loss_proxy = pathway_diff.item()
                    expert_loss_proxy = l_feat.item()  # ä½¿ç”¨æ€»ä½“ç‰¹å¾æŸå¤±ä½œä¸ºä¸“å®¶æ€§èƒ½ä»£ç†

                    # æ›´æ–°EMA
                    encoder.moe.update_performance_ema(direct_loss_proxy, expert_loss_proxy)

                    # è®°å½•åˆ°epoch metrics
                    epoch_metrics.setdefault('pathway_diff_loss', 0.0)
                    epoch_metrics.setdefault('pathway_expert_proxy_loss', 0.0)
                    epoch_metrics['pathway_diff_loss'] += direct_loss_proxy * batch_size
                    epoch_metrics['pathway_expert_proxy_loss'] += expert_loss_proxy * batch_size

        # ğŸ”§ FIX 8: å¢å¼ºè°ƒè¯•ä¿¡æ¯ï¼Œç›‘æ§æ¢¯åº¦æµå’Œå…³é”®æŒ‡æ ‡
        # Progress logging with enhanced gradient flow monitoring
        log_interval = max(1, int(getattr(args, 'log_interval', 50)))
        if batch_idx % log_interval == 0:
            # è®°å½•å…³é”®æ¢¯åº¦æµä¿¡æ¯
            with torch.no_grad():
                # æ£€æŸ¥featsçš„æ¢¯åº¦è¿æ¥çŠ¶æ€
                feats_grad_connected = feats.requires_grad if isinstance(feats, torch.Tensor) else False
                # æ£€æŸ¥teacher forcingæ¯”ä¾‹å’Œæ¢¯åº¦ä¼ æ’­æ¯”ä¾‹
                tf_ratio = tf if 'tf' in locals() else 0.0
                # æ£€æŸ¥æ˜¯å¦è®¡ç®—äº†wave_loss
                wave_computed = y_hat_audio is not None

        if batch_idx % log_interval == 0:
            # Safe float conversions for printing
            def _sf(x):
                try:
                    return float(x)
                except Exception:
                    try:
                        return float(x.item())
                    except Exception:
                        return 0.0

            tl = _sf(loss_for_metrics)
            ff = _sf(l_feat)
            ww = _sf(l_wave)
            mm = _sf(l_moe)
            # æ–°ç‰ˆSpecializedMicroMoEçš„æŸå¤±
            mb = _sf(enc_logs.get('moe_balance_loss', 0.0) if isinstance(enc_logs, dict) else 0.0)
            ms = _sf(enc_logs.get('moe_harmonic_pref', 0.0) if isinstance(enc_logs, dict) else 0.0)  # ä½¿ç”¨å®é™…å­˜åœ¨çš„æŒ‡æ ‡
            lr = optimizer.param_groups[0].get('lr', 0.0)
            # ETA from tqdm
            remaining = progress.format_dict.get('remaining', None)
            import time as _t
            eta_str = _t.strftime('%H:%M:%S', _t.gmtime(remaining)) if remaining is not None else 'NA'
            # Progress bar postfix - æ˜¾ç¤ºå®é™…æœ‰æ„ä¹‰çš„æŒ‡æ ‡ + æ¢¯åº¦ç›‘æ§
            post = {
                'loss': f"{tl:.4f}",
                'feat': f"{ff:.4f}",
                'wave': f"{ww:.4f}",
                'lr': f"{lr:.2e}",
                'eta': eta_str,
                'bp': f"{wave_bp_ratio:.2f}",
                'warm': f"{warm_ratio:.2f}",
                # æ–°å¢æ¢¯åº¦æµç›‘æ§ä¿¡æ¯
                'tf': f"{tf_ratio:.2f}",  # teacher forcingæ¯”ä¾‹
                'fgrad': 'âœ“' if feats_grad_connected else 'âœ—',  # featsæ¢¯åº¦è¿æ¥
                'wcomp': 'âœ“' if wave_computed else 'âœ—',  # wave_lossè®¡ç®—
            }

            # åªåœ¨MoEçœŸæ­£å¯ç”¨æ—¶æ˜¾ç¤ºMoEæŒ‡æ ‡
            mm = float(l_moe)
            mb = float(enc_logs.get('moe_balance_loss', 0.0)) if isinstance(enc_logs, dict) else 0.0
            mt = float(enc_logs.get('moe_token_balance_loss', 0.0)) if isinstance(enc_logs, dict) else 0.0
            post.update({
                'moe': f"{mm:.4f}",
                'moe_b': f"{mb:.4f}",
                'moe_t': f"{mt:.4f}",
            })

            # Optional profile timings if enabled
            prof_int = int(getattr(args, 'profile_interval', 0) or 0)
            if prof_int > 0 and batch_idx % prof_int == 0:
                try:
                    post.update({
                        't_fwd': f"{(t_fwd1 - t_fwd0)*1000:.0f}ms",
                        't_feat': f"{(t_feat - t_fwd1)*1000:.0f}ms",
                        't_wave': f"{(t_wave - t_feat)*1000:.0f}ms",
                        't_back': f"{(t_back - t_wave)*1000:.0f}ms",
                    })
                except Exception:
                    pass
            progress.set_postfix(post)

            # ç¬¬ 0 ä¸ª batch æ‰“ä¸€è¡Œ MoE è‡ªæ£€ + æ¢¯åº¦æµè¯Šæ–­
            if batch_idx == 0:
                try:
                    n_exp = getattr(encoder.moe, 'n_experts', '?')
                    top_k = getattr(encoder.moe, 'top_k', '?')
                    sm = getattr(encoder.moe, 'specialized_moe', None)
                    token_level = getattr(sm, 'use_token_level', False) if sm else False
                    tqdm.write(f"    âœ… MoE on: n_experts={n_exp}, top_k={top_k}, token_level={token_level}")
                except Exception:
                    pass

                # æ¢¯åº¦æµè¯Šæ–­ä¿¡æ¯
                tqdm.write(f"    ğŸ” Gradient Flow Status:")
                tqdm.write(f"       feats gradient: {'Connected' if feats_grad_connected else 'DISCONNECTED'}")
                tqdm.write(f"       teacher forcing: {tf_ratio:.3f}")
                tqdm.write(f"       wave backprop: {wave_bp_ratio:.3f}")
                tqdm.write(f"       wave computed: {'Yes' if wave_computed else 'No'}")
                if hasattr(args, 'amp'):
                    tqdm.write(f"       AMP mode: {args.amp}")

            # Health æ‰“å°ï¼šç¬¬ 0 æ­¥ä¹Ÿæ‰“ï¼›é—´éš”=max(4*log_interval, 50)
            moe_monitor_interval = max(4 * int(getattr(args, 'log_interval', 50)), 50)
            if encoder.use_moe and (batch_idx == 0 or batch_idx % moe_monitor_interval == 0):
                # åœ¨ç»•è¿‡æ¨¡å¼ä¸‹ï¼Œæ˜¾ç¤ºç»•è¿‡çŠ¶æ€ä¿¡æ¯
                if args.emergency_bypass_moe:
                    # ç»•è¿‡æ¨¡å¼ï¼šå°è¯•æ˜¾ç¤ºæ¨¡æ‹Ÿçš„å‡åŒ€åˆ†å¸ƒç»Ÿè®¡
                    if hasattr(encoder, 'moe') and encoder.moe is not None:
                        try:
                            with torch.no_grad():
                                expert_util = encoder.moe.get_expert_utilization()
                                usage_str = ', '.join([f'{util.item():.3f}' for util in expert_util])
                                tqdm.write(f"    MoE Health (BYPASS): usage=[{usage_str}] (simulated uniform distribution)")
                        except Exception:
                            n_experts = getattr(encoder.moe, 'n_experts', 3) if hasattr(encoder, 'moe') else 3
                            bypass_usage = ', '.join(['0.333'] * n_experts)
                            tqdm.write(f"    MoE Health (BYPASS): usage=[{bypass_usage}] (routing bypassed)")
                    else:
                        tqdm.write(f"    MoE Health (BYPASS): MoE module not available")
                elif moe_metrics:
                    # æ­£å¸¸æ¨¡å¼ï¼šæ˜¾ç¤ºå®Œæ•´çš„MoEå¥åº·ä¿¡æ¯
                    expert_min = moe_metrics.get('expert_usage_min', 0)
                    expert_max = moe_metrics.get('expert_usage_max', 0)
                    expert_entropy = moe_metrics.get('expert_entropy', 0)
                    expert_usage_all = moe_metrics.get('expert_usage_all', 'N/A')

                    tqdm.write(f"    MoE Health: usage=[{expert_usage_all}] entropy={expert_entropy:.3f}")

                    # ä¸“å®¶åˆ©ç”¨ç‡è­¦å‘Šï¼ˆä»…åœ¨ä¸¥é‡ä¸å‡è¡¡æ—¶ï¼‰
                    # æ¢¯åº¦ç›‘æ§è­¦å‘Šï¼šåœ¨MoEå¥åº·æ£€æŸ¥ä¸­åŒæ—¶æ£€æŸ¥fargan_coreæ¢¯åº¦
                    if expert_min < 0.1:  # æé«˜é˜ˆå€¼ä»0.15åˆ°0.1ï¼Œå‡å°‘è­¦å‘Šé¢‘ç‡
                        tqdm.write(f"    âš ï¸  Warning: Expert usage imbalance detected (min={expert_min:.3f})")

                    # æ¢¯åº¦è­¦å‘Šï¼šæ£€æŸ¥fargan_coreæ¢¯åº¦çŠ¶æ€
                    if hasattr(train_one_epoch, '_last_fargan_grad_norm'):
                        last_fg_norm = train_one_epoch._last_fargan_grad_norm
                        if last_fg_norm < 1e-6:
                            tqdm.write(f"    ğŸ˜¨ Warning: FARGAN core gradient very low ({last_fg_norm:.2e})")
                            tqdm.write(f"       Check: feature connectivity, wave_bp_ratio, dtype consistency")
                        # é¢å¤–çš„MoEè·¯ç”±è¯Šæ–­
                        if hasattr(encoder, 'moe') and encoder.moe is not None:
                            try:
                                with torch.no_grad():
                                    # è·å–æœ€è¿‘ä¸€æ¬¡çš„è·¯ç”±logits/probsï¼ˆå¦‚æœå¯ç”¨ï¼‰
                                    if hasattr(encoder.moe, '_last_router_logits'):
                                        logits = encoder.moe._last_router_logits
                                        tqdm.write(f"        Last router logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
                                        probs = torch.softmax(logits, dim=-1)
                                        tqdm.write(f"        Last router probs mean: {probs.mean(dim=0).tolist()}")
                            except Exception as e:
                                tqdm.write(f"        MoE diagnosis failed: {e}")

                    if expert_entropy < 0.7:  # æé«˜é˜ˆå€¼ä»0.8åˆ°0.7
                        tqdm.write(f"    âš ï¸  Warning: Low expert entropy detected ({expert_entropy:.3f})")

                # ç›´æµé€šè·¯ç›‘æ§è¾“å‡º - æ¶æ„çº§ç»•è¿‡æ”¯æŒ
                if hasattr(encoder, 'moe') and hasattr(encoder.moe, 'get_performance_stats'):
                    pathway_stats = encoder.moe.get_performance_stats()
                    if pathway_stats:
                        bypass_weight = pathway_stats.get('bypass_weight', 0.0)
                        expert_weight = pathway_stats.get('expert_weight', 1.0)
                        performance_ratio = pathway_stats.get('performance_ratio', 1.0)
                        pathway_mode = pathway_stats.get('pathway_mode', 'unknown')
                        stage1_equivalent = pathway_stats.get('stage1_equivalent', False)
                        complexity_ratio = pathway_stats.get('complexity_ratio', 1.0)

                        # æ ¼å¼åŒ–æƒé‡æ˜¾ç¤º
                        weight_display = f"direct={bypass_weight:.2f}, expert={expert_weight:.2f}"
                        ratio_display = f"perf_ratio={performance_ratio:.3f}"
                        mode_display = f"mode={pathway_mode}, complexity={complexity_ratio:.1f}x"

                        tqdm.write(f"    Pathway Balance: {weight_display}, {ratio_display}")
                        tqdm.write(f"    System Mode: {mode_display}")

                        # æ¶æ„çº§ç»•è¿‡çŠ¶æ€æ˜¾ç¤º
                        if stage1_equivalent:
                            tqdm.write(f"    ğŸŸ¢ Architectural Bypass: Stage1-equivalent mode active")
                        elif pathway_mode == 'mixed':
                            tqdm.write(f"    ğŸŸ¡ Mixed Mode: Transitioning to expert system")
                        elif pathway_mode == 'pure_expert':
                            tqdm.write(f"    ğŸ”µ Pure Expert Mode: Full MoE active")

                        # æ€§èƒ½è­¦å‘Š - é’ˆå¯¹ä¸åŒæ¨¡å¼è°ƒæ•´
                        if pathway_mode == 'architectural_bypass':
                            tqdm.write(f"    âœ… Training in Stage1-equivalent mode for stability")
                        elif pathway_mode == 'mixed':
                            if performance_ratio > 1.5:
                                tqdm.write(f"    âš ï¸  Direct pathway strongly outperforming (ratio={performance_ratio:.3f})")
                            elif performance_ratio < 0.7:
                                tqdm.write(f"    âœ… Expert system outperforming direct pathway (ratio={performance_ratio:.3f})")
                        elif pathway_mode == 'pure_expert':
                            tqdm.write(f"    âœ… Expert system fully engaged")

    # Average metrics
    for key in epoch_metrics:
        epoch_metrics[key] /= max(total_samples, 1)

    return epoch_metrics, step + len(loader)


def main() -> int:
    """Stage3è®­ç»ƒä¸»å‡½æ•° - æŒ‰AETHERä»»åŠ¡æ¸…å•è¦æ±‚é…ç½®"""

    # åˆ›å»ºtqdmå…¼å®¹çš„æ‰“å°å‡½æ•°ï¼ˆé˜²æ­¢è¢«è¿›åº¦æ¡è¦†ç›–ï¼‰
    def safe_print(msg: str, flush: bool = True):
        """å®‰å…¨æ‰“å°å‡½æ•°ï¼šåœ¨è¿›åº¦æ¡å­˜åœ¨æ—¶ä½¿ç”¨tqdm.writeï¼Œå¦åˆ™ä½¿ç”¨æ™®é€šprint"""
        try:
            # å°è¯•ä½¿ç”¨tqdm.writeï¼ˆå¦‚æœtqdmæ´»è·ƒæ—¶ï¼‰
            tqdm.write(msg)
        except:
            # å›é€€åˆ°æ™®é€šprint
            print(msg)
        if flush:
            import sys
            sys.stdout.flush()
    p = argparse.ArgumentParser(description='Stage 3: MoEå¼•å…¥è®­ç»ƒ (ç¦ç”¨FiLMï¼Œå•å˜é‡éªŒè¯)')
    p.add_argument('--moe-w', type=float, default=0.05, help='sample-level MoE balance loss æƒé‡')
    p.add_argument('--moe-token-w', type=float, default=0.02, help='token-level MoE balance loss æƒé‡')
    p.add_argument('--features', type=str, required=True, help='Features file path')
    p.add_argument('--pcm', type=str, required=True, help='Audio PCM file path')
    p.add_argument('--stage1-checkpoint', type=str, default=None, help='Optional Stage 1 checkpoint for warm start (AETHER encoder/decoder)')
    p.add_argument('--fargan-checkpoint', type=str, help='Pre-trained FARGAN checkpoint (optional)')
    p.add_argument('--output-dir', type=str, default='checkpoints_stage3')
    p.add_argument('--device', type=str, default='auto')
    p.add_argument('--epochs', type=int, default=3, help='Training epochs (task: 3 epochs)')
    p.add_argument('--batch-size', type=int, default=4)
    p.add_argument('--seq-len', type=int, default=800)
    p.add_argument('--num-workers', type=int, default=4)
    p.add_argument('--feature-dims', type=int, default=36)
    p.add_argument('--alpha-feat', type=float, default=1.0, help='Feature loss base weight (if no schedule)')
    p.add_argument('--alpha-wave', type=float, default=1.0, help='Wave loss weight')
    p.add_argument('--alpha-sem', type=float, default=0.2, help='Semantic proxy loss weight (encoder semantic head vs priors)')
    p.add_argument('--amp', type=str, default='fp16', choices=['none', 'fp16', 'bf16'], help='Mixed precision mode for CUDA (fp16 recommended for stability)')
    p.add_argument('--wave-warmup-steps', type=int, default=3000, help='Linear warmup steps for wave loss weight (alpha_wave)')
    p.add_argument('--wave-start-step', type=int, default=1500, help='Do not compute vocoder/wave loss before this global step')
    p.add_argument('--preheat-frames', type=int, default=2, help='Ignore first N frames for loss (Stage1-like warm start)')
    # DataLoader knobs
    p.add_argument('--stride-frames', type=int, default=None, help='Data loader stride in frames (None=auto-adaptive)')
    p.add_argument('--semantic-source', type=str, default='fused', choices=['fused', 'ribbon', 'thread'], help='Semantic head source inside DualStream')
    # alpha_feat anneal schedule: start -> end over steps
    p.add_argument('--alpha-feat-start', type=float, default=None, help='Optional alpha_feat start value (overrides --alpha-feat)')
    p.add_argument('--alpha-feat-end', type=float, default=0.0, help='alpha_feat end value')
    p.add_argument('--alpha-feat-steps', type=int, default=1000, help='Linear anneal steps for alpha_feat')
    p.add_argument('--log-interval', type=int, default=50, help='Steps between progress updates')
    p.add_argument('--profile-interval', type=int, default=0, help='If >0, include per-step timings every N steps')
    # Checkpointing controls
    p.add_argument('--save-every-epochs', type=int, default=0, help='Save a checkpoint every N epochs (0 disables)')
    p.add_argument('--always-save-last', action='store_true', help='Always save a final checkpoint at the end')

    # Stage3ç‰¹å®šé…ç½® (æŒ‰ä»»åŠ¡è¦æ±‚)
    p.add_argument('--moe', action='store_true', default=True, help='Enable MoE (Stage3 default: enabled)')
    p.add_argument('--no-moe', action='store_true', help='ğŸš¨ Disable MoE for debugging (ä¸´æ—¶è¯Šæ–­é€‰é¡¹)')
    p.add_argument('--enable-rate', action='store_true',
                   help='Enable rate regularizer (disabled by default for Stage3 - only use with quantization/JSCC)')
    p.add_argument('--router-no-csi', action='store_true', default=True,
                   help='Routerä¸ä½¿ç”¨CSI (Stage3å•å˜é‡éªŒè¯)')
    # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
    p.add_argument('--gradient-accumulation-steps', type=int, default=1,
                   help='Number of steps to accumulate gradients before updating (for memory efficiency)')
    p.add_argument('--use-compile', action='store_true', default=False,
                   help='Use torch.compile for model optimization (PyTorch 2.0+)')
    p.add_argument('--moe-token-warmup-steps', type=int, default=0,
                   help='Use sample-level routing only for first N steps, then enable token-level')
    p.add_argument('--debug-nan', action='store_true', default=False,
                   help='Enable detailed NaN debugging output')
    p.add_argument('--safe-init', action='store_true', default=True,
                   help='Use safe initialization for numerical stability')
    p.add_argument('--emergency-bypass-moe', action='store_true', default=False,
                   help='ğŸš¨ Emergency: completely bypass MoE for NaN diagnosis')
    p.add_argument('--wave-stride', type=int, default=1,
               help='è®¡ç®— vocoder / wave loss çš„æ­¥é¢‘ï¼šæ¯ N ä¸ª batch æ‰è®¡ç®—ä¸€æ¬¡ (é»˜è®¤ 1ï¼Œå³æ¯æ­¥éƒ½ç®—)')
    p.add_argument('--wave-min-bp', type=float, default=0.1,
                help='ğŸ”§ æ³¢å½¢åˆ†æ”¯åä¼ æ¯”ä¾‹çš„ä¸‹é™ï¼Œé¿å… warmup åˆæœŸé•¿æœŸä¸º 0ï¼ˆé»˜è®¤ 0.1ï¼Œæé«˜æ¢¯åº¦ä¼ æ’­ï¼‰')
    p.add_argument('--router-jitter', type=float, default=0.01,
                help='è®­ç»ƒæ€å¯¹è·¯ç”± logits æ–½åŠ çš„é«˜æ–¯æŠ–åŠ¨å¼ºåº¦ï¼Œç”¨äºä¿ƒä½¿ä¸“å®¶æ¢ç´¢ï¼ˆé»˜è®¤ 0.01ï¼‰')
    # ç›´æµé€šè·¯ç›¸å…³å‚æ•° - æ¶æ„çº§ç»•è¿‡ä¼˜åŒ–
    p.add_argument('--enable-direct-pathway', action='store_true', default=True,
                help='å¯ç”¨MoEç›´æµé€šè·¯ï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”éªŒè¯ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    p.add_argument('--disable-direct-pathway', action='store_true', default=False,
                help='ç¦ç”¨ç›´æµé€šè·¯ï¼Œä½¿ç”¨çº¯ä¸“å®¶ç³»ç»Ÿï¼ˆç”¨äºå¯¹ç…§å®éªŒï¼‰')
    p.add_argument('--initial-bypass-weight', type=float, default=0.95,
                help='ç›´æµé€šè·¯åˆå§‹æƒé‡ (0.0-1.0)ï¼Œæ§åˆ¶è®­ç»ƒå¼€å§‹æ—¶ç›´æµvsä¸“å®¶çš„æ¯”ä¾‹ï¼Œé»˜è®¤0.95å¯ç”¨æ¶æ„çº§ç»•è¿‡')
    p.add_argument('--adaptive-threshold', type=float, default=0.10,
                help='æ€§èƒ½å·®å¼‚é˜ˆå€¼ï¼Œè§¦å‘æƒé‡è°ƒæ•´ï¼ˆé»˜è®¤10%ï¼‰')
    p.add_argument('--pathway-warmup-steps', type=int, default=2000,
                help='ç›´æµæƒé‡warmupæ­¥æ•°ï¼Œå‰ä¸€åŠä¸ºæ¶æ„çº§ç»•è¿‡æœŸ')

    args = p.parse_args()

    # å¼ºåˆ¶Stage3é…ç½® (æŒ‰ä»»åŠ¡æ¸…å•è¦æ±‚)
    if args.no_moe:
        safe_print("ğŸš¨ DEBUG: Disabling MoE for gradient explosion diagnosis")
        args.moe = False
    elif not args.moe:
        safe_print("âš ï¸  Warning: Force enabling MoE for Stage3 (per task requirements)")
        args.moe = True

    # ç›´æµé€šè·¯é…ç½®å¤„ç†
    if args.disable_direct_pathway:
        args.enable_direct_pathway = False
        safe_print("ğŸš¨ ç›´æµé€šè·¯å·²ç¦ç”¨ - çº¯ä¸“å®¶ç³»ç»Ÿæ¨¡å¼")
    elif args.enable_direct_pathway:
        safe_print(f"âœ… ç›´æµé€šè·¯å·²å¯ç”¨ - åˆå§‹æƒé‡: {args.initial_bypass_weight:.2f}")
        safe_print(f"   è‡ªé€‚åº”é˜ˆå€¼: {args.adaptive_threshold:.3f}, Warmupæ­¥æ•°: {args.pathway_warmup_steps}")

    # è·å–Stage3é…ç½®
    stage_cfg = get_stage_config("stage3")
    # ç¡®ä¿ç¦ç”¨FiLM (å•å˜é‡éªŒè¯è¦æ±‚)
    stage_cfg.use_film = False
    stage_cfg.apply_channel = False  # ç¦ç”¨ä¿¡é“æ¨¡æ‹Ÿ

    safe_print("ğŸš€ Starting Stage3 Training - MoEå¼•å…¥ (ç¦ç”¨FiLMï¼Œå•å˜é‡éªŒè¯)")
    safe_print(f"   MoE enabled: {args.moe}")
    safe_print(f"   FiLM disabled: {not stage_cfg.use_film}")
    safe_print(f"   Channel simulation disabled: {not stage_cfg.apply_channel}")
    safe_print(f"   Router strategy: {'no-CSI' if args.router_no_csi else 'with-CSI'}")
    safe_print(f"   AMP mode: {args.amp} | Preheat frames: {getattr(args, 'preheat_frames', 0)} | Wave warmup: {args.wave_warmup_steps}")
    safe_print(f"   DataLoader: workers={args.num_workers} | stride_frames={args.stride_frames}")
    safe_print(f"   Performance optimizations: grad_accum={args.gradient_accumulation_steps} | torch_compile={args.use_compile}")

    device = torch.device('cuda' if (args.device == 'auto' and torch.cuda.is_available()) else
                         args.device if args.device != 'auto' else 'cpu')
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Data loader
    train_loader, dataset = create_aether_data_loader(
        data_dir=str(Path(args.features).parent.parent) if 'lmr_export' in Path(args.features).parts else str(Path(args.features).parent),
        sequence_length=args.seq_len,
        batch_size=args.batch_size,
        max_samples=None,
        num_workers=max(1, int(args.num_workers)),
        energy_selection=True,
        test_mode=False,
        feature_spec_type='fargan',
        features_file=args.features,
        audio_file=args.pcm,
        stride_frames=args.stride_frames,  # æ–°å¢æ­¥å¹…é…ç½®
    )

    # Create models using simplified architecture
    config = {
        "d_in": args.feature_dims,
        "d_model": 128,
        "dz": 24,
        "d_csi": 10,
        "use_film": False,  # Stage3: ç¦ç”¨FiLM
        "use_moe": args.moe,  # Stage3: å¯ç”¨MoE
        "use_quantization": False,
        "latent_bits": 4,
        "n_experts": 3,    # Stage3: 3ä¸“å®¶ç³»ç»Ÿ (E1:Harmonic, E2:Transient, E3:BurstInpaint)
        "top_k": 2,        # Stage3: TOP-2è·¯ç”±
        "moe_router_use_csi": (not args.router_no_csi),  # Routerä¸ä½¿ç”¨CSI â†’ False
        "use_semantic_head": True,
        "semantic_dim": 6,
        "semantic_source": args.semantic_source,
        # ç›´æµé€šè·¯é…ç½®
        "enable_direct_pathway": args.enable_direct_pathway,
        "initial_bypass_weight": args.initial_bypass_weight,
        "adaptive_threshold": args.adaptive_threshold,
        "pathway_warmup_steps": args.pathway_warmup_steps,
    }

    encoder, _ = create_aether_codec(config)
    encoder = encoder.to(device)
    if hasattr(encoder, 'moe') and hasattr(encoder.moe, 'specialized_moe'):
        try:
            encoder.moe.specialized_moe.router_jitter = float(getattr(args, 'router_jitter', 0.0))
        except Exception:
            pass
    def _cast_rnns_fp32(module: torch.nn.Module):
        for m in module.modules():
            name = m.__class__.__name__
            if name in ('LSTM', 'GRU', 'RobustLSTM', 'RobustGRU'):
                m.to(torch.float32)

    _cast_rnns_fp32(encoder)



    # ğŸš¨ ç´§æ€¥ç»•è¿‡MoEæ¨¡å¼
    if args.emergency_bypass_moe and hasattr(encoder, 'moe') and encoder.moe is not None:
        safe_print("ğŸš¨ EMERGENCY: Bypassing MoE completely for NaN diagnosis")
        encoder.moe.specialized_moe._emergency_bypass = True

    # è‡ªå®šä¹‰decoder with FARGAN
    decoder = AETHERFARGANDecoder(
        d_out=args.feature_dims,
        d_csi=10,
        enable_synth=True,
        use_film=False  # Stage3: è§£ç ç«¯ç¦ç”¨FiLM
    ).to(device)

    # ğŸ”§ FIX 6: æ¡ä»¶æ€§mixed precisioné…ç½®ï¼Œé¿å…dtypeå†²çª
    # ä»…åœ¨éAMPæ¨¡å¼ä¸‹å¼ºåˆ¶float32ï¼ŒAMPæ¨¡å¼ä¸‹ä¿æŒä¸€è‡´æ€§
    if args.amp == 'none':
        decoder.fargan_core.float()  # éAMPæ¨¡å¼ä½¿ç”¨float32ç¡®ä¿ç¨³å®š
    # AMPæ¨¡å¼ä¸‹è®©autocastè‡ªåŠ¨ç®¡ç†dtypeï¼Œé¿å…å†²çª
    _cast_rnns_fp32(decoder)
    # å¯é€‰çš„æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
    if args.use_compile:
        try:
            safe_print("  ğŸš€ Compiling models with torch.compile...")
            encoder = torch.compile(encoder, mode='default')
            decoder = torch.compile(decoder, mode='default')
            safe_print("  âœ… Model compilation successful")
        except Exception as e:
            safe_print(f"  âš ï¸  Model compilation failed: {e}")
            safe_print("  ğŸ“‹ Continuing without compilation")

    # Optional: Load Stage 1 checkpoint (AETHER warm start)
    if args.stage1_checkpoint:
        try:
            stage1_ckpt = torch.load(args.stage1_checkpoint, map_location='cpu')
            enc_sd = stage1_ckpt.get('encoder_state_dict') or {}
            dec_sd = stage1_ckpt.get('decoder_state_dict') or {}

            # Load encoder with shape check
            enc_state = encoder.state_dict()
            enc_loaded = enc_skipped = 0
            for k, v in enc_sd.items():
                if k in enc_state and enc_state[k].shape == v.shape:
                    enc_state[k] = v
                    enc_loaded += 1
                else:
                    enc_skipped += 1
            encoder.load_state_dict(enc_state, strict=False)
            safe_print(f"âœ… Stage1: loaded encoder params: {enc_loaded} matched, {enc_skipped} skipped")

            # Partially load decoder (Stage1 AETHERDecoder â†’ AETHERFARGANDecoder common parts)
            dec_state = decoder.state_dict()
            dec_loaded = dec_skipped = 0
            for k, v in dec_sd.items():
                if k in dec_state and dec_state[k].shape == v.shape:
                    dec_state[k] = v
                    dec_loaded += 1
                else:
                    dec_skipped += 1
            decoder.load_state_dict(dec_state, strict=False)
            safe_print(f"âœ… Stage1: loaded decoder params: {dec_loaded} matched, {dec_skipped} skipped (strict=False)")
        except Exception as e:
            safe_print(f"âš ï¸  Failed to load Stage1 checkpoint: {e}")
    def force_vocoder_fp32(decoder):
        # åªæŠŠ fargan_core å¼ºåˆ¶åˆ° float32ï¼Œå…¶ä»–è§£ç å¤´ç»§ç»­è·Ÿéš AMP
        decoder.fargan_core.float()
        for m in decoder.fargan_core.modules():
            name = m.__class__.__name__
            if name in ('LSTM', 'GRU', 'RobustLSTM', 'RobustGRU'):
                m.to(torch.float32)
    # Load FARGAN weights if provided
    if args.fargan_checkpoint:
        try:
            ckpt = torch.load(args.fargan_checkpoint, map_location='cpu')
            state = ckpt.get('model_state_dict', ckpt.get('state_dict', ckpt))

            # å°è¯•åŠ è½½FARGAN coreæƒé‡å¹¶ç»Ÿè®¡
            decoder_state = decoder.state_dict()
            loaded = skipped = 0
            for k, v in state.items():
                if k.startswith('fargan_core.') and k in decoder_state and decoder_state[k].shape == v.shape:
                    decoder_state[k] = v
                    loaded += 1
                elif k.startswith('fargan_core.'):
                    skipped += 1

            decoder.load_state_dict(decoder_state, strict=False)
            safe_print(f"âœ… Stage2(FARGAN): loaded {loaded} params into decoder, skipped {skipped}")
            # é‡æ–°è®¾ç½®dtype
            # if args.amp == 'fp16':
            #     decoder = decoder.to(torch.float16)
            # elif args.amp == 'bf16':
            #     decoder = decoder.to(torch.bfloat16)
        except Exception as e:
            safe_print(f"âš ï¸  Failed to load FARGAN checkpoint: {e}")
        # ğŸ”§ FIX 7: åº”ç”¨ä¸€è‡´çš„mixed precisionç­–ç•¥
        if args.amp == 'none':
            force_vocoder_fp32(decoder)  # ä»…éAMPæ¨¡å¼å¼ºåˆ¶float32

    # Optimizer with differential learning rates
    lr = getattr(stage_cfg, 'learning_rate', 2e-4)

    # FARGANï¼šæ ¸å¿ƒä¸ parametrizations å•ç‹¬ param groupï¼ˆæ›´å° lrï¼Œä¸”ç¦ WDï¼‰
    decoder_params, fargan_core_params, fargan_parametrizations = [], [], []
    for name, p in decoder.named_parameters():
        if 'parametrizations' in name:
            fargan_parametrizations.append(p)
        elif 'fargan_core' in name:
            fargan_core_params.append(p)
        else:
            decoder_params.append(p)
    # åœ¨åˆ›å»º optimizer ä¹‹å‰ï¼Œæ›¿æ¢ encoder çš„åˆ†ç»„ï¼š
    enc_backbone, enc_attn = [], []
    for n, p in encoder.named_parameters():
        if any(k in n for k in ['thread_blocks', 'qkv', 'out_proj', 'mix']):
            enc_attn.append(p)       # æ³¨æ„åŠ›/æ··åˆç›¸å…³
        else:
            enc_backbone.append(p)   # å…¶å®ƒ
    param_groups = [
        {'params': enc_backbone, 'lr': lr,      'weight_decay': 1e-6},
        {'params': enc_attn,     'lr': lr*0.5,  'weight_decay': 1e-6},  # æ³¨æ„åŠ›å±‚åŠé€Ÿ,
        {'params': decoder_params,                  'lr': lr,       'weight_decay': 1e-6},
        {'params': fargan_core_params,              'lr': lr*0.1,   'weight_decay': 0.0},
        {'params': fargan_parametrizations,         'lr': lr*0.1,   'weight_decay': 0.0},
    ]


    optimizer = optim.AdamW(param_groups, weight_decay=1e-6)
    use_fp16 = (args.amp == 'fp16')
    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)
    accum_steps = max(1, int(getattr(args, 'gradient_accumulation_steps', 1)))
    safe_print(f"ğŸ“Š Training setup:")
    safe_print(f"   Model params: Encoder={sum(p.numel() for p in encoder.parameters()):,}, "
          f"Decoder={sum(p.numel() for p in decoder.parameters()):,}")
    safe_print(f"   Learning rate: {lr}")
    safe_print(f"   Batch size: {args.batch_size}, Sequence length: {args.seq_len}")
    safe_print(f"   Total batches per epoch: {len(train_loader)}")

    # Training loop
    best_loss = float('inf')
    global_step = 0

    safe_print("\nğŸ¯ Stage3 éªŒæ”¶æ ‡å‡†:")
    safe_print("   - ç‰¹å¾é‡å»ºæŸå¤± â‰¤ 0.25")
    safe_print("   - MoEä¸“å®¶åˆ©ç”¨ç‡ > 75% (3ä¸ªä¸“å®¶å‡è¡¡ä½¿ç”¨: Harmonic, Transient, BurstInpaint)")
    safe_print("   - æ³¢å½¢RMSåç§» < 5dB")
    safe_print("   - MoEæŸå¤±æ”¶æ•›ç¨³å®š")
    safe_print("   - æ— FiLMæ¡ä»¶ä¸‹è®­ç»ƒç¨³å®š")
    safe_print("   - è·³è¿‡LowSNRExpert (æ— CSIè¾“å…¥)")
    safe_print(f"   - Rate loss {'å¯ç”¨' if args.enable_rate else 'ç¦ç”¨'} (Stage3é»˜è®¤ç¦ç”¨ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸)\n")

    # CUDA backend optimisations
    if device.type == 'cuda':
        try:
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.set_float32_matmul_precision('high')
        except Exception:
            pass

    # å¼ºåˆ¶flushæ‰€æœ‰é…ç½®ä¿¡æ¯ï¼Œç¡®ä¿åœ¨è¿›åº¦æ¡å‡ºç°å‰æ˜¾ç¤º
    import sys
    sys.stdout.flush()
    sys.stderr.flush()

    for epoch in range(1, args.epochs + 1):
        safe_print(f"ğŸ”„ Epoch {epoch}/{args.epochs}")

        epoch_metrics, global_step = train_one_epoch(
            encoder, decoder, train_loader, device, optimizer,
            stage_cfg, global_step, args, epoch_idx=epoch,
            scaler=scaler, 
        )

        # Epoch summary
        safe_print(f"ğŸ“Š Epoch {epoch} Summary:")
        safe_print(f"   Total Loss: {epoch_metrics['total_loss']:.6f}")
        safe_print(f"   Feature Loss: {epoch_metrics['feature_loss']:.6f}")
        safe_print(f"   Wave Loss: {epoch_metrics['wave_loss']:.6f}")
        safe_print(f"   MoE Loss: {epoch_metrics['moe_loss']:.6f}")
        safe_print(f"   Rate Loss: {epoch_metrics['rate_loss']:.6f}")

        # MoE health check (åªåœ¨éç»•è¿‡æ¨¡å¼ä¸‹æ˜¾ç¤º)
        if args.emergency_bypass_moe:
            safe_print(f"   ğŸš¨ MoE Status: BYPASSED (emergency diagnosis mode)")
        elif encoder.use_moe:
            expert_min = epoch_metrics.get('expert_usage_min', 0)
            expert_max = epoch_metrics.get('expert_usage_max', 0)
            expert_entropy = epoch_metrics.get('expert_entropy', 0)
            expert_balance = 1.0 - (expert_max - expert_min)  # å‡è¡¡åº¦

            # Display individual expert usage rates if available - åŠ¨æ€ä¸“å®¶æ•°é‡
            n_experts = getattr(encoder.moe, 'n_experts', 3) if hasattr(encoder, 'moe') else 3
            expert_usage_display = []
            for i in range(n_experts):  # åŠ¨æ€ä¸“å®¶æ•°é‡
                usage_key = f'expert_{i}_usage'
                if usage_key in epoch_metrics:
                    expert_usage_display.append(f"{epoch_metrics[usage_key]:.3f}")
                else:
                    expert_usage_display.append("N/A")

            expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"][:n_experts]  # æ ¹æ®ä¸“å®¶æ•°é‡æˆªæ–­

            safe_print(f"   MoE Health:")
            safe_print(f"     Expert Usage: [{', '.join(expert_usage_display)}]")
            for i, (name, usage) in enumerate(zip(expert_names, expert_usage_display)):
                safe_print(f"       E{i+1} {name}: {usage}")
            safe_print(f"     Expert Entropy: {expert_entropy:.3f}")
            safe_print(f"     Balance Score: {expert_balance:.3f}")

            # éªŒæ”¶æ ‡å‡†æ£€æŸ¥
            if expert_min > 0.2:  # æ¯ä¸ªä¸“å®¶>20%ä½¿ç”¨ç‡
                safe_print("     âœ… Expert utilization criterion met")
            else:
                safe_print("     âŒ Expert utilization below threshold")

            if expert_entropy > 0.8:
                safe_print("     âœ… Expert entropy criterion met")
            else:
                safe_print("     âŒ Expert entropy below threshold")

        # Periodic epoch checkpoint
        if args.save_every_epochs and args.save_every_epochs > 0 and (epoch % args.save_every_epochs == 0):
            epoch_ckpt = {
                'epoch': epoch,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': float(epoch_metrics['total_loss']),
                'metrics': epoch_metrics,
                'config': config,
                'args': vars(args)
            }
            ckpt_path = out_dir / f'stage3_epoch_{epoch}.pth'
            torch.save(epoch_ckpt, ckpt_path)
            safe_print(f"ğŸ’¾ Saved epoch checkpoint: {ckpt_path}")

        # Save best checkpoint
        if epoch_metrics['total_loss'] < best_loss:
            best_loss = epoch_metrics['total_loss']
            checkpoint = {
                'epoch': epoch,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'metrics': epoch_metrics,
                'config': config,
                'args': vars(args)
            }

            ckpt_path = out_dir / 'stage3_best.pth'
            torch.save(checkpoint, ckpt_path)
            # Also save a copy with the epoch suffix for reproducibility
            ckpt_epoch_path = out_dir / f'stage3_best_epoch_{epoch}.pth'
            try:
                torch.save(checkpoint, ckpt_epoch_path)
            except Exception as _e:
                safe_print(f"âš ï¸  Failed to save epoch-suffixed best checkpoint: {ckpt_epoch_path} ({_e})")
            safe_print(f"ğŸ’¾ Saved best checkpoint: {ckpt_path} (epoch copy: {ckpt_epoch_path})")

            # éªŒæ”¶æ ‡å‡†ç»¼åˆæ£€æŸ¥
            feature_loss_ok = epoch_metrics['feature_loss'] <= 0.25
            expert_util_ok = encoder.use_moe and expert_min > 0.2

            safe_print(f"ğŸ“‹ Stage3 éªŒæ”¶è¿›åº¦:")
            safe_print(f"   Feature Loss â‰¤ 0.25: {'âœ…' if feature_loss_ok else 'âŒ'} ({epoch_metrics['feature_loss']:.4f})")
            if encoder.use_moe:
                safe_print(f"   Expert Utilization > 20%: {'âœ…' if expert_util_ok else 'âŒ'} (min={expert_min:.3f})")

    safe_print(f"\nğŸ‰ Stage3 training completed!")
    safe_print(f"   Best loss: {best_loss:.6f}")
    safe_print(f"   Final checkpoint: {out_dir / 'stage3_best.pth'}")

    # Always save last if requested
    if args.always_save_last:
        last_ckpt = {
            'epoch': args.epochs,
            'encoder_state_dict': encoder.state_dict(),
            'decoder_state_dict': decoder.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': float(best_loss),
            'config': config,
            'args': vars(args)
        }
        ckpt_path = out_dir / 'stage3_last.pth'
        torch.save(last_ckpt, ckpt_path)
        safe_print(f"ğŸ’¾ Saved last checkpoint: {ckpt_path}")

    # æœ€ç»ˆéªŒæ”¶æŠ¥å‘Š
    if best_loss <= 0.25:
        safe_print("âœ… Stage3 training PASSED - ready for Stage4")
    else:
        safe_print("âš ï¸  Stage3 training needs improvement - consider adjusting hyperparameters")

    return 0


if __name__ == '__main__':
    raise SystemExit(main())
