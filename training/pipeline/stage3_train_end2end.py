#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage 3: MoEå¼•å…¥è®­ç»ƒ (ç¦ç”¨FiLMï¼Œå•å˜é‡éªŒè¯MoEè´¡çŒ®)

æŒ‰ç…§AETHERå·¥ç¨‹çº§ä»»åŠ¡æ‰§è¡Œæ¸…å•è¦æ±‚:
- ç›®æ ‡: éš”ç¦»è¯„ä¼°MoEå¯¹ç“¶é¢ˆè¡¨è¾¾çš„è´¡çŒ®
- æ¨¡å—: ä¿ç•™DualStream+GLAï¼›å¯ç”¨Micro-MoEï¼›ç¦ç”¨FiLMï¼›ç¦ç”¨ä¿¡é“æ¨¡æ‹Ÿ
- å•å˜é‡åŸåˆ™: é¿å…æ··æ·†MoEä¸CSI/FiLMçš„è´¡çŒ®
"""

from __future__ import annotations
import argparse
from pathlib import Path
from typing import Dict, Optional, Tuple, Any

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from tqdm.auto import tqdm
import soundfile as sf

# ä½¿ç”¨ç®€åŒ–çš„æ¶æ„
import sys
import os
# Ensure final_version root is on sys.path; avoid inserting subdirs (e.g. models)
# to prevent shadowing top-level packages like 'utils'.
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from models.enhanced_aether_integration import AETHEREncoder, AETHERDecoder, create_aether_codec
from models.maybe_useless.aether_fargan_decoder import AETHERFARGANDecoder
from utils.real_data_loader import create_aether_data_loader
from utils.real_data_loader import create_combined_data_loader
from training.pipeline.stages import StageConfig, get_stage_config
# ğŸ”¥ æ¢å¤FARGANæ ‡å‡†æŸå¤±ï¼Œç§»é™¤è‡ªå®šä¹‰audio_usability_loss
from training.pipeline.wave_loss import fargan_wave_losses
from training.losses import rate_loss, compute_layered_loss
from models.utils import validate_csi_config, extract_acoustic_priors
from models.semantic_fargan_adapter import create_semantic_fargan_adapter
from models.semantic_extractor import create_semantic_extractor
# ---- æ”¾åœ¨æ–‡ä»¶é¡¶éƒ¨åˆé€‚ä½ç½®ï¼ˆæˆ– train_one_epoch å†…éƒ¨å¼€å¤´ï¼‰----
def _sum_grad_norm(named_params, include_key=None, exclude_key=None):
    """å¯¹æŒ‡å®šå‚æ•°é›†åˆæ±‚æ¢¯åº¦èŒƒæ•°ä¹‹å’Œï¼ˆå…ˆå‡€åŒ– NaN/Infï¼‰ï¼Œè¿”å› (sum_norm, n_tensors)ã€‚"""
    total = 0.0
    n = 0
    for name, p in named_params:
        if p.grad is None:
            continue
        if include_key is not None and include_key not in name:
            continue
        if exclude_key is not None and exclude_key in name:
            continue
        g = p.grad.detach()
        # å…³é”®ï¼šå‡€åŒ– NaN/Infï¼Œé¿å…æ•´æ®µç»Ÿè®¡å˜ NaN
        g = torch.nan_to_num(g, nan=0.0, posinf=0.0, neginf=0.0)
        # ç”¨ float() å†å–èŒƒæ•°ï¼Œé¿å…åŠç²¾åº¦ä¸‹æº¢
        val = g.float().norm()
        # åŒé‡ä¿é™©ï¼šèŒƒæ•°æœ¬èº«è‹¥ä»éæœ‰é™ï¼Œå°±è·³è¿‡
        if torch.isfinite(val):
            total += val.item()
            n += 1
    return total, n

def _topk_grad_norm(named_params, k=5, include_key=None, exclude_key=None):
    """å¯é€‰ï¼šæ‰“å°æœ€å¤§æ¢¯åº¦èŒƒæ•°çš„è‹¥å¹²å‚æ•°ï¼Œä¾¿äºå®šä½å¼‚å¸¸ã€‚"""
    arr = []
    for name, p in named_params:
        if p.grad is None:
            continue
        if include_key is not None and include_key not in name:
            continue
        if exclude_key is not None and exclude_key in name:
            continue
        g = torch.nan_to_num(p.grad.detach(), nan=0.0, posinf=0.0, neginf=0.0)
        val = g.float().norm()
        if torch.isfinite(val):
            arr.append((val.item(), name))
    arr.sort(key=lambda x: x[0], reverse=True)
    return arr[:k]

def _print_feature_reconstruction_stats(pred_feats, orig_feats, global_step, batch_idx):
    """
    æ‰“å°36ç»´FARGANç‰¹å¾çš„é‡å»ºç»Ÿè®¡ä¿¡æ¯ï¼ˆå16ç»´ä¸å†æ‰“å°LPCï¼Œæ”¹ç”±è¯­ä¹‰ç»Ÿè®¡å•ç‹¬è¾“å‡ºï¼‰
    pred_feats: å¤åŸç‰¹å¾ [B, T, 36]
    orig_feats: åŸå§‹ç‰¹å¾ [B, T, 36]
    """
    with torch.no_grad():
        tqdm.write(f"\n========== ç‰¹å¾é‡å»ºç»Ÿè®¡ (Step {global_step}, Batch {batch_idx}) ==========")

        # å‰18ç»´å€’è°±ç‰¹å¾ç»Ÿè®¡
        tqdm.write("--- å€’è°±ç‰¹å¾ (Dims 0-17) ---")
        for dim in range(18):
            pred_dim = pred_feats[:, :, dim].flatten()
            orig_dim = orig_feats[:, :, dim].flatten()

            pred_mean, pred_std = pred_dim.mean().item(), pred_dim.std().item()
            pred_min, pred_max = pred_dim.min().item(), pred_dim.max().item()
            orig_mean, orig_std = orig_dim.mean().item(), orig_dim.std().item()
            orig_min, orig_max = orig_dim.min().item(), orig_dim.max().item()

            tqdm.write(f"  Dim[{dim:2d}] | Pred: mean={pred_mean:+6.3f} std={pred_std:6.3f} range=[{pred_min:+6.3f}, {pred_max:+6.3f}]")
            tqdm.write(f"         | Orig: mean={orig_mean:+6.3f} std={orig_std:6.3f} range=[{orig_min:+6.3f}, {orig_max:+6.3f}]")

        # ç¬¬19ç»´F0ç‰¹å¾ç»Ÿè®¡ (DNN Pitch)
        tqdm.write("\n--- F0/åŸºé¢‘ç‰¹å¾ (Dim 18) ---")
        pred_f0 = pred_feats[:, :, 18].flatten()
        orig_f0 = orig_feats[:, :, 18].flatten()

        pred_f0_mean, pred_f0_std = pred_f0.mean().item(), pred_f0.std().item()
        pred_f0_min, pred_f0_max = pred_f0.min().item(), pred_f0.max().item()
        orig_f0_mean, orig_f0_std = orig_f0.mean().item(), orig_f0.std().item()
        orig_f0_min, orig_f0_max = orig_f0.min().item(), orig_f0.max().item()

        tqdm.write(f"  F0     | Pred: mean={pred_f0_mean:+6.3f} std={pred_f0_std:6.3f} range=[{pred_f0_min:+6.3f}, {pred_f0_max:+6.3f}]")
        tqdm.write(f"         | Orig: mean={orig_f0_mean:+6.3f} std={orig_f0_std:6.3f} range=[{orig_f0_min:+6.3f}, {orig_f0_max:+6.3f}]")

        # æ¸…æµŠéŸ³ç»Ÿè®¡ (åŸºäºF0é˜ˆå€¼åˆ¤æ–­)
        pred_voiced = (pred_f0 > -1.0).float().mean().item()
        orig_voiced = (orig_f0 > -1.0).float().mean().item()
        tqdm.write(f"  Voice  | Pred: voiced={pred_voiced:.3f} unvoiced={1-pred_voiced:.3f}")
        tqdm.write(f"         | Orig: voiced={orig_voiced:.3f} unvoiced={1-orig_voiced:.3f}")

        # ç¬¬20ç»´å¸§ç›¸å…³æ€§ç‰¹å¾
        tqdm.write("\n--- å¸§ç›¸å…³æ€§ç‰¹å¾ (Dim 19) ---")
        pred_corr = pred_feats[:, :, 19].flatten()
        orig_corr = orig_feats[:, :, 19].flatten()

        pred_corr_mean, pred_corr_std = pred_corr.mean().item(), pred_corr.std().item()
        pred_corr_min, pred_corr_max = pred_corr.min().item(), pred_corr.max().item()
        orig_corr_mean, orig_corr_std = orig_corr.mean().item(), orig_corr.std().item()
        orig_corr_min, orig_corr_max = orig_corr.min().item(), orig_corr.max().item()

        tqdm.write(f"  Corr   | Pred: mean={pred_corr_mean:+6.3f} std={pred_corr_std:6.3f} range=[{pred_corr_min:+6.3f}, {pred_corr_max:+6.3f}]")
        tqdm.write(f"         | Orig: mean={orig_corr_mean:+6.3f} std={orig_corr_std:6.3f} range=[{orig_corr_min:+6.3f}, {orig_corr_max:+6.3f}]")

        # æ•´ä½“é‡å»ºè´¨é‡è¯„ä¼°ï¼ˆæœ¬èŠ‚ä¸å†çº³å…¥å16ç»´ï¼‰
        tqdm.write("\n--- æ•´ä½“é‡å»ºè´¨é‡ï¼ˆä¸å«å16ç»´è¯­ä¹‰ï¼‰ ---")
        overall_mse = F.mse_loss(pred_feats[:, :, :20], orig_feats[:, :, :20]).item()
        overall_mae = F.l1_loss(pred_feats[:, :, :20], orig_feats[:, :, :20]).item()

        # åˆ†æ®µè¯„ä¼°ï¼ˆä¸å«å16ç»´ï¼‰
        cepstral_mse = F.mse_loss(pred_feats[:, :, :18], orig_feats[:, :, :18]).item()
        f0_mse = F.mse_loss(pred_feats[:, :, 18:19], orig_feats[:, :, 18:19]).item()
        corr_mse = F.mse_loss(pred_feats[:, :, 19:20], orig_feats[:, :, 19:20]).item()

        tqdm.write(f"  Overall MSE: {overall_mse:.6f}, MAE: {overall_mae:.6f}")
        tqdm.write(f"  Cepstral MSE: {cepstral_mse:.6f}")
        tqdm.write(f"  F0 MSE: {f0_mse:.6f}")
        tqdm.write(f"  Correlation MSE: {corr_mse:.6f}")

        tqdm.write("=" * 65)

def _print_semantic_alignment_stats(semantic_pred: torch.Tensor, semantic_target: torch.Tensor, global_step: int, batch_idx: int):
    """
    æ‰“å°å16ç»´è¯­ä¹‰ç‰¹å¾çš„å¯¹é½ç»Ÿè®¡ä¿¡æ¯
    semantic_pred:   [B, T, 16]
    semantic_target: [B, T, 16]
    """
    with torch.no_grad():
        tqdm.write(f"\n---------- è¯­ä¹‰ç‰¹å¾å¯¹é½ (Step {global_step}, Batch {batch_idx}) ----------")
        for dim in range(16):
            pred_dim = semantic_pred[:, :, dim].flatten()
            tgt_dim  = semantic_target[:, :, dim].flatten()

            p_mean, p_std = pred_dim.mean().item(), pred_dim.std().item()
            p_min, p_max  = pred_dim.min().item(), pred_dim.max().item()
            t_mean, t_std = tgt_dim.mean().item(),  tgt_dim.std().item()
            t_min, t_max  = tgt_dim.min().item(),  tgt_dim.max().item()

            tqdm.write(f"  Sem[{dim:2d}] | Pred: mean={p_mean:+6.3f} std={p_std:6.3f} range=[{p_min:+6.3f}, {p_max:+6.3f}]")
            tqdm.write(f"           | Tgt : mean={t_mean:+6.3f} std={t_std:6.3f} range=[{t_min:+6.3f}, {t_max:+6.3f}]")

        sem_mse = F.mse_loss(semantic_pred, semantic_target).item()
        sem_mae = F.l1_loss(semantic_pred, semantic_target).item()
        tqdm.write(f"  Semantic MSE: {sem_mse:.6f}, MAE: {sem_mae:.6f}")
        tqdm.write("-" * 65)

class CharbonnierLoss(nn.Module):
    def __init__(self, eps: float = 1e-3):
        super().__init__()
        self.eps = eps
    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        # ç¡®ä¿epsä¸è¾“å…¥tensorçš„dtypeä¸€è‡´
        eps_tensor = torch.tensor(self.eps, dtype=x.dtype, device=x.device)
        return torch.mean(torch.sqrt((x - y) **2 + eps_tensor** 2))

class SimplifiedStage3Trainer:
    """Simplified Stage3 trainer focusing on MoE validation only."""
    def __init__(self, config: Dict[str, Any]):
        self.config = config

        # Stage3: å¼ºåˆ¶ç¦ç”¨FiLMï¼Œå¯ç”¨3ä¸“å®¶MoE (æ— CSIï¼Œä¸éœ€è¦LowSNRExpert)
        stage3_config = {
            "d_in": 36,
            "d_model": 128,
            "dz": 24,
            "d_csi": 10,
            "use_film": False,  # Stage3: ç¦ç”¨FiLM
            "use_moe": True,   # Stage3: å¯ç”¨ç®€åŒ–ç»Ÿä¸€MoE
            "use_quantization": False,
            "latent_bits": 4,
            "n_experts": 4,    # Stage3: 4ä¸ªåŠŸèƒ½ä¸“å®¶ (Harmonic/Transient/BurstInpaint/LowSNR)
            "top_k": 2,        # Stage3: TOP-2è·¯ç”±ï¼Œæ ¹æ®éŸ³é¢‘å†…å®¹é€‰æ‹©åˆé€‚ä¸“å®¶
            # æ ¹æ®ä¸“å®¶æ•°æ®éªŒè¯æŠ¥å‘Šä¼˜åŒ–çš„MoEé…ç½®
            "moe_balance_weight": 0.5,      # éªŒè¯æŠ¥å‘Šå»ºè®®ï¼šä¸­ç­‰è´¨é‡æ•°æ®ä½¿ç”¨0.5
            "expert_dropout": 0.15,         # éªŒè¯æŠ¥å‘Šå»ºè®®ï¼šä½¿ç”¨0.15
            "router_jitter": 0.0,           # ç¦ç”¨è·¯ç”±æŠ–åŠ¨ï¼Œä¸“æ³¨ä¸“å®¶å·®å¼‚åŒ–
            "moe_router_use_csi": False,    # æ–°è®¾è®¡ï¼šç¦ç”¨CSIè·¯ç”±ï¼Œæ”¹ä¸ºçº¯éŸ³é¢‘ç‰¹å¾è·¯ç”±
            "enable_direct_pathway": True,   # å¯ç”¨ç›´æµé€šè·¯ï¼Œæƒé‡0.1
            "initial_bypass_weight": 0.1,   # éªŒè¯æŠ¥å‘Šå»ºè®®çš„ç›´æµé€šè·¯æƒé‡
            "adaptive_threshold": 0.15,     # é€‚åº”æ€§é˜ˆå€¼
        }

        # æ‰“å°Stage3 MoEé…ç½®æ‘˜è¦
        print("=" * 60)
        print("Stage3 Training - Unified MoE Configuration")
        print("=" * 60)
        print(f"Expert Count: {stage3_config['n_experts']} specialized experts")
        print(f"Top-K Routing: {stage3_config['top_k']} (competitive expert selection)")
        print(f"Architecture: Audio-Scenario Specialized UnifiedAudioExpert")
        print(f"Expert Functions: Harmonic/Transient/BurstInpaint/LowSNR")
        print(f"Balance Weight: {stage3_config['moe_balance_weight']} (optimized by expert data validation)")
        print(f"Expert Dropout: {stage3_config['expert_dropout']} (optimized by expert data validation)")
        print(f"Router Jitter: {stage3_config['router_jitter']} (disabled for specialization focus)")
        print(f"Direct Pathway: {stage3_config['enable_direct_pathway']} (weight: {stage3_config.get('initial_bypass_weight', 0.1)})")
        print("Key Improvements:")
        print("  - Expert data augmentation: Targeted datasets for each expert specialization")
        print("  - Audio scenario specialization: Harmonic(tonal) / Transient(dynamic) / Repair(gaps) / LowSNR(feature-noise)")
        print("  - Top-K=2 selects most relevant experts based on audio content analysis")
        print("  - Scenario-specific initialization patterns match processing requirements")
        print("  - AcousticFeatureExtractor provides expert routing signals from raw features")
        print("  - LowSNR expert focuses on raw feature quality, not channel noise")
        print("=" * 60)

        self.encoder, self.decoder = create_aether_codec(stage3_config)

    def get_moe_metrics(self) -> Dict[str, float]:
        """è·å–MoEä¸“å®¶åˆ©ç”¨ç‡æŒ‡æ ‡ã€‚"""
        metrics = {}
        if hasattr(self.encoder, 'moe') and self.encoder.moe is not None:
            try:
                expert_util = self.encoder.moe.get_expert_utilization()
                metrics['expert_usage_min'] = expert_util.min().item()
                metrics['expert_usage_max'] = expert_util.max().item()
                metrics['expert_entropy'] = -(expert_util * torch.log(expert_util + 1e-8)).sum().item()
                metrics['expert_balance'] = 1.0 - expert_util.std().item()
                # Store individual expert utilization rates
                for i, util in enumerate(expert_util):
                    metrics[f'expert_{i}_usage'] = util.item()

                # Store as formatted string for display with expert names
                expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"]
                expert_usage_named = []
                for i, util in enumerate(expert_util):
                    name = expert_names[i] if i < len(expert_names) else f"E{i}"
                    expert_usage_named.append(f"{name}:{util.item():.3f}")
                metrics['expert_usage_all'] = ', '.join(expert_usage_named)
            except (AttributeError, RuntimeError):
                # Fallback for MoE without utilization tracking
                metrics['expert_usage_min'] = 0.25  # Placeholder
                metrics['expert_usage_max'] = 0.25
                metrics['expert_entropy'] = 1.386  # log(4) for 4 experts
                metrics['expert_balance'] = 0.8
                # ä½¿ç”¨å¸¦åç§°çš„å ä½ç¬¦
                metrics['expert_usage_all'] = 'Harmonic:0.250, Transient:0.250, BurstInpaint:0.250, LowSNR:0.250'
        return metrics


def train_one_epoch(
    encoder: nn.Module,
    decoder: nn.Module,
    loader,
    device: torch.device,
    optimizer: optim.Optimizer,
    stage_cfg: StageConfig,
    current_step: int,
    args: argparse.Namespace,
    epoch_idx: Optional[int] = None,
    scaler: Optional[torch.cuda.amp.GradScaler] = None, 
) -> Tuple[Dict[str, float], int]:
    """Train one epoch with MoE monitoring (encoder+decoder provided)."""
    encoder.train()
    decoder.train()
    use_fp16 = (scaler is not None) and scaler.is_enabled()
    epoch_metrics = {
        'total_loss': 0.0,
        'feature_loss': 0.0,
        'wave_loss': 0.0,
        'moe_loss': 0.0,
        'rate_loss': 0.0,
        'expert_entropy': 0.0,
        'expert_usage_min': 0.0,
        'expert_usage_max': 0.0,
    }
    total_samples = 0
    step = current_step

    progress = tqdm(
        enumerate(loader),
        total=len(loader),
        desc=f"Epoch {epoch_idx}/{args.epochs}" if epoch_idx is not None else "Train",
        dynamic_ncols=True,
        leave=False,
    )

    if not hasattr(train_one_epoch, '_gn_ema'):
        train_one_epoch._gn_ema = 0.0
    if not hasattr(train_one_epoch, '_wave_bp_ratio'):
        train_one_epoch._wave_bp_ratio = 0.0

    # ğŸ”¥ ç§»é™¤æ—§çš„CharbonnierLossï¼Œæ–°ç³»ç»Ÿä½¿ç”¨audio_usability_loss
    # wave_char = CharbonnierLoss(eps=1e-3)

    # === ğŸ”¥ è¯­ä¹‰æ„ŸçŸ¥ç³»ç»Ÿå¼ºåˆ¶å¯ç”¨ ===
    # åˆå§‹åŒ–è¯­ä¹‰FarGANé€‚é…å™¨
    if not hasattr(train_one_epoch, '_semantic_adapter'):
        train_one_epoch._semantic_adapter = create_semantic_fargan_adapter(
            adapter_type="progressive",
            input_dim=36,
            output_dim=36
        ).to(device)

        # å¼ºåˆ¶æ·»åŠ é€‚é…å™¨å‚æ•°åˆ°ä¼˜åŒ–å™¨
        adapter_params = [p for p in train_one_epoch._semantic_adapter.parameters() if p.requires_grad]
        if adapter_params:
            current_lr = optimizer.param_groups[0]['lr']
            optimizer.add_param_group({
                'params': adapter_params,
                'lr': current_lr * 0.1,
                'weight_decay': 0.0
            })
            print(f"      ğŸ”¥ Semantic adapter activated: {len(adapter_params)} parameters")

    semantic_adapter = train_one_epoch._semantic_adapter

    for batch_idx, batch in progress:

        global_step = step + batch_idx

        # Optional: token-level router warmup (use sample-level only for first N steps)
        warmup_tok = int(getattr(args, 'moe_token_warmup_steps', 0) or 0)
        if warmup_tok > 0 and hasattr(encoder, 'moe') and encoder.moe is not None:
            try:
                # CompatibleMicroMoE wrapper path
                encoder.moe.specialized_moe.use_token_level = (global_step >= warmup_tok)
            except Exception:
                # Legacy MicroMoE or other: ignore
                pass

        # Optional: Top-K warm-start for early steps (e.g., force k=1 for the first N steps)
        try:
            topk_warm_steps = int(getattr(args, 'topk_warm_steps', 0) or 0)
            if topk_warm_steps > 0 and hasattr(encoder, 'moe') and encoder.moe is not None \
               and hasattr(encoder.moe, 'specialized_moe') and hasattr(encoder.moe.specialized_moe, 'topk'):
                sm = encoder.moe.specialized_moe
                if not hasattr(sm, '_topk_orig'):
                    sm._topk_orig = int(getattr(sm, 'topk', 2) or 2)
                if global_step < topk_warm_steps:
                    sm.topk = int(getattr(args, 'topk_warm_k', 1) or 1)
                else:
                    sm.topk = int(getattr(sm, '_topk_orig', 2) or 2)
        except Exception:
            pass

        # Data to device
        x = batch['x'].to(device, non_blocking=True)  # Features [B, T, 36]
        y = batch['y'].to(device, non_blocking=True)  # Target features [B, T, 36]
        audio = batch['audio'].to(device, non_blocking=True)  # Target audio [B, L]

        # Stage3: ä½¿ç”¨å›ºå®šdummy CSIå‡å°‘è®¡ç®—å¼€é”€ (çœŸæ­£çš„å•å˜é‡éªŒè¯)
        batch_size = x.size(0)

        # æ¢¯åº¦ç´¯ç§¯ï¼šä»…åœ¨å¼€å§‹æ—¶æ¸…é›¶ï¼ˆåœ¨ç´¯ç§¯å¾ªç¯å†…éƒ¨å¤„ç†ï¼‰
        accum_steps = max(1, int(getattr(args, 'gradient_accumulation_steps', 1)))
        if batch_idx % accum_steps == 0:
            optimizer.zero_grad(set_to_none=True)

        # è®¡ç®—æ³¢å½¢å¯ç”¨ä¸çƒ­èº«ï¼šå…ˆä¸¥æ ¼æ‰§è¡Œ wave_start_stepï¼Œå†è¿›è¡Œ warmup æ‹‰èµ·
        start_step = int(getattr(args, 'wave_start_step', 0) or 0)
        warm_steps = int(getattr(args, 'wave_warmup_steps', 0) or 0)
        active_wave = (global_step >= start_step)
        if active_wave and warm_steps > 0:
            warm_ratio = min(1.0, max(0.0, (global_step - start_step) / float(warm_steps)))
        else:
            warm_ratio = 0.0

        # ç›®æ ‡åä¼ æ¯”ä¾‹ï¼šæ›´æ¸©å’Œçš„äºŒæ¬¡æ›²çº¿
        bp_target = warm_ratio ** 2
        # ä»…å½“æœ€è¿‘æ¢¯åº¦EMAè¾ƒä½æ—¶ï¼Œæ‰å…è®¸æ”¾å¼€åä¼ æ¯”ä¾‹ï¼ˆåªå¢ä¸å‡ï¼‰
        gn_ema = float(train_one_epoch._gn_ema)
        wave_bp_ratio = float(train_one_epoch._wave_bp_ratio)
        if gn_ema < 3.0:
            wave_bp_ratio = max(wave_bp_ratio, bp_target)
        # >>> ğŸ”§ FIX 3: ä¼˜åŒ–æ¢¯åº¦ä¼ æ’­æ§åˆ¶ï¼Œå¤§å¹…æé«˜ä¼ æ’­æ¯”ä¾‹ <<<
        # è§£å†³fargan_coreæ¢¯åº¦è¿‡å°çš„é—®é¢˜ï¼šä»50%æé«˜åˆ°80%
        min_bp = float(getattr(args, 'wave_min_bp', 0.8))  # æé«˜åˆ°80%
        if active_wave:
            wave_bp_ratio = max(wave_bp_ratio, min_bp)   # ç¡®ä¿æœ€å°80%æ¢¯åº¦ä¼ æ’­

        # åœ¨è®­ç»ƒæ—©æœŸè¿›ä¸€æ­¥å¢å¼ºæ¢¯åº¦ä¼ æ’­
        if global_step < 2000:  # å‰2000æ­¥ä½¿ç”¨æ›´é«˜çš„ä¼ æ’­æ¯”ä¾‹
            wave_bp_ratio = max(wave_bp_ratio, 0.9)

        # <<< DEBUG/SAFETY end <<<

        # ä¿®å¤æŸå¤±æƒé‡ï¼šæŒ‰ç”¨æˆ·è¦æ±‚è°ƒæ•´æƒé‡æ¯”ä¾‹ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        # feat_loss=0.1, wave_loss=0.7, moe_loss=0.2
        # æ³¨æ„ï¼šwave lossé€šå¸¸è¾ƒå¤§ï¼Œæ‰€ä»¥å®é™…æƒé‡è¦æ›´å°
        # Deprecated static alpha; dynamic weights are applied later.
        # alpha_wave_eff = args.alpha_wave * 0.1
        # å†™å›çŠ¶æ€ï¼ˆæœ¬æ­¥ç”¨"æ—§EMA"ï¼Œåœ¨ç»“å°¾æ›´æ–°EMAï¼‰
        train_one_epoch._wave_bp_ratio = wave_bp_ratio

        # Forward pass (optional AMP autocast)
        # === æ­£ç¡®çš„AMPé…ç½®ï¼šå‰å‘ç”¨fp16ï¼ŒæŸå¤±è®¡ç®—ç”¨float32 ===
        amp_mode    = getattr(args, 'amp', 'none')
        # å…³é”®ä¿®å¤ï¼šAMPåœ¨éæ³¢å½¢é˜¶æ®µä¹Ÿå¯ç”¨ï¼Œåªæ˜¯ä¸è®¡ç®—æ³¢å½¢æŸå¤±
        amp_enabled = (device.type == 'cuda' and amp_mode in ('fp16', 'bf16'))
        amp_dtype   = torch.float16 if amp_mode == 'fp16' else torch.bfloat16
        # æŸå¤±è®¡ç®—å§‹ç»ˆç”¨float32ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        loss_dtype  = torch.float32

        # åˆ›å»ºå›ºå®šçš„dummy CSIï¼ˆå§‹ç»ˆç”¨float32ï¼Œé¿å…dtypeé—®é¢˜ï¼‰
        if not hasattr(train_one_epoch, '_dummy_csi_cache'):
            train_one_epoch._dummy_csi_cache = {
                'snr_db': torch.tensor(15.0, device=device, dtype=torch.float32),
                'fading_onehot': torch.zeros(8, device=device, dtype=torch.float32),
                'ber': torch.tensor(0.001, device=device, dtype=torch.float32)
            }
            train_one_epoch._dummy_csi_cache['fading_onehot'][0] = 1.0

        # å¤åˆ¶åˆ°å½“å‰batch_size
        csi_dict = {
            'snr_db': train_one_epoch._dummy_csi_cache['snr_db'].expand(batch_size),
            'fading_onehot': train_one_epoch._dummy_csi_cache['fading_onehot'].unsqueeze(0).expand(batch_size, -1),
            'ber': train_one_epoch._dummy_csi_cache['ber'].expand(batch_size)
        }

        # å‰å‘ä¼ æ’­ï¼šè¾“å…¥ä¿æŒåŸå§‹dtypeï¼Œåªåœ¨autocastå†…éƒ¨è‡ªåŠ¨è½¬æ¢
        with torch.cuda.amp.autocast(enabled=False):
            z, enc_logs = encoder(x, csi_dict=None, training_step=global_step)

        # 2) è§£ç å™¨ï¼šå¯ç”¨ AMPï¼ˆæ³¨æ„ï¼šä½ çš„ vocoder å†…éƒ¨å·²ç¦ç”¨ autocastï¼‰
        if amp_enabled:
            with torch.autocast(device_type='cuda', dtype=amp_dtype):
                feats = decoder(z, csi_dict=csi_dict, return_wave=False, target_len=None)
        else:
            feats = decoder(z, csi_dict=csi_dict, return_wave=False, target_len=None)

        # 3) ç»Ÿä¸€è½¬å› FP32ï¼Œé¿å…åç»­æ•°å€¼ä¸ç¨³
        z = z.float()
        feats = feats.float()

        # 4) enc_logs ä¸­çš„æ‰€æœ‰å¼ é‡ï¼ˆåŒ…å« dict/list/tuple åµŒå¥—ï¼‰ä¹Ÿè½¬æˆ FP32
        def _to_float32_inplace(obj):
            if isinstance(obj, torch.Tensor):
                return obj.float()
            if isinstance(obj, dict):
                for k, v in obj.items():
                    obj[k] = _to_float32_inplace(v)
                return obj
            if isinstance(obj, (list, tuple)):
                return type(obj)(_to_float32_inplace(v) for v in obj)
            return obj

        enc_logs = _to_float32_inplace(enc_logs)
        # ----------------------------------------------------------

        # ğŸ”§ FIX 1: ç¡®ä¿featsä¿æŒæ¢¯åº¦è¿æ¥ï¼Œé¿å…è®¡ç®—å›¾æ–­è£‚
        if not isinstance(feats, torch.Tensor) or not feats.requires_grad:
            # å°† feats ä¸ z çš„è®¡ç®—å›¾å»ºç«‹ä¾èµ–ï¼Œé¿å…è¢«ä¸Šæ¸¸ detach/no_grad æ–­å¼€
            feats = feats + 0.0 * z.sum()

        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿featså§‹ç»ˆä¿æŒæ¢¯åº¦
        if isinstance(feats, torch.Tensor) and not feats.requires_grad:
            feats = feats.detach().requires_grad_(True)
        # --- Wave / Vocoder branch: only after wave_start_step ---
        y_hat_audio = None  # é»˜è®¤ä¸ç®—æ³¢å½¢
        wave_computed_flag = False  # æ ‡è®°æœ¬batchæ˜¯å¦è®¡ç®—è¿‡æ³¢å½¢

        # æ£€æŸ¥vocoderè°ƒç”¨æ¡ä»¶
        wave_stride = int(getattr(args, 'wave_stride', 1) or 1)
        should_call_vocoder = active_wave and (batch_idx % wave_stride == 0)

        # Helper: consistent DNN pitch -> f0_hz mapping (align with AETHERDecoder)
        def _decode_f0_hz_from_dnn_pitch(pitch_log2: torch.Tensor, sr: float = 16000.0) -> torch.Tensor:
            # AETHER convention: f0_hz = sr * 2**(dnn_pitch - 6.5)
            return (sr * torch.pow(2.0, pitch_log2 - 6.5)).clamp(50.0, 400.0)

        if should_call_vocoder:
            # ç°åœºæ„é€  vocoder æ¡ä»¶ï¼šfeats20 (20-d cepstrum) + period
            # å½“å‰è§£ç å™¨è¾“å‡º feats æ˜¯ FARGAN 36 ç»´ï¼šceps(18) + dnn_pitch(1, idx=18) + frame_corr(1) + lpc(16)

            # === 1) ğŸ”§ FIX 2: æ”¹è¿›teacher forcingç­–ç•¥ï¼Œç¡®ä¿æ¢¯åº¦è·¯å¾„æ¸…æ™° ===
            # é™ä½æœ€å¤§teacher forcingæ¯”ä¾‹ï¼Œå‡å°‘å¯¹GTçš„ä¾èµ–
            tf = max(0.0, min(0.3, 0.3 * (1.0 - warm_ratio)))  # ä»0.5é™ä½åˆ°0.3

            # ä½¿ç”¨detach()æ˜ç¡®æ§åˆ¶æ¢¯åº¦æµï¼šGTéƒ¨åˆ†ä¸å‚ä¸åä¼ ï¼Œé¢„æµ‹éƒ¨åˆ†ä¿æŒæ¢¯åº¦
            if tf > 0.0:
                feats_mix = tf * y.detach() + (1.0 - tf) * feats  # GTéƒ¨åˆ†æ˜¾å¼detach
            else:
                feats_mix = feats  # å®Œå…¨ä½¿ç”¨é¢„æµ‹ç‰¹å¾ï¼Œä¿æŒæ¢¯åº¦

            # === ç›´æ¥ä½¿ç”¨åŸå§‹36ç»´ç‰¹å¾ï¼Œè·³è¿‡Semantic Adapterçº¦æŸ ===
            # å–æ¶ˆè¯­ä¹‰é€‚é…å™¨å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨è§£ç å™¨è¾“å‡ºçš„åŸå§‹ç‰¹å¾
            # semantic_adapter.update_training_stage(global_step)
            # feats_adapted = semantic_adapter(feats_mix, global_step=global_step)

            # é€‚é…å™¨çŠ¶æ€ç›‘æ§ - å·²ç¦ç”¨
            # if batch_idx % 100 == 0:
            #     adapter_status = semantic_adapter.get_status()
            #     tqdm.write(f"      ğŸ”§ Adapter: stage={semantic_adapter.get_stage_name()}, "
            #               f"strength={adapter_status['adaptation_strength']:.2f}")

            # â‘  å€’è°±ç»™ vocoderï¼šç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾çš„å‰20ç»´
            feats20 = feats_mix[..., :20].contiguous()      # [B,T,20] â† ä½¿ç”¨åŸå§‹æœªé€‚é…ç‰¹å¾

            # â‘¡ å‘¨æœŸä¹Ÿç”¨é¢„æµ‹æµçš„ dnn_pitchï¼ˆç¬¬18ç»´ï¼‰
            pred_pitch = feats_mix[..., 18].float()         # [B,T]
            # Use the same mapping as AETHERDecoder to avoid periodic artifacts
            f0_hz = _decode_f0_hz_from_dnn_pitch(pred_pitch)  # [B,T]
            period = (16000.0 / f0_hz).clamp(32.0, 255.0).round().to(torch.long)


            # === 2) æ—¶é—´è½´æ¡¥æ¥ï¼šè‹¥ä¸Šæ¸¸æ˜¯ 50 Hzï¼ˆå¸¸è§ï¼šT_audioâ‰ˆ2*T_featï¼‰ï¼Œåˆ™ä¸Šé‡‡æ ·åˆ° 100 Hz ===
            T_feat = int(feats20.size(1))
            audio_frames = int(audio.size(-1) // 160)  # 100 Hz å¸§
            if audio_frames >= 2 * T_feat - 4:  # ç»éªŒé˜ˆï¼šå½“éŸ³é¢‘å¸§æ•°æ˜¾è‘—å¤§äºç‰¹å¾å¸§æ—¶ï¼Œè®¤å®šä¸º50â†’100
                feats20 = F.interpolate(feats20.permute(0, 2, 1), scale_factor=2.0,
                                        mode="linear", align_corners=False).permute(0, 2, 1).contiguous()
                period = F.interpolate(period.float().unsqueeze(1), scale_factor=2.0,
                                       mode="nearest").squeeze(1).to(torch.long).contiguous()
                T_feat = int(feats20.size(1))

            # === 3) å…ˆåœ¨â€œè¿›å…¥ FARGANCond ä¹‹å‰â€æŠŠæ—¶é—´ç»´å¯¹é½ï¼šfeatures ä¸ period å¿…é¡»åŒé•¿ ===
            T0 = min(T_feat, int(period.size(1)))
            if T0 <= 0:
                tqdm.write(f"      âŒ T0={T0} <= 0, T_feat={T_feat}, period.size(1)={period.size(1)}")
                y_hat_audio = None
            else:
                feats20 = feats20[:, :T0, :].contiguous()
                period  = period[:,  :T0   ].contiguous()

                # === 4) ä¸¥æ ¼æŒ‰ FARGANCond çš„å†…éƒ¨æ”¶ç¼©è®¡ç®—å¯ç”¨å¸§æ•° ===
                # FARGANCond.forward å†…éƒ¨ä¼šï¼š
                #   a) ä¸¢å‰ 2 å¸§ â†’ T-2
                #   b) ä¸å‘¨æœŸå¯¹é½ cat åï¼Œèµ° k=3 çš„ valid å·ç§¯ â†’ å†å‡ 2 å¸§ï¼Œæœ€ç»ˆæœ‰æ•ˆä¸º (T-4)
                # ä¸ºäº†è®© f/p åœ¨ cat å‰â€œæ—¶é—´ç»´å®Œå…¨ç›¸ç­‰â€ï¼Œæˆ‘ä»¬å¯¹å¤–éƒ¨è¾“å…¥ä¹Ÿç»Ÿä¸€ç»™â€œ_nb+4â€é•¿åº¦ï¼Œ
                # è¿™æ ·å†…éƒ¨ä¸¢ 2 å¸§åéƒ½ä¸º (_nb+2)ï¼Œå†ç» k=3 valid å·ç§¯ â†’ (_nb)ï¼ˆåˆšå¥½ç­‰äºç›®æ ‡ nbï¼‰ã€‚
                nb_pre = 2                                 # 2 å¸§é¢„çƒ­ï¼ˆä¸ Stage2 ä¸€è‡´ï¼‰
                pre = audio[..., : nb_pre * 160]           # é¢„çƒ­æ³¢å½¢
                cond_len   = max(0, T0 - 4)                # æ¡ä»¶åˆ†æ”¯æœ‰æ•ˆå¸§ï¼ˆTâ†’T-4ï¼‰
                period_len = max(0, int(period.size(1)) - 4)  # å‘¨æœŸä¹ŸæŒ‰ -4 ä¼°ç®—ï¼Œä»¥â€œç­‰é•¿ +4 è£•é‡â€ç­–ç•¥å–‚å…¥
                target_len = max(0, audio_frames - nb_pre) # å»æ‰é¢„çƒ­åçš„å¯åˆæˆå¸§
                nb_frames  = min(cond_len, period_len, target_len)

                # ä»¥ 5 å¸§ç²’åº¦å¯¹é½ï¼ˆFARGAN å†…æ ¸å¤šä»¥ 5 ä¸ºç²’åº¦ï¼‰
                nb_frames = (nb_frames // 5) * 5

                if nb_frames < 5:
                    tqdm.write(f"      âŒ nb_frames={nb_frames} < 5")
                    y_hat_audio = None
                else:
                    # â€œç­‰é•¿ +4 è£•é‡â€è£ç‰‡ï¼šä¸¤è€…éƒ½è£åˆ° nb+4ï¼Œç¡®ä¿è¿›å…¥ cond_net å‰æ—¶é—´ç»´å®Œå…¨ä¸€è‡´
                    feats20_vc = feats20[:, : nb_frames + 4, :].contiguous()
                    period_vc  = period[:,  : nb_frames + 4   ].contiguous()  # æ³¨æ„ï¼š+4ï¼ˆä¸æ˜¯ +2ï¼‰

                    def _call_vocoder_nb(_nb: int):
                        # â‘  å…ˆè£ç‰‡ï¼ˆç­‰é•¿ +4 è£•é‡ï¼‰
                        f = feats20_vc[:, : _nb + 4, :].contiguous()
                        p = period_vc[:,  : _nb + 4   ].clamp(32, 255).to(torch.long).contiguous()

                        # â‘¡ å†æ‰“å°ä¸€æ¬¡æ€§æ¡¥æ¥æ£€æŸ¥
                        if not hasattr(train_one_epoch, '_vc_checked'):
                            T_in = f.size(1)  # = _nb + 4
                            tqdm.write(f"[VocoderBridge] T_in={T_in}, will request nb={_nb}, "
                                    f"expect cond_len={T_in-4}, pre_frames={nb_pre}")
                            train_one_epoch._vc_checked = True

                        # â‘¢ ğŸ”§ FIX 5: ç»Ÿä¸€FARGANè°ƒç”¨ç­–ç•¥ï¼Œä¿æŒæ¢¯åº¦è¿æ¥ä¸”ä½¿ç”¨ eval æ¨¡å¼é¿å…è®­ç»ƒæ€å™ªå£°
                        prev_mode_fc = None
                        if hasattr(decoder, 'fargan_core'):
                            prev_mode_fc = decoder.fargan_core.training
                            decoder.fargan_core.eval()
                        y_audio, aux = decoder.fargan_core(f, p, _nb, pre=pre)
                        if hasattr(decoder, 'fargan_core') and prev_mode_fc is not None:
                            decoder.fargan_core.train(prev_mode_fc)

                        # ä¼˜åŒ–NaNå¤„ç†ï¼Œä¿æŒæ¢¯åº¦è¿æ¥è€Œéå®Œå…¨æ›¿æ¢
                        if torch.isnan(y_audio).any() or torch.isinf(y_audio).any():
                            # ä½¿ç”¨æ¡ä»¶æ›¿æ¢ä¿æŒæ¢¯åº¦ï¼Œè€Œétorch.nan_to_num
                            y_audio = torch.where(
                                torch.isnan(y_audio) | torch.isinf(y_audio),
                                torch.zeros_like(y_audio),
                                y_audio
                            )
                        return y_audio.float(), aux



                    # === 5) è‡ªé€‚åº”é‡è¯•ï¼ˆæœ€å¤š 4 æ¬¡ï¼‰ï¼šä»â€œåˆæ³• nbâ€ç›´æ¥èµ·æ­¥ï¼Œå¿…è¦æ—¶æŒ‰ 5 å¸§é€’å‡ ===
                    y_hat_audio = None
                    max_tries = 4
                    nb_try = max(5, (int(nb_frames) // 5) * 5)  # åˆæ³• nbï¼ˆå·²è€ƒè™‘ -4 æ”¶ç¼©ï¼‰

                    for try_idx in range(max_tries):
                        try:
                            if not hasattr(train_one_epoch, '_vc_checked'):
                                T_in = int(feats20_vc.size(1))  # é¢„æœŸ = nb_try + 4
                                train_one_epoch._vc_checked = True

                            y_hat_audio, _ = _call_vocoder_nb(nb_try)
                            L = int(y_hat_audio.size(-1))

                            # âœ… æ­£ç¡®çš„æœŸæœ›ï¼švocoder åªè¿”å›â€œé¢„æµ‹æ®µâ€ï¼Œä¸å« pre æ®µ
                            exp_len = max(0, (nb_try - nb_pre) * 160)

                            if L > exp_len:
                                y_hat_audio = y_hat_audio[..., :exp_len]
                            elif L < exp_len:
                                # ç”¨è¿”å›é•¿åº¦åæ¨æœ€å¯èƒ½çš„ nbï¼šframes_pred = L/160ï¼Œnb = pre + frames_pred
                                frames_pred = max(0, L // 160)
                                nb_est = frames_pred + nb_pre
                                # ä»¥ 5 å¸§ç²’åº¦å›é€€
                                nb_back = max(5, (nb_est // 5) * 5)
                                if nb_back < nb_try:
                                    nb_try = nb_back
                                    tqdm.write(f"  ğŸ” vocoder retry: nb_tryâ†’{nb_try}")
                                    y_hat_audio = None
                                    continue

                            break
                        except IndexError as ie:
                            # æç«¯è¶Šç•Œï¼ŒæŒ‰ 5 å¸§é€€
                            tqdm.write(f"      âŒ IndexError in vocoder (try {try_idx+1}/{max_tries}): {str(ie)[:100]}")
                            nb_new = max(5, nb_try - 5)
                            if nb_new >= nb_try:
                                nb_new = nb_try - 5
                            nb_try = nb_new
                            tqdm.write(f"  ğŸ” vocoder retry: nb_tryâ†’{nb_try}")
                        except Exception:
                            # å…¶å®ƒå¼‚å¸¸ä¿ç•™æŠ›å‡ºï¼Œä¾¿äºå®šä½
                            raise

                if y_hat_audio is None:
                    tqdm.write("  âŒ VOCODER FAILURE: All retries failed, skipping wave loss for this batch.")
                    tqdm.write(f"      Final nb_try={nb_try}, nb_frames={nb_frames}, nb_pre={nb_pre}")

        # === Quick validation audio export (pred + original) ===
        snap_every = int(getattr(args, 'val_audio_interval', 0) or 0)
        if snap_every > 0 and y_hat_audio is not None:
            if global_step % snap_every == 0:
                try:
                    out_root = Path(args.output_dir) / 'audio_snaps'
                    out_root.mkdir(parents=True, exist_ok=True)
                    # slice first sample
                    pred = y_hat_audio[0]
                    orig = audio[0]
                    # ensure 1-D
                    if pred.dim() > 1:
                        pred = pred.view(-1)
                    if orig.dim() > 1:
                        orig = orig.view(-1)
                    # clamp length to requested seconds and availability
                    max_len = int(getattr(args, 'val_audio_seconds', 10) * 16000)
                    L = min(pred.numel(), orig.numel(), max_len)
                    pred_np = torch.clamp(pred[:L].detach().cpu(), -1.0, 1.0).numpy()
                    # Optional de-emphasis (preview only)
                    deemph = float(getattr(args, 'val_audio_deemph', 0.85))
                    if deemph > 0.0:
                        y_prev = 0.0
                        for i in range(pred_np.shape[0]):
                            y_prev = float(pred_np[i]) + deemph * y_prev
                            pred_np[i] = y_prev
                    orig_np = torch.clamp(orig[:L].detach().cpu(), -1.0, 1.0).numpy()
                    sf.write(str(out_root / f'step_{global_step:06d}_pred.wav'), pred_np, 16000, subtype='PCM_16')
                    sf.write(str(out_root / f'step_{global_step:06d}_orig.wav'), orig_np, 16000, subtype='PCM_16')

                    # Optional: teacher-forced preview (use GT features -> vocoder)
                    if bool(getattr(args, 'val_audio_teacher', True)):
                        # Build teacher-forced features: ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œè·³è¿‡adapter
                        tf_feats = y.detach()  # [B,T,36] ground-truth features
                        # å¼ºåˆ¶è·³è¿‡semantic adapterï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
                        # if bool(getattr(args, 'val_audio_teacher_no_adapter', False)):
                        tf_adapted = tf_feats  # ç›´æ¥ä½¿ç”¨åŸå§‹GTç‰¹å¾
                        # else:
                        #     tf_adapted = semantic_adapter(tf_feats, global_step=global_step)
                        tf_feats20 = tf_adapted[..., :20].contiguous()
                        # Period from GT pitch (index 18), same mapping as pred
                        tf_pitch = tf_feats[..., 18].float()
                        tf_f0_hz = _decode_f0_hz_from_dnn_pitch(tf_pitch)
                        tf_period = (16000.0 / tf_f0_hz).clamp(32.0, 255.0).round().to(torch.long)

                        # 50->100Hz bridge if needed
                        T_feat_tf = int(tf_feats20.size(1))
                        audio_frames = int(audio.size(-1) // 160)
                        if audio_frames >= 2 * T_feat_tf - 4:
                            tf_feats20 = F.interpolate(tf_feats20.permute(0, 2, 1), scale_factor=2.0,
                                                       mode="linear", align_corners=False).permute(0, 2, 1).contiguous()
                            tf_period = F.interpolate(tf_period.float().unsqueeze(1), scale_factor=2.0,
                                                      mode="nearest").squeeze(1).to(torch.long).contiguous()
                            T_feat_tf = int(tf_feats20.size(1))

                        # Align lengths using the same nb logic
                        nb_pre = 0 if bool(getattr(args, 'val_audio_no_preheat', True)) else 2
                        pre_wav = audio[..., : nb_pre * 160] if nb_pre > 0 else None
                        cond_len = max(0, T_feat_tf - 4)
                        period_len = max(0, int(tf_period.size(1)) - 4)
                        target_len = max(0, audio_frames - nb_pre)
                        nb_frames = min(cond_len, period_len, target_len)
                        nb_frames = (nb_frames // 5) * 5
                        tf_audio_np = None
                        if nb_frames >= 5:
                            tf_f = tf_feats20[:, : nb_frames + 4, :].contiguous()
                            tf_p = tf_period[:,  : nb_frames + 4   ].clamp(32, 255).to(torch.long).contiguous()
                        try:
                            tf_audio, _ = decoder.fargan_core(tf_f, tf_p, nb_frames, pre=pre_wav)
                            # clip to requested seconds too
                            Ltf = min(int(tf_audio.size(-1)), max_len)
                            tf_np = torch.clamp(tf_audio[0, :Ltf].detach().cpu(), -1.0, 1.0).numpy()
                            sf.write(str(out_root / f'step_{global_step:06d}_teacher.wav'), tf_np, 16000, subtype='PCM_16')
                        except RuntimeError as _e:
                            msg = str(_e).lower()
                            if 'out of memory' in msg and bool(getattr(args, 'val_audio_teacher', True)):
                                # Optional CPU fallback for preview-only path
                                try:
                                    tqdm.write("    âš ï¸ CUDA OOM on teacher preview, falling back to CPU (short clip)")
                                    orig_dev = next(decoder.fargan_core.parameters()).device
                                    # Shorten nb for CPU preview to reduce latency
                                    nb_cpu = max(5, min(nb_frames, int((max_len // 160) + nb_pre)))
                                    tf_f_cpu = tf_f[:1, : nb_cpu + 4, :].contiguous().cpu()
                                    tf_p_cpu = tf_p[:1, : nb_cpu + 4].contiguous().cpu()
                                    pre_cpu  = pre_wav[:1].contiguous().cpu()
                                    decoder.fargan_core.to('cpu')
                                    with torch.no_grad():
                                        tf_audio_cpu, _ = decoder.fargan_core(tf_f_cpu, tf_p_cpu, nb_cpu, pre=pre_cpu)
                                    Ltf = min(int(tf_audio_cpu.size(-1)), max_len)
                                    tf_np = torch.clamp(tf_audio_cpu[0, :Ltf], -1.0, 1.0).numpy()
                                    if deemph > 0.0:
                                        y_prev = 0.0
                                        for i in range(tf_np.shape[0]):
                                            y_prev = float(tf_np[i]) + deemph * y_prev
                                            tf_np[i] = y_prev
                                    sf.write(str(out_root / f'step_{global_step:06d}_teacher.wav'), tf_np, 16000, subtype='PCM_16')
                                except Exception as _e2:
                                    tqdm.write(f"    âš ï¸ CPU fallback failed: {_e2}")
                                finally:
                                    # move vocoder back to original device
                                    decoder.fargan_core.to(orig_dev)
                            else:
                                tqdm.write(f"    âš ï¸ Failed to synth teacher-forced preview: {_e}")
                        except Exception as _e:
                            tqdm.write(f"    âš ï¸ Failed to synth teacher-forced preview: {_e}")
                        else:
                            # Add de-emphasis for GPU path too (after writing raw, overwrite)
                            try:
                                # Force eval mode for preview to match generate_10s_audio.py behavior
                                prev_mode_fc = decoder.fargan_core.training if hasattr(decoder, 'fargan_core') else None
                                if hasattr(decoder, 'fargan_core'):
                                    decoder.fargan_core.eval()
                                tf_audio, _ = decoder.fargan_core(tf_f, tf_p, nb_frames, pre=pre_wav)
                                Ltf = min(int(tf_audio.size(-1)), max_len)
                                tf_np = torch.clamp(tf_audio[0, :Ltf].detach().cpu(), -1.0, 1.0).numpy()
                                if deemph > 0.0:
                                    y_prev = 0.0
                                    for i in range(tf_np.shape[0]):
                                        y_prev = float(tf_np[i]) + deemph * y_prev
                                        tf_np[i] = y_prev
                                sf.write(str(out_root / f'step_{global_step:06d}_teacher.wav'), tf_np, 16000, subtype='PCM_16')
                                if hasattr(decoder, 'fargan_core') and prev_mode_fc is not None:
                                    decoder.fargan_core.train(prev_mode_fc)
                            except Exception:
                                pass

                    tqdm.write(f"    ğŸ§ Saved preview audio at step {global_step} ({L/16000.0:.1f}s)")
                    # Force sync to surface CUDA errors near preview instead of later
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.synchronize()
                        except Exception:
                            pass
                except Exception as e:
                    tqdm.write(f"    âš ï¸ Failed to save preview audio: {e}")

        # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
        if feats.size(1) != y.size(1):
            min_len = min(feats.size(1), y.size(1))
            feats = feats[:, :min_len, :]
            y = y[:, :min_len, :]


        # Stage1-like warm start: ignore first N frames for loss
        fs = max(0, int(getattr(args, 'preheat_frames', 0)))
        if fs > 0 and feats.size(1) > fs:
            feats_loss = feats[:, fs:, :]
            y_loss = y[:, fs:, :]
        else:
            feats_loss, y_loss = feats, y

        # === Fused Loss Computation - æŸå¤±è®¡ç®—ç”¨float32 ===

        # åˆå§‹åŒ–æ‰€æœ‰æŸå¤±ç»„ä»¶ï¼Œç”¨float32ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        l_feat = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_wave = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_moe  = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_rate = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_sem  = torch.tensor(0.0, device=device, dtype=loss_dtype)

        moe_metrics = {}

        # 1. Feature reconstruction lossï¼ˆè½¬æ¢ä¸ºfloat32è¿›è¡Œç¨³å®šè®¡ç®—ï¼‰
        feats_loss_safe = feats_loss.float()
        y_loss_safe     = y_loss.float()


        # æ£€æŸ¥ç‰¹å¾é‡å»ºè¾“å…¥æ˜¯å¦æœ‰å¼‚å¸¸
        if torch.isnan(feats_loss_safe).any() or torch.isinf(feats_loss_safe).any():
            tqdm.write(f"    ğŸš¨ NaN/Inf in decoded features before loss computation!")
            tqdm.write(f"      feats_loss range: [{feats_loss_safe.min().item():.6f}, {feats_loss_safe.max().item():.6f}]")
            feats_loss_safe = torch.where(torch.isnan(feats_loss_safe) | torch.isinf(feats_loss_safe),
                                        torch.zeros_like(feats_loss_safe), feats_loss_safe)

        if torch.isnan(y_loss_safe).any() or torch.isinf(y_loss_safe).any():
            tqdm.write(f"    ğŸš¨ NaN/Inf in target features before loss computation!")
            tqdm.write(f"      y_loss range: [{y_loss_safe.min().item():.6f}, {y_loss_safe.max().item():.6f}]")
            y_loss_safe = torch.where(torch.isnan(y_loss_safe) | torch.isinf(y_loss_safe),
                                    torch.zeros_like(y_loss_safe), y_loss_safe)

        # === ç‰¹å¾ç»Ÿè®¡æ‰“å°ï¼šå®¡æŸ¥ç‰¹å¾é‡å»ºæ•ˆæœ ===
        if global_step % 20 == 0:  # æ¯20ä¸ªstepæ‰“å°ä¸€æ¬¡
            _print_feature_reconstruction_stats(feats_loss_safe, y_loss_safe, global_step, batch_idx)

        l_feat = F.mse_loss(feats_loss_safe, y_loss_safe)

        # Add layered loss if enabled
        if stage_cfg.layered_enabled(global_step):
            layered_loss, _, _ = compute_layered_loss(
                feats, y,
                current_step=global_step,
                feature_spec_type='fargan'
            )
            l_feat = l_feat + layered_loss.float()

        # 2. Wave loss (ç»§æ‰¿Stage2æƒé‡è°ƒåº¦)
        if y_hat_audio is not None and wave_bp_ratio > 0.0:
            # å¯¹é½wave lossçš„å…³æ³¨åŒºåŸŸï¼šè£å‰ªæ‰å‰ fs å¸§å¯¹åº”çš„æ ·æœ¬æ•°
            wave_audio_target = audio
            wave_audio_pred = y_hat_audio
            if fs > 0:
                sample_off = fs * 160  # 16kHz, 10ms hop
                if y_hat_audio.size(-1) > sample_off:
                    wave_audio_pred = y_hat_audio[..., sample_off:]
                if audio.size(-1) > sample_off:
                    wave_audio_target = audio[..., sample_off:]

            # â‘  ä¿®å¤æ¢¯åº¦ä¼ æ’­ï¼šç§»é™¤detachæ“ä½œï¼Œé¿å…äººä¸ºå‰Šå¼±FARGANæ¢¯åº¦
            # é—®é¢˜åˆ†æï¼šdetach()å¯¼è‡´FARGANæ¢¯åº¦åªæœ‰50%ï¼Œé€ æˆæ¢¯åº¦ä¸å¹³è¡¡
            wave_audio_pred_bp = wave_audio_pred  # ç›´æ¥ä¼ æ’­ï¼Œä¸å†äººä¸ºå‰Šå¼±æ¢¯åº¦
            # æ ‡è®°å·²è®¡ç®—æ³¢å½¢
            wave_computed_flag = True
            # â‘¡ è®¡ç®—wave lossçš„çª—å£å¤§å° - ä½¿ç”¨æ›´å¤§çš„åŸºç¡€çª—å£
            # ä¿®å¤ï¼šç¡®ä¿æœ€å°çª—å£è‡³å°‘80%ï¼Œé¿å…å› bp_ratioè¿‡å°è€Œå¯¼è‡´çª—å£å¤ªå°
            window_ratio = max(0.8, 0.6 + 0.4 * wave_bp_ratio)  # æœ€å°80%ï¼Œæœ€å¤§100%
            m = int(min(
                wave_audio_pred_bp.size(-1),
                window_ratio * wave_audio_pred_bp.size(-1)
            ))
            # Cap wave loss window by seconds to reduce memory
            max_wave_len = int(float(getattr(args, 'wave_loss_seconds', 6.0)) * 16000)
            m = min(m, max_wave_len)
            wave_pred_head = wave_audio_pred_bp[..., :m]
            wave_tgt_head  = wave_audio_target[..., :m]
            # Release large tensors early
            try:
                del y_hat_audio
            except Exception:
                pass
            # â‘¢ ä½¿ç”¨FARGANæ ‡å‡†waveæŸå¤±å‡½æ•°
            # ğŸ”¥ æ¢å¤fargan_wave_lossesï¼Œç§»é™¤è‡ªå®šä¹‰audio_usability_loss
            # ç¡®ä¿periodç»´åº¦æ­£ç¡®å¯¹é½
            audio_frames = wave_pred_head.size(-1) // 160
            if period.size(1) > audio_frames:
                period_aligned = period[:, :audio_frames]
            else:
                # å¦‚æœperiodä¸å¤Ÿé•¿ï¼Œæ‰©å±•åˆ°æ‰€éœ€é•¿åº¦
                period_aligned = period.repeat(1, (audio_frames // period.size(1)) + 1)[:, :audio_frames]

            # === ğŸ”„ ä½¿ç”¨FARGANæ ‡å‡†æ³¢å½¢æŸå¤± ===
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(wave_pred_head).any() or torch.isinf(wave_pred_head).any():
                tqdm.write(f"    WARNING: wave_pred_head contains NaN/Inf")
                wave_pred_head = torch.clamp(wave_pred_head, -1.0, 1.0)

            if torch.isnan(wave_tgt_head).any() or torch.isinf(wave_tgt_head).any():
                tqdm.write(f"    WARNING: wave_tgt_head contains NaN/Inf")
                wave_tgt_head = torch.clamp(wave_tgt_head, -1.0, 1.0)

            # å¼ºåˆ¶ä½¿ç”¨fp32è®¡ç®—FARGANæ³¢å½¢æŸå¤±
            with torch.autocast(device_type='cuda', enabled=False):
                try:
                    l_wave_raw, fargan_details = fargan_wave_losses(
                        wave_pred_head.float(),
                        wave_tgt_head.float(),
                        period_aligned,
                        comprehensive_weight=0.0,  # å¯è°ƒæ•´
                        original_weight=0.0,       # å¯è°ƒæ•´
                        train_weights=None,        # ä½¿ç”¨é»˜è®¤æƒé‡
                        device=device
                    )
                except Exception as e:
                    tqdm.write(f"    ERROR in fargan_wave_losses: {e}")
                    l_wave_raw = torch.tensor(0.1, device=device, dtype=loss_dtype, requires_grad=True)
                    fargan_details = {'primary': l_wave_raw}

            # ğŸ”§ ä¿å­˜fargan_detailsä¾›åç»­æƒé‡è°ƒæ•´ä½¿ç”¨
            train_one_epoch._last_fargan_details = fargan_details

            # â‘£ æœ‰æ•ˆæƒé‡ï¼ˆæ³¨æ„ï¼šè¿™é‡Œçš„alpha_wave_effè¿˜æ˜¯æ—§çš„å€¼ï¼Œç¨åä¼šè¢«æ–°æƒé‡è¦†ç›–ï¼‰
            l_wave = l_wave_raw  # æš‚æ—¶ä¸åº”ç”¨æƒé‡ï¼Œç­‰å¾…åŠ¨æ€æƒé‡è®¡ç®—

            # RMSè´¨é‡æ£€æŸ¥ï¼ˆæ‰¹é‡æ£€æŸ¥ï¼Œå‡å°‘æ‰“å°é¢‘ç‡ï¼‰
            # ä½¿ç”¨å·²è£å‰ªçš„ wave_pred_headï¼Œé¿å…å¯¹å®Œæ•´ y_hat_audio è¿›è¡Œå¤§è§„æ¨¡å½’çº¦å ç”¨æ˜¾å­˜
            try:
                pred_rms = torch.sqrt(wave_pred_head.float().pow(2).mean(dim=-1) + 1e-8)
            except RuntimeError:
                # GPU OOM å›é€€åˆ° CPU è®¡ç®—
                pred_rms = torch.sqrt(wave_pred_head.detach().float().cpu().pow(2).mean(dim=-1) + 1e-8)
            pred_rms_db = 20.0 * torch.log10(pred_rms.mean() + 1e-8)

            # è®°å½•ä½RMSäº‹ä»¶ï¼Œä½†å‡å°‘æ‰“å°é¢‘ç‡
            if not hasattr(train_one_epoch, '_low_rms_count'):
                train_one_epoch._low_rms_count = 0
                train_one_epoch._last_rms_report = 0

            if pred_rms_db < -40.0:
                train_one_epoch._low_rms_count += 1
                # æ¯50æ¬¡ä½RMSäº‹ä»¶æ‰æŠ¥å‘Šä¸€æ¬¡
                if train_one_epoch._low_rms_count - train_one_epoch._last_rms_report >= 50:
                    tqdm.write(f"      Low RMS: {pred_rms_db:.1f} dB (occurred {train_one_epoch._low_rms_count} times)")
                    train_one_epoch._last_rms_report = train_one_epoch._low_rms_count

        # 3. Rate loss (Stage3é»˜è®¤ç¦ç”¨)
        l_rate = torch.tensor(0.0, device=device, dtype=loss_dtype)

        # 3.5. åŒè·¯å¾„ç‰¹å¾åˆ†ç¦»æŸå¤±ï¼š20ç»´å£°å­¦ç‰¹å¾ + 16ç»´è¯­ä¹‰ç‰¹å¾
        # æŒ‰ç…§é‡æ–°è®¾è®¡çš„æ¡†æ¶ï¼šç‰©ç†éš”ç¦»ï¼Œç‹¬ç«‹ä¼˜åŒ–
        l_acoustic = torch.tensor(0.0, device=device, dtype=loss_dtype)
        l_semantic = torch.tensor(0.0, device=device, dtype=loss_dtype)

        # ä½¿ç”¨å½“å‰å¯¹é½åçš„ç‰¹å¾å¼ é‡ feats/y è®¡ç®—åˆ†ç¦»æŸå¤±
        if feats is not None:  # åªæœ‰åœ¨æˆåŠŸè§£ç å‡ºç‰¹å¾æ—¶æ‰è®¡ç®—åˆ†ç¦»æŸå¤±
            # æå–åŸå§‹éŸ³é¢‘çš„16ç»´è¯­ä¹‰ç‰¹å¾ä½œä¸ºç›®æ ‡
            try:
                # ç¡®ä¿éŸ³é¢‘åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œæ ¼å¼ä¸Š
                audio_for_semantic = audio.detach().float().to(device)
                # é€šè¿‡ç¼“å­˜çš„æå–å™¨ï¼ˆç”±mainæ³¨å…¥ï¼‰
                sem_ext = getattr(train_one_epoch, '_semantic_extractor', None)
                if sem_ext is None:
                    raise RuntimeError('semantic_extractor not initialized')
                with torch.no_grad():  # è¯­ä¹‰æå–ä¸éœ€è¦æ¢¯åº¦
                    semantic_target = sem_ext(audio_for_semantic, target_frames=feats.size(1))  # [B,T,16]

                # æå–é¢„æµ‹ç‰¹å¾ä¸­çš„20ç»´å£°å­¦éƒ¨åˆ†å’Œ16ç»´è¯­ä¹‰éƒ¨åˆ†
                acoustic_pred = feats[..., :20]      # [B,T,20] å‰20ç»´ï¼šå€’è°±+F0+ç›¸å…³æ€§
                acoustic_target = y[..., :20]        # [B,T,20] å¯¹åº”çš„GTå£°å­¦ç‰¹å¾

                semantic_pred = feats[..., 20:36]    # [B,T,16] å16ç»´ï¼šè¯­ä¹‰ç‰¹å¾åˆ†é‡
                # semantic_targetå·²åœ¨ä¸Šé¢è®¡ç®—      # [B,T,16] SSLæå–çš„è¯­ä¹‰ç›®æ ‡

                # è®¡ç®—åˆ†ç¦»æŸå¤±ï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
                l_acoustic = F.mse_loss(acoustic_pred.float(), acoustic_target.float())
                l_semantic = F.mse_loss(semantic_pred.float(), semantic_target.float())

                # è®°å½•åˆ†ç¦»æŸå¤±ç»Ÿè®¡
                if batch_idx % 20 == 0:  # æ¯20æ­¥è®°å½•ä¸€æ¬¡
                    with torch.no_grad():
                        acoustic_mse = l_acoustic.item()
                        semantic_mse = l_semantic.item()
                        tqdm.write(f"    åˆ†ç¦»æŸå¤±: å£°å­¦MSE={acoustic_mse:.6f}, è¯­ä¹‰MSE={semantic_mse:.6f}")
                        # è¿½åŠ å16ç»´è¯­ä¹‰ç‰¹å¾çš„ç»´åº¦çº§ç»Ÿè®¡
                        try:
                            _print_semantic_alignment_stats(semantic_pred.float(), semantic_target.float(), global_step, batch_idx)
                        except Exception:
                            pass

            except Exception as e:
                tqdm.write(f"    è¯­ä¹‰ç‰¹å¾æå–å¤±è´¥: {e}")
                l_semantic = torch.tensor(0.0, device=device, dtype=loss_dtype)

        # 4. Semantic proxy loss (encoder semantic head vs acoustic priors)
        if isinstance(enc_logs, dict) and 'semantic_pred' in enc_logs and args.alpha_sem > 0:
            sem_pred = enc_logs['semantic_pred']  # [B,T,6]
            sem_pred_avg = sem_pred.mean(dim=1)   # [B,6]
            # ç¡®ä¿extract_acoustic_priorsçš„è¾“å…¥æ˜¯float32
            sem_target = extract_acoustic_priors(y.float())
            l_sem = args.alpha_sem * F.mse_loss(sem_pred_avg.float(), sem_target.float())


        # 5. MoE auxiliary losses â€”â€” ç›´æ¥è¿›å›¾ï¼Œå‚ä¸åä¼ 
        l_moe = torch.tensor(0.0, device=device, dtype=loss_dtype)
        moe_metrics = {}

        if isinstance(enc_logs, dict):
            # è®°å½•åˆ©ç”¨ç‡ï¼ˆåªåœ¨éœ€è¦æ—¶ï¼Œæ— æ¢¯åº¦ï¼‰
            util_interval = max(50, int(getattr(args, 'log_interval', 50)))
            if hasattr(encoder, 'moe') and encoder.moe is not None and (batch_idx == 0 or batch_idx % util_interval == 0):
                with torch.no_grad():
                    try:
                        expert_util = encoder.moe.get_expert_utilization()
                        moe_metrics['expert_usage_min'] = expert_util.min().item()
                        moe_metrics['expert_usage_max'] = expert_util.max().item()
                        moe_metrics['expert_entropy']   = -(expert_util * torch.log(expert_util + 1e-8)).sum().item()

                        # æ„å»ºå¸¦ä¸“å®¶åç§°çš„ä½¿ç”¨ç‡å­—ç¬¦ä¸²
                        expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"]
                        expert_usage_named = []
                        for i, u in enumerate(expert_util):
                            name = expert_names[i] if i < len(expert_names) else f"E{i}"
                            expert_usage_named.append(f"{name}:{u.item():.3f}")
                        moe_metrics['expert_usage_all'] = ', '.join(expert_usage_named)
                    except Exception:
                        pass

            # **å…³é”®ï¼šæŒ‰ CLI æƒé‡å¹¶å…¥ lossï¼ˆä¿æŒ float32ï¼Œå‚ä¸åä¼ ï¼‰**
            mb = enc_logs.get('moe_balance_loss', None)
            mt = enc_logs.get('moe_token_balance_loss', None)

            # è°ƒè¯•MoEæŸå¤±è®¡ç®— + è·¯ç”±å­¦ä¹ æˆæœå±•ç¤º
            if batch_idx == 0 or batch_idx % 50 == 0:
                moe_w = getattr(args, 'moe_w', 0.05)
                moe_token_w = getattr(args, 'moe_token_w', 0.02)

                # ç²¾ç®€MoEå…³é”®æŒ‡æ ‡ç›‘æ§
                if hasattr(encoder, 'moe'):
                    try:
                        expert_util = encoder.moe.get_expert_utilization()
                        expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"]
                        # æ„å»ºå¸¦ä¸“å®¶åç§°çš„ä½¿ç”¨ç‡æ˜¾ç¤º
                        expert_usage_named = []
                        for i, util in enumerate(expert_util):
                            name = expert_names[i] if i < len(expert_names) else f"E{i}"
                            expert_usage_named.append(f"{name}:{util:.3f}")
                        tqdm.write(f"    [MoE] Expert usage: [{', '.join(expert_usage_named)}]")

                        # æ€§èƒ½æ¯”è¾ƒï¼ˆæœ€å…³é”®æŒ‡æ ‡ï¼‰
                        if hasattr(encoder.moe, 'performance_ratio'):
                            perf_ratio = encoder.moe.performance_ratio
                            status = "Learning" if perf_ratio > 1.5 else "Competitive"
                            tqdm.write(f"    [MoE] Performance: {status} ({perf_ratio:.2f}x vs direct)")

                    except Exception as e:
                        tqdm.write(f"    [MoE] Analysis failed: {e}")

            if isinstance(mb, torch.Tensor):
                # ç®€åŒ–MoEæŸå¤±è®¡ç®—ï¼šç»Ÿä¸€æƒé‡ï¼Œä¸“æ³¨å¹³è¡¡æ€§
                expert_util = None
                try:
                    expert_util = encoder.moe.get_expert_utilization() if hasattr(encoder, 'moe') else None
                except Exception:
                    expert_util = None

                if expert_util is not None:
                    # å°†è®¡ç®—æŒªåˆ°CPUï¼Œè§„é¿å¼‚æ­¥CUDAé”™è¯¯åœ¨æ­¤å¤„æŠ›å‡º
                    eu_cpu = expert_util.detach().cpu()
                    min_util = float(eu_cpu.min().item())
                    max_util = float(eu_cpu.max().item())
                    util_variance = float(eu_cpu.var().item())

                    # åŸºäºæ–¹å·®çš„å¹³è¡¡æƒé‡ï¼šæ–¹å·®è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜
                    balance_multiplier = 1.0 + min(util_variance * 10.0, 3.0)  # 1.0-4.0å€

                    if batch_idx == 0 or batch_idx % 50 == 0:
                        tqdm.write(f"    Expert Util: min={min_util:.3f}, max={max_util:.3f}, var={util_variance:.3f}")
                else:
                    balance_multiplier = 1.5

                moe_contrib = float(getattr(args, 'moe_w', 0.1)) * balance_multiplier * mb.float()
                l_moe = l_moe + moe_contrib
                if batch_idx == 0 or batch_idx % 50 == 0:
                    tqdm.write(f"    Balance Loss: {moe_contrib.item():.6f} (Ã—{balance_multiplier:.1f})")

            if isinstance(mt, torch.Tensor):
                # Tokençº§åˆ«æŸå¤±ï¼šé€‚åº¦æƒé‡ï¼Œé¼“åŠ±ç»†ç²’åº¦è·¯ç”±
                token_contrib = float(getattr(args, 'moe_token_w', 0.05)) * 2.0 * mt.float()
                l_moe = l_moe + token_contrib
                if batch_idx == 0 or batch_idx % 50 == 0:
                    tqdm.write(f"    Token Loss: {token_contrib.item():.6f}")

            # å…¶ä»–å¯èƒ½çš„ MoE çº¦æŸï¼ˆå¦‚ä¸€è‡´æ€§ç­‰ï¼‰ï¼Œç»Ÿä¸€ç”¨è¾ƒå°æƒé‡
            for k, v in enc_logs.items():
                if not isinstance(v, torch.Tensor) or (k in ('moe_balance_loss','moe_token_balance_loss')):
                    continue
                if k.startswith('moe_') and v.requires_grad:
                    l_moe = l_moe + 0.05 * v.float()

            # ç›‘ç£è·¯ç”±æš–å¯åŠ¨ï¼š
            # - è‹¥æ‰¹æ¬¡åŒ…å« per-sample 'expert_class'ï¼Œåˆ™æŒ‰æ ·æœ¬ç›‘ç£ï¼ˆæ›´ç²¾ç»†ï¼‰
            # - å¦åˆ™é€€åŒ–ä¸ºæ•´æ‰¹åŒä¸€ focus çš„ sample-level ç›‘ç£
            if getattr(args, 'router_sup', 0.0) > 0.0 and hasattr(encoder, 'moe') and encoder.moe is not None:
                expert_map = {
                    'harmonic': 0,
                    'transient': 1,
                    'burst_inpaint': 2,
                    'low_snr': 3,
                }
                # å–å‡ºæœ€è¿‘ä¸€æ¬¡å‰å‘çš„ sample-level router logitsï¼ˆç”±å¢å¼ºMoEå†…éƒ¨ç¼“å­˜ï¼‰
                logits = None
                try:
                    if hasattr(encoder.moe, 'specialized_moe') and hasattr(encoder.moe.specialized_moe, '_last_router_logits'):
                        logits = encoder.moe.specialized_moe._last_router_logits
                    elif hasattr(encoder.moe, '_last_router_logits'):
                        logits = encoder.moe._last_router_logits
                except Exception:
                    logits = None

                if logits is not None and torch.is_tensor(logits):
                    # ç›®æ ‡æ ‡ç­¾ï¼šä¼˜å…ˆ per-sample æ ‡ç­¾ï¼Œå…¶æ¬¡ä½¿ç”¨ --expert-focus
                    if isinstance(batch, dict) and ('expert_class' in batch):
                        target_id = batch['expert_class'].to(device=logits.device, dtype=torch.long)
                        focus_tag = 'per-sample'
                    else:
                        focus_name = str(getattr(args, 'expert_focus', '')).strip().lower()
                        if focus_name not in expert_map:
                            target_id = None
                        else:
                            target_id = torch.full((logits.size(0),), expert_map[focus_name], device=logits.device, dtype=torch.long)
                        focus_tag = focus_name or 'none'

                    if target_id is not None:
                        # çº¿æ€§è¡°å‡ï¼šåœ¨ router_sup_decay_steps å†…ä»sup_wåˆ°0.1*sup_w
                        sup_w = float(getattr(args, 'router_sup', 0.0))
                        decay_steps = int(getattr(args, 'router_sup_decay_steps', 0) or 0)
                        if decay_steps > 0:
                            pdec = min(1.0, float(global_step) / float(decay_steps))
                            sup_w = sup_w * (1.0 - 0.9 * pdec)
                        sup_loss = F.cross_entropy(logits.float(), target_id)
                        l_moe = l_moe + sup_w * sup_loss.to(l_moe.dtype)
                        if batch_idx == 0 or batch_idx % 50 == 0:
                            tqdm.write(f"    RouterSup CE: {sup_loss.item():.6f} (w={sup_w:.3f}, focus={focus_tag})")


        # === ğŸ”¥ æ–°çš„ä¸‰é˜¶æ®µè¯­ä¹‰æ„ŸçŸ¥æƒé‡ç­–ç•¥ï¼ˆå®Œå…¨æ›¿æ¢æ—§è°ƒåº¦ï¼‰ ===
        # æ ¹æ®è®­ç»ƒæ­¥æ•°å’Œè´¨é‡çŠ¶å†µåŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡

        # åŸºç¡€é˜¶æ®µæƒé‡åˆ†é… - å¼ºåŒ–"é‡è¯­ä¹‰ï¼Œè½»ç‰¹å¾é‡å»º"ç­–ç•¥
        if global_step < 1000:
            # Foundationé˜¶æ®µï¼šå»ºç«‹è¯­ä¹‰åŸºç¡€ï¼Œé€‚åº¦ç‰¹å¾å­¦ä¹ 
            base_weights = {"feat": 0.3, "wave": 0.4, "moe": 0.3}
            stage_name = "Foundation"
        elif global_step < 5000:
            # Balancedé˜¶æ®µï¼šè¿›ä¸€æ­¥é™ä½ç‰¹å¾é‡å»ºæƒé‡
            base_weights = {"feat": 0.15, "wave": 0.55, "moe": 0.3}
            stage_name = "Balanced"
        else:
            # Qualityé˜¶æ®µï¼šæå¤§é™ä½ç‰¹å¾é‡å»ºæƒé‡ï¼Œä¸“æ³¨è¯­ä¹‰éŸ³é¢‘è´¨é‡
            base_weights = {"feat": 0.05, "wave": 0.75, "moe": 0.2}
            stage_name = "Quality"

        # === ğŸ§  æ™ºèƒ½çš„è‡ªé€‚åº”æƒé‡è°ƒæ•´ç­–ç•¥ ===
        # åŸºäºå¤šç»´åº¦éŸ³é¢‘è´¨é‡æŒ‡æ ‡çš„åŠ¨æ€æƒé‡è°ƒæ•´
        quality_score = 0.5  # é»˜è®¤ä¸­æ€§è´¨é‡è¯„åˆ†
        detailed_quality_metrics = {
            'audibility': 0.5,
            'intelligibility': 0.5,
            'quality': 0.5
        }

        # è·å–è¯¦ç»†çš„è´¨é‡è¯„ä¼°ä¿¡æ¯
        if hasattr(train_one_epoch, '_last_fargan_details'):
            fargan_details = getattr(train_one_epoch, '_last_fargan_details', {})

            # è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆåŸºäºFARGANæŸå¤±ï¼‰
            primary_loss = fargan_details.get('primary', torch.tensor(1.0))
            if isinstance(primary_loss, torch.Tensor):
                raw_total = primary_loss.item()
            else:
                raw_total = float(primary_loss)
            quality_score = max(0.0, min(1.0, 1.0 - raw_total))

            # åŸºäºFARGANæŸå¤±ç»„ä»¶è®¡ç®—è´¨é‡å¾—åˆ†
            detailed_quality_metrics.update({
                'audibility': quality_score,      # åŸºäºprimary loss
                'intelligibility': quality_score,  # åŸºäºprimary loss
                'quality': quality_score           # åŸºäºprimary loss
            })

        # å¤šå±‚æ¬¡æƒé‡è°ƒæ•´ç­–ç•¥
        # 1. åŸºäºç»¼åˆè´¨é‡çš„ä¸»è¦è°ƒæ•´
        if quality_score < 0.2:  # è´¨é‡ä¸¥é‡ä¸è¶³
            # æ¿€è¿›åœ°å¢å¼ºéŸ³é¢‘æŸå¤±æƒé‡
            wave_boost = 2.0
            feat_reduction = 0.6
        elif quality_score < 0.4:  # è´¨é‡è¾ƒå·®
            # é€‚åº¦å¢å¼ºéŸ³é¢‘æŸå¤±æƒé‡
            wave_boost = 1.5
            feat_reduction = 0.8
        elif quality_score > 0.8:  # è´¨é‡å¾ˆå¥½
            # å¯ä»¥æ›´æ³¨é‡è¯­ä¹‰ä¿æŒ
            wave_boost = 0.8
            feat_reduction = 1.3
        elif quality_score > 0.9:  # è´¨é‡ä¼˜ç§€
            # å¤§åŠ›åŠ å¼ºè¯­ä¹‰å­¦ä¹ 
            wave_boost = 0.6
            feat_reduction = 1.5
        else:  # è´¨é‡ä¸­ç­‰
            # ä¿æŒåŸºç¡€æƒé‡å¹³è¡¡
            wave_boost = 1.0
            feat_reduction = 1.0

        # 2. åŸºäºç‰¹å®šæŒ‡æ ‡çš„å¾®è°ƒ
        # å¯å¬æ€§å·®æ—¶ï¼Œä¼˜å…ˆä¿®å¤åŸºç¡€éŸ³é¢‘é—®é¢˜
        if detailed_quality_metrics['audibility'] < 0.3:
            wave_boost *= 1.2

        # æ¸…æ™°åº¦å·®æ—¶ï¼Œå¹³è¡¡éŸ³é¢‘å’Œç‰¹å¾æƒé‡
        if detailed_quality_metrics['intelligibility'] < 0.3:
            wave_boost *= 1.1
            feat_reduction *= 0.9

        # æ„ŸçŸ¥è´¨é‡å·®æ—¶ï¼Œæ³¨é‡éŸ³é¢‘ç²¾ç»†åŒ–
        if detailed_quality_metrics['quality'] < 0.3:
            wave_boost *= 1.15

        # åº”ç”¨æƒé‡è°ƒæ•´
        base_weights["wave"] *= wave_boost
        base_weights["feat"] *= feat_reduction

        # 3. è®­ç»ƒé˜¶æ®µç‰¹å®šçš„æƒé‡ä¿æŠ¤
        # Foundationé˜¶æ®µï¼šç¡®ä¿åŸºç¡€åŠŸèƒ½ä¸è¢«è¿‡åº¦å‰Šå¼±
        if stage_name == "Foundation":
            base_weights["wave"] = min(base_weights["wave"], 0.4)  # é™åˆ¶æ³¢å½¢æŸå¤±æƒé‡
            base_weights["feat"] = max(base_weights["feat"], 0.4)  # ä¿è¯ç‰¹å¾å­¦ä¹ 
        # Qualityé˜¶æ®µï¼šç¡®ä¿éŸ³é¢‘è´¨é‡ä¼˜å…ˆ
        elif stage_name == "Quality":
            base_weights["wave"] = max(base_weights["wave"], 0.3)  # ä¿è¯éŸ³é¢‘å…³æ³¨åº¦
            base_weights["feat"] = min(base_weights["feat"], 0.5)  # é™åˆ¶ç‰¹å¾é‡å»º

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(base_weights.values())
        alpha_feat_eff = base_weights["feat"] / total_weight
        alpha_wave_final = base_weights["wave"] / total_weight
        alpha_moe_eff = base_weights["moe"] / total_weight

        # è¯­ä¹‰è®­ç»ƒç­–ç•¥ç›‘æ§
        if batch_idx % 200 == 0:
            tqdm.write(f"      [SEMANTIC] Stage: {stage_name} (step {global_step})")
            tqdm.write(f"      [SEMANTIC] Weights: recon={alpha_feat_eff:.3f}, audio_quality={alpha_wave_final:.3f}, routing={alpha_moe_eff:.3f}")
            tqdm.write(f"      [SEMANTIC] Quality: overall={quality_score:.3f}, aud={detailed_quality_metrics['audibility']:.3f}, int={detailed_quality_metrics['intelligibility']:.3f}, qual={detailed_quality_metrics['quality']:.3f}")

        # åº”ç”¨æœ€ç»ˆæƒé‡è®¡ç®—æŸå¤± - å¢åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        # æ£€æŸ¥æ¯ä¸ªæŸå¤±åˆ†é‡çš„æœ‰æ•ˆæ€§
        if torch.isnan(l_feat) or torch.isinf(l_feat):
            tqdm.write(f"    WARNING: l_feat is NaN/Inf: {l_feat.item()}")
            l_feat = torch.tensor(0.0, device=device, dtype=loss_dtype, requires_grad=True)

        if torch.isnan(l_wave) or torch.isinf(l_wave):
            tqdm.write(f"    WARNING: l_wave is NaN/Inf: {l_wave.item()}")
            l_wave = torch.tensor(0.0, device=device, dtype=loss_dtype, requires_grad=True)

        if torch.isnan(l_moe) or torch.isinf(l_moe):
            tqdm.write(f"    WARNING: l_moe is NaN/Inf: {l_moe.item()}")
            l_moe = torch.tensor(0.0, device=device, dtype=loss_dtype, requires_grad=True)

        if torch.isnan(l_sem) or torch.isinf(l_sem):
            tqdm.write(f"    WARNING: l_sem is NaN/Inf: {l_sem.item()}")
            l_sem = torch.tensor(0.0, device=device, dtype=loss_dtype, requires_grad=True)

        # å®‰å…¨çš„æƒé‡æ£€æŸ¥
        if not torch.isfinite(torch.tensor(alpha_feat_eff)):
            alpha_feat_eff = 0.1
        if not torch.isfinite(torch.tensor(alpha_wave_final)):
            alpha_wave_final = 0.1
        if not torch.isfinite(torch.tensor(alpha_moe_eff)):
            alpha_moe_eff = 0.1

        # è®¡ç®—åŒè·¯å¾„åˆ†ç¦»æŸå¤±æƒé‡
        alpha_acoustic_eff = args.alpha_acoustic if hasattr(args, 'alpha_acoustic') else 1.0
        alpha_semantic_eff = args.alpha_semantic if hasattr(args, 'alpha_semantic') else 0.5

        total_loss = (alpha_feat_eff * l_feat + alpha_wave_final * l_wave + l_rate + l_sem +
                     alpha_moe_eff * l_moe + alpha_acoustic_eff * l_acoustic + alpha_semantic_eff * l_semantic)

        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            tqdm.write(f"    CRITICAL: total_loss is NaN/Inf, using fallback")
            total_loss = torch.tensor(1.0, device=device, dtype=loss_dtype, requires_grad=True)

        # ä¿å­˜å½“å‰æƒé‡ç”¨äºåç»­åˆ†æ
        if not hasattr(train_one_epoch, '_weight_history'):
            train_one_epoch._weight_history = []

        train_one_epoch._weight_history.append({
            'step': global_step,
            'stage': stage_name,
            'weights': {
                'feat': alpha_feat_eff,
                'wave': alpha_wave_final,
                'moe': alpha_moe_eff
            },
            'quality_metrics': detailed_quality_metrics.copy(),
            'quality_score': quality_score
        })

        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(train_one_epoch._weight_history) > 1000:
            train_one_epoch._weight_history = train_one_epoch._weight_history[-500:]


        # ğŸš¨ å¢å¼ºçš„NaNæ£€æµ‹å’Œè¯Šæ–­ - åˆ†è§£æ£€æŸ¥æ¯ä¸ªæŸå¤±åˆ†é‡
        if torch.isnan(l_feat) or torch.isinf(l_feat):
            tqdm.write(f"      âš ï¸ NaN/Inf in l_feat: {l_feat.item()}")
        if torch.isnan(l_wave) or torch.isinf(l_wave):
            tqdm.write(f"      âš ï¸ NaN/Inf in l_wave: {l_wave.item()}")
        if torch.isnan(l_moe) or torch.isinf(l_moe):
            tqdm.write(f"      âš ï¸ NaN/Inf in l_moe: {l_moe.item()}")
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            tqdm.write(f"      âš ï¸ NaN/Inf in total_loss: {total_loss.item()}")
            tqdm.write(f"      Components: feat={l_feat.item():.4f}, wave={l_wave.item():.4f}, moe={l_moe.item():.4f}")

        def check_tensor(name, tensor):
            """æ£€æŸ¥tensorä¸­çš„å¼‚å¸¸å€¼"""
            if tensor is None:
                return f"{name}: None"
            if not isinstance(tensor, torch.Tensor):
                return f"{name}: not tensor ({type(tensor)})"
            has_nan = torch.isnan(tensor).any()
            has_inf = torch.isinf(tensor).any()
            min_val = tensor.min().item() if tensor.numel() > 0 else 0
            max_val = tensor.max().item() if tensor.numel() > 0 else 0
            return f"{name}: nan={has_nan}, inf={has_inf}, range=[{min_val:.3f}, {max_val:.3f}]"

        # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åŒ…å«NaN/Inf
        param_nan_count = 0
        for name, param in list(encoder.named_parameters()) + list(decoder.named_parameters()):
            if torch.isnan(param).any() or torch.isinf(param).any():
                param_nan_count += 1
                tqdm.write(f"    CRITICAL: Parameter {name} contains NaN/Inf")
                # ç´§æ€¥é‡ç½®å‚æ•°
                with torch.no_grad():
                    param.data = torch.randn_like(param.data) * 0.01

        if param_nan_count > 0:
            tqdm.write(f"    EMERGENCY: Reset {param_nan_count} parameters with NaN/Inf")

        # è¯¦ç»†çš„NaNè¯Šæ–­
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                tqdm.write(f"    ğŸš¨ NaN/Inf detected in total_loss: {total_loss.item() if not torch.isnan(total_loss) else 'NaN'}")
            elif args.debug_nan:
                tqdm.write(f"    ğŸ” Debug NaN mode - batch {batch_idx} diagnostics:")

            # è¯­ä¹‰æ„ŸçŸ¥æ–¹æ¡ˆçš„å…³é”®æŒ‡æ ‡
            tqdm.write(f"      Semantic Loss Components:")
            if hasattr(train_one_epoch, '_last_fargan_details') and train_one_epoch._last_fargan_details:
                fargan = train_one_epoch._last_fargan_details
                try:
                    tqdm.write(
                        f"        Audio Usability: aud={detailed_quality_metrics.get('audibility', 0.0):.3f}, "
                        f"int={detailed_quality_metrics.get('intelligibility', 0.0):.3f}, "
                        f"qual={detailed_quality_metrics.get('quality', 0.0):.3f}")
                except Exception:
                    pass

            if hasattr(train_one_epoch, '_semantic_adapter'):
                adapter_status = train_one_epoch._semantic_adapter.get_status()
                tqdm.write(f"        Adapter: strength={adapter_status['adaptation_strength']:.1f}")

            tqdm.write(f"      Loss Values:")
            tqdm.write(f"        {check_tensor('feat', l_feat)}")
            tqdm.write(f"        {check_tensor('wave', l_wave)}")
            tqdm.write(f"        {check_tensor('moe', l_moe)}")

            # æ£€æŸ¥è¾“å…¥æ•°æ®
            tqdm.write(f"      Input data:")
            tqdm.write(f"        {check_tensor('features_x', x)}")
            tqdm.write(f"        {check_tensor('features_y', y)}")
            tqdm.write(f"        {check_tensor('audio', audio)}")

            # æ£€æŸ¥å‰å‘è¾“å‡º
            tqdm.write(f"      Forward outputs:")
            tqdm.write(f"        {check_tensor('latent_z', z)}")
            tqdm.write(f"        {check_tensor('decoded_feats', feats)}")
            if y_hat_audio is not None:
                tqdm.write(f"        {check_tensor('decoded_audio', y_hat_audio)}")

            # æ£€æŸ¥MoEç›¸å…³ä¿¡æ¯
            if encoder.use_moe and isinstance(enc_logs, dict):
                tqdm.write(f"      MoE logs:")
                for key, value in enc_logs.items():
                    if isinstance(value, torch.Tensor):
                        tqdm.write(f"        {check_tensor(key, value)}")

            # ä»…åœ¨çœŸæ­£çš„NaNæ—¶è¿›è¡Œæ¢å¤
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                # è·³è¿‡è¿™ä¸ªbatchï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªæœ‰æ•ˆloss
                if hasattr(train_one_epoch, '_last_valid_loss'):
                    total_loss = train_one_epoch._last_valid_loss.clone().detach().requires_grad_()
                    tqdm.write(f"      Recovery: using last valid loss {total_loss.item():.6f}")
                else:
                    # ä½¿ç”¨ä¿å®ˆçš„fallback loss
                    total_loss = torch.tensor(1.0, device=device, dtype=loss_dtype, requires_grad=True)
                    tqdm.write(f"      Recovery: using fallback loss {total_loss.item():.6f}")

        if not (torch.isnan(total_loss) or torch.isinf(total_loss)):
            # ä¿å­˜æœ‰æ•ˆlossç”¨äºæ¢å¤
            train_one_epoch._last_valid_loss = total_loss.clone().detach()

        # æ¢¯åº¦ç´¯ç§¯æ”¯æŒ
        accumulation_steps = getattr(args, 'gradient_accumulation_steps', 1)
        scaled_loss = total_loss / accumulation_steps


        # è®¡ç®—å¤æ‚åº¦ç›‘æ§ï¼ˆå¯é€‰ï¼‰
        complexity_monitor_interval = 200
        if global_step % complexity_monitor_interval == 0 and hasattr(encoder, 'moe'):
            try:
                with torch.no_grad():
                    # ç»Ÿè®¡ä¸åŒpathwayæ¨¡å¼çš„è®¡ç®—é‡
                    pathway_stats = encoder.moe.get_performance_stats() if hasattr(encoder.moe, 'get_performance_stats') else {}
                    pathway_mode = pathway_stats.get('pathway_mode', 'unknown')
                    complexity_ratio = pathway_stats.get('complexity_ratio', 1.0)

                    # è®°å½•åˆ°epoch metricsç”¨äºåç»­åˆ†æ
                    if 'pathway_complexity_samples' not in epoch_metrics:
                        epoch_metrics['pathway_complexity_samples'] = 0
                        epoch_metrics['pathway_complexity_total'] = 0.0

                    epoch_metrics['pathway_complexity_samples'] += batch_size
                    epoch_metrics['pathway_complexity_total'] += complexity_ratio * batch_size

                    # æ¯1000æ­¥æŠ¥å‘Šä¸€æ¬¡å¤æ‚åº¦çŠ¶æ€
                    if global_step % 1000 == 0:
                        avg_complexity = epoch_metrics['pathway_complexity_total'] / max(1, epoch_metrics['pathway_complexity_samples'])
                        tqdm.write(f"    Complexity: {avg_complexity:.2f}x baseline")
            except Exception as e:
                pass  # å¤æ‚åº¦ç›‘æ§å¤±è´¥ä¸å½±å“è®­ç»ƒ

        # åä¼ ï¼šfp16 ç”¨ GradScalerï¼Œbf16/none ç›´æ¥ backward
        if use_fp16:
            scaler.scale(scaled_loss).backward()
        else:
            scaled_loss.backward()

        # åªæœ‰åœ¨ç´¯ç§¯è¾¹ç•Œæ‰åš unscale/clip/æ—¥å¿—/step
        if (batch_idx + 1) % accumulation_steps == 0:
            # å…ˆ unscale å† clipï¼ˆfp16 ä¸“ç”¨ï¼‰
            if use_fp16:
                scaler.unscale_(optimizer)

            # ï¼ˆæŠŠä½ åŸæ¥â€œå¯¹ parametrizations ä¸ fargan_core çš„æ‰‹åŠ¨ clampâ€ç§»åˆ°è¿™é‡Œæ¥ï¼Œ
            #  ç¡®ä¿åœ¨ unscale ä¹‹åã€clip ä¹‹å‰æ‰§è¡Œï¼‰
            with torch.no_grad():
                for name, param in decoder.named_parameters():
                    if param.grad is None:
                        continue
                    if 'parametrizations' in name:
                        param.grad.clamp_(-0.1, 0.1)
                    elif 'fargan_core' in name:
                        param.grad.clamp_(-1.0, 1.0)

            # å…ˆåšä¸€æ¬¡ NaN/Inf æ¸…æ´—ï¼ˆæ›´æ¿€è¿›çš„ç‰ˆæœ¬ï¼‰
            with torch.no_grad():
                cleaned_grads = 0
                for p in list(encoder.parameters()) + list(decoder.parameters()):
                    if p.grad is not None:
                        if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                            cleaned_grads += 1
                            # æ¸©å’Œä¿®å¤ï¼šä»…æŠŠå¼‚å¸¸å…ƒç´ æ›¿æ¢ä¸º0ï¼Œä¿ç•™å…¶ä½™æœ‰æ•ˆæ¢¯åº¦
                            p.grad.nan_to_num_(nan=0.0, posinf=0.0, neginf=0.0)
                        # å¯¹æ¢¯åº¦æ•´ä½“åšä¿å®ˆçš„clampï¼Œé˜²æ­¢çˆ†ç‚¸
                        p.grad.clamp_(-10.0, 10.0)

                if cleaned_grads > 0:
                    tqdm.write(f"    Cleaned {cleaned_grads} gradients with NaN/Inf")

            # å…¨æ¨¡å‹ clipï¼ˆå»ºè®® 1.0 æ›´ç¨³ï¼‰
            total_norm = torch.nn.utils.clip_grad_norm_(
                list(encoder.parameters()) + list(decoder.parameters()), max_norm=1.0
            )

            # è®¡ç®—æ¢¯åº¦ç»Ÿè®¡ï¼Œä½†å‡å°‘æ‰“å°é¢‘ç‡
            gn_vc, n_vc  = _sum_grad_norm(decoder.named_parameters(), include_key='fargan_core')
            gn_dec, n_dec = _sum_grad_norm(decoder.named_parameters(), exclude_key='fargan_core')
            gn_enc, n_enc = _sum_grad_norm(encoder.named_parameters())

            # æ¯100æ­¥æ‰“å°ä¸€æ¬¡æ¢¯åº¦ç»Ÿè®¡ï¼Œæˆ–è€…åœ¨fargan_coreæ¢¯åº¦å¼‚å¸¸ä½æ—¶æ‰“å°
            if batch_idx % 100 == 0 or gn_vc < 1e-4:
                tqdm.write(f"[GRAD] fargan_core={gn_vc:.3e}, decoder={gn_dec:.3e}, encoder={gn_enc:.3e}")

            # ä¿®å¤ï¼šä¸“å®¶å·®å¼‚åŒ–åˆ†æç›‘æ§ - ç‹¬ç«‹äºæ¢¯åº¦ç›‘æ§æ¡ä»¶ï¼Œç¡®ä¿æ‰€æœ‰é˜¶æ®µéƒ½æ˜¾ç¤º
            expert_monitor_interval = max(50, int(getattr(args, 'log_interval', 50)))
            if batch_idx % expert_monitor_interval == 0:
                # è°ƒè¯•ä¿¡æ¯ï¼šç¡®è®¤ä¸“å®¶ç›‘æ§æ¡ä»¶è§¦å‘
                # tqdm.write(f"[DEBUG] Expert monitoring triggered at batch {batch_idx}, interval={expert_monitor_interval}")
                if hasattr(encoder, 'moe') and encoder.moe is not None:
                    try:
                        # è·å–å®é™…çš„MoEå®ç°
                        actual_moe = encoder.moe
                        if hasattr(encoder.moe, 'specialized_moe'):
                            actual_moe = encoder.moe.specialized_moe

                        # æ‰¾åˆ°çœŸæ­£çš„experts
                        experts_container = None
                        if hasattr(actual_moe, 'experts'):
                            experts_container = actual_moe
                        elif hasattr(actual_moe, 'moe_system') and hasattr(actual_moe.moe_system, 'experts'):
                            experts_container = actual_moe.moe_system

                        # æ£€æŸ¥ä¸“å®¶å·®å¼‚åŒ–å’Œä¸“ä¸šåŒ–å­¦ä¹ 
                        expert_biases = []
                        specialization_analysis = []

                        if experts_container and hasattr(experts_container, 'experts') and len(experts_container.experts) > 0:
                            expert_bias_values = []
                            expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"]

                            for i, expert in enumerate(experts_container.experts):
                                if hasattr(expert, 'expert_bias'):
                                    bias_norm = expert.expert_bias.norm().item()
                                    spec_norm = expert.specialization_weights.norm().item() if hasattr(expert, 'specialization_weights') else 0.0
                                    expert_name = expert_names[i] if i < len(expert_names) else f"E{i}"
                                    expert_biases.append(f"{expert_name}:{bias_norm:.3f}")
                                    expert_bias_values.append(bias_norm)
                                else:
                                    expert_biases.append(f"E{i}:no_bias")
                                    expert_bias_values.append(0.0)

                            # åˆ†æä¸“å®¶å·®å¼‚åŒ–ç¨‹åº¦
                            if len(expert_bias_values) > 1:
                                bias_variance = torch.tensor(expert_bias_values).var().item()
                                bias_max_diff = max(expert_bias_values) - min(expert_bias_values)
                                specialization_analysis.append(f"bias_var:{bias_variance:.4f}")
                                specialization_analysis.append(f"max_diff:{bias_max_diff:.3f}")

                                # åˆ¤æ–­ä¸“ä¸šåŒ–çŠ¶æ€
                                if bias_variance < 0.001 and bias_max_diff < 0.02:
                                    spec_status = "SYNCED"  # åŒæ­¥å¢é•¿ï¼Œç¼ºä¹å·®å¼‚åŒ–
                                elif bias_variance > 0.01:
                                    spec_status = "DIVERGING"  # æ­£åœ¨å­¦ä¹ å·®å¼‚åŒ–
                                else:
                                    spec_status = "LEARNING"  # ä¸­ç­‰å·®å¼‚åŒ–

                                specialization_analysis.append(f"status:{spec_status}")

                            # æ£€æŸ¥ä¸“å®¶ä½¿ç”¨ç‡å·®å¼‚ï¼ˆå¦ä¸€ä¸ªä¸“ä¸šåŒ–æŒ‡æ ‡ï¼‰
                            if hasattr(encoder.moe, 'get_expert_utilization'):
                                try:
                                    expert_util = encoder.moe.get_expert_utilization()
                                    util_variance = expert_util.var().item()
                                    if util_variance > 0.05:
                                        routing_status = "SPECIALIZED"  # è·¯ç”±å™¨å­¦åˆ°äº†ä¸“ä¸šåŒ–
                                    elif util_variance > 0.02:
                                        routing_status = "LEARNING"     # æ­£åœ¨å­¦ä¹ ä¸“ä¸šåŒ–
                                    else:
                                        routing_status = "UNIFORM"      # å‡åŒ€åˆ†å¸ƒï¼Œç¼ºä¹ä¸“ä¸šåŒ–
                                    specialization_analysis.append(f"routing:{routing_status}")
                                except:
                                    pass

                        if expert_biases:
                            tqdm.write(f"[EXPERT] Differentiation: {', '.join(expert_biases)}")
                            if specialization_analysis:
                                tqdm.write(f"[EXPERT] Specialization: {', '.join(specialization_analysis)}")
                        else:
                            tqdm.write(f"[EXPERT] No differentiation metrics found")

                    except Exception as e:
                        tqdm.write(f"[DEBUG] Expert analysis failed: {e}")

            # ä¿å­˜fargan_coreæ¢¯åº¦çŠ¶æ€ç”¨äºåç»­æ£€æŸ¥
            train_one_epoch._last_fargan_grad_norm = gn_vc

            # éæœ‰é™æ€»ä½“èŒƒæ•°ï¼šåšä¸€æ¬¡æ·±åº¦æ¸…æ´—å¹¶è·³è¿‡æœ¬æ­¥
            if not torch.isfinite(total_norm):
                tqdm.write("    ğŸš¨ Non-finite grad norm detected. Sanitizing & skipping this step.")
                with torch.no_grad():
                    for p in list(encoder.parameters()) + list(decoder.parameters()):
                        if p.grad is not None:
                            p.grad.nan_to_num_(0.0, posinf=0.0, neginf=0.0)
                optimizer.zero_grad(set_to_none=True)
                # é‡ç½®scalerçŠ¶æ€ä»¥é˜²æ­¢åç»­é—®é¢˜
                if use_fp16:
                    scaler.update()
            else:
                # æ­£å¸¸æ›´æ–°ï¼šfp16 ç”¨ scaler.step/updateï¼Œå…¶ä»–ç›´æ¥ step
                if use_fp16:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad(set_to_none=True)

            # æ›´æ–°æ¢¯åº¦ 
            with torch.no_grad():
                tn = float(total_norm) if torch.isfinite(total_norm) else 10.0
                beta = 0.98
                train_one_epoch._gn_ema = beta * train_one_epoch._gn_ema + (1.0 - beta) * tn

            # Step-based checkpoint saving (optional)
            save_steps = int(getattr(args, 'save_every_steps', 0) or 0)
            if save_steps > 0 and (global_step % save_steps == 0):
                try:
                    step_ckpt = {
                        'epoch': int(epoch_idx) if epoch_idx is not None else -1,
                        'step': int(global_step),
                        'encoder_state_dict': encoder.state_dict(),
                        'decoder_state_dict': decoder.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'loss': float(total_loss),
                    }
                    step_path = Path(args.output_dir) / f'stage3_step_{global_step:06d}.pth'
                    torch.save(step_ckpt, step_path)
                    tqdm.write(f"ğŸ’¾ Saved step checkpoint: {step_path}")
                except Exception as _e:
                    tqdm.write(f"âš ï¸  Failed to save step checkpoint at {global_step}: {_e}")



        # ä¸ºmetricsè®°å½•ä½¿ç”¨åŸå§‹lossï¼ˆæœªç¼©æ”¾ï¼‰
        loss_for_metrics = total_loss

        # Update metrics
        batch_size = x.size(0)
        total_samples += batch_size

        epoch_metrics['total_loss'] += loss_for_metrics.item() * batch_size
        epoch_metrics['feature_loss'] += l_feat.item() * batch_size
        epoch_metrics['wave_loss'] += l_wave.item() * batch_size
        epoch_metrics['moe_loss'] += l_moe.item() * batch_size
        epoch_metrics['rate_loss'] += l_rate.item() * batch_size
        if isinstance(l_sem, (int, float)):
            sem_item = l_sem
        else:
            sem_item = l_sem.item()
        epoch_metrics.setdefault('semantic_loss', 0.0)
        epoch_metrics['semantic_loss'] += sem_item * batch_size

        # MoE metrics
        for key in ['expert_entropy', 'expert_usage_min', 'expert_usage_max']:
            if key in moe_metrics:
                epoch_metrics[key] += moe_metrics[key] * batch_size

        # Individual expert metrics - åŠ¨æ€ä¸“å®¶æ•°é‡
        n_experts = getattr(encoder.moe, 'n_experts', 4) if hasattr(encoder, 'moe') else 4
        for i in range(n_experts):  # åŠ¨æ€ä¸“å®¶æ•°é‡
            expert_key = f'expert_{i}_usage'
            if expert_key in moe_metrics:
                if expert_key not in epoch_metrics:
                    epoch_metrics[expert_key] = 0.0
                epoch_metrics[expert_key] += moe_metrics[expert_key] * batch_size

        # ç›´æµé€šè·¯æ€§èƒ½ç›‘æ§
        if hasattr(encoder, 'moe') and hasattr(encoder.moe, 'get_performance_stats'):
            pathway_stats = encoder.moe.get_performance_stats()
            for key, value in pathway_stats.items():
                pathway_key = f'pathway_{key}'
                if pathway_key not in epoch_metrics:
                    epoch_metrics[pathway_key] = 0.0
                if isinstance(value, (int, float)):
                    epoch_metrics[pathway_key] += value * batch_size
                elif isinstance(value, list) and len(value) > 0:
                    # å¯¹äºåˆ—è¡¨ç±»å‹ï¼Œè®¡ç®—å¹³å‡å€¼
                    avg_value = sum(value) / len(value)
                    epoch_metrics[pathway_key] += avg_value * batch_size

            # åˆ†ç¦»æŸå¤±è®¡ç®—å’ŒEMAæ›´æ–°
            if hasattr(encoder.moe, 'get_separated_outputs'):
                direct_output, expert_output = encoder.moe.get_separated_outputs()
                if direct_output is not None and expert_output is not None:
                    # è®¡ç®—åˆ†ç¦»æŸå¤±ï¼šç›´æµvsä¸“å®¶çš„å†…åœ¨å·®å¼‚ï¼ˆL2è·ç¦»ï¼‰
                    # è¿™é‡Œæ¯”è¾ƒçš„æ˜¯ä¸¤ç§å¤„ç†æ–¹å¼çš„å·®å¼‚ï¼Œè€Œä¸æ˜¯ä¸åŸå§‹è¾“å…¥çš„é‡å»ºè¯¯å·®
                    pathway_diff = F.mse_loss(direct_output, expert_output)

                    # ä½¿ç”¨pathway_diffä½œä¸ºæ€§èƒ½æŒ‡æ ‡ï¼šå€¼è¶Šå¤§è¯´æ˜ä¸¤ç§æ–¹æ³•å·®å¼‚è¶Šå¤§
                    # ç†æƒ³æƒ…å†µä¸‹ä¸“å®¶ç³»ç»Ÿåº”è¯¥äº§ç”Ÿä¸ç›´æµä¸åŒä½†æ›´ä¼˜çš„ç‰¹å¾
                    direct_loss_proxy = pathway_diff.item()
                    expert_loss_proxy = l_feat.item()  # ä½¿ç”¨æ€»ä½“ç‰¹å¾æŸå¤±ä½œä¸ºä¸“å®¶æ€§èƒ½ä»£ç†

                    # æ›´æ–°EMA
                    encoder.moe.update_performance_ema(direct_loss_proxy, expert_loss_proxy)

                    # è®°å½•åˆ°epoch metrics
                    epoch_metrics.setdefault('pathway_diff_loss', 0.0)
                    epoch_metrics.setdefault('pathway_expert_proxy_loss', 0.0)
                    epoch_metrics['pathway_diff_loss'] += direct_loss_proxy * batch_size
                    epoch_metrics['pathway_expert_proxy_loss'] += expert_loss_proxy * batch_size

        # ğŸ”§ FIX 8: å¢å¼ºè°ƒè¯•ä¿¡æ¯ï¼Œç›‘æ§æ¢¯åº¦æµå’Œå…³é”®æŒ‡æ ‡
        # Progress logging with enhanced gradient flow monitoring
        log_interval = max(1, int(getattr(args, 'log_interval', 50)))
        if batch_idx % log_interval == 0:
            # è®°å½•å…³é”®æ¢¯åº¦æµä¿¡æ¯
            with torch.no_grad():
                # æ£€æŸ¥featsçš„æ¢¯åº¦è¿æ¥çŠ¶æ€
                feats_grad_connected = feats.requires_grad if isinstance(feats, torch.Tensor) else False
                # æ£€æŸ¥teacher forcingæ¯”ä¾‹å’Œæ¢¯åº¦ä¼ æ’­æ¯”ä¾‹
                tf_ratio = tf if 'tf' in locals() else 0.0
                # æ£€æŸ¥æ˜¯å¦è®¡ç®—äº†wave_loss
                wave_computed = wave_computed_flag

        if batch_idx % log_interval == 0:
            # Safe float conversions for printing
            def _sf(x):
                try:
                    return float(x)
                except Exception:
                    try:
                        return float(x.item())
                    except Exception:
                        return 0.0

            tl = _sf(loss_for_metrics)
            ff = _sf(l_feat)
            ww = _sf(l_wave)
            mm = _sf(l_moe)
            # åŒè·¯å¾„åˆ†ç¦»æŸå¤±
            aa = _sf(l_acoustic)  # å£°å­¦æŸå¤±
            ss = _sf(l_semantic)  # è¯­ä¹‰æŸå¤±
            # æ–°ç‰ˆSpecializedMicroMoEçš„æŸå¤±
            mb = _sf(enc_logs.get('moe_balance_loss', 0.0) if isinstance(enc_logs, dict) else 0.0)
            ms = _sf(enc_logs.get('moe_harmonic_pref', 0.0) if isinstance(enc_logs, dict) else 0.0)  # ä½¿ç”¨å®é™…å­˜åœ¨çš„æŒ‡æ ‡
            lr = optimizer.param_groups[0].get('lr', 0.0)
            # ETA from tqdm
            remaining = progress.format_dict.get('remaining', None)
            import time as _t
            eta_str = _t.strftime('%H:%M:%S', _t.gmtime(remaining)) if remaining is not None else 'NA'
            # è¯­ä¹‰æ„ŸçŸ¥æ–¹æ¡ˆçš„å…³é”®æŒ‡æ ‡
            post = {
                'loss': f"{tl:.4f}",
                'feat': f"{ff:.3f}",
                'wave': f"{ww:.3f}",
                'acou': f"{aa:.3f}",  # å£°å­¦æŸå¤±
                'sem': f"{ss:.3f}",   # è¯­ä¹‰æŸå¤±
                'lr': f"{lr:.2e}",
                'eta': eta_str,
            }

            # è¯­ä¹‰é€‚é…å™¨çŠ¶æ€
            if hasattr(train_one_epoch, '_semantic_adapter'):
                adapter_status = train_one_epoch._semantic_adapter.get_status()
                post['adapt'] = f"{adapter_status['adaptation_strength']:.1f}"

            # éŸ³é¢‘å¯ç”¨æ€§æŒ‡æ ‡
            if hasattr(train_one_epoch, '_last_fargan_details') and train_one_epoch._last_fargan_details:
                fargan = train_one_epoch._last_fargan_details
                # ä½¿ç”¨å·²è®¡ç®—çš„è´¨é‡æŒ‡æ ‡ï¼ˆdetailed_quality_metricsï¼‰è¿›è¡Œå±•ç¤º
                try:
                    post.update({
                        'aud': f"{detailed_quality_metrics.get('audibility', 0.0):.2f}",
                        'int': f"{detailed_quality_metrics.get('intelligibility', 0.0):.2f}",
                        'qual': f"{detailed_quality_metrics.get('quality', 0.0):.2f}",
                    })
                except Exception:
                    pass
            else:
                post.update({
                    'feat': f"{ff:.4f}",
                    'wave': f"{ww:.4f}",
                })

            # è®­ç»ƒæ§åˆ¶çŠ¶æ€
            post.update({
                'warm': f"{warm_ratio:.2f}",
                'bp': f"{wave_bp_ratio:.2f}",
            })

            # åªåœ¨MoEçœŸæ­£å¯ç”¨æ—¶æ˜¾ç¤ºMoEæŒ‡æ ‡
            mm = float(l_moe)
            mb = float(enc_logs.get('moe_balance_loss', 0.0)) if isinstance(enc_logs, dict) else 0.0
            mt = float(enc_logs.get('moe_token_balance_loss', 0.0)) if isinstance(enc_logs, dict) else 0.0
            md = float(enc_logs.get('expert_diversification_loss', 0.0)) if isinstance(enc_logs, dict) else 0.0
            post.update({
                'moe': f"{mm:.4f}",
                'moe_b': f"{mb:.4f}",
                'moe_t': f"{mt:.4f}",
                'moe_d': f"{md:.4f}",
            })

            # Optional profile timings if enabled
            prof_int = int(getattr(args, 'profile_interval', 0) or 0)
            if prof_int > 0 and batch_idx % prof_int == 0:
                try:
                    post.update({
                        't_fwd': f"{(t_fwd1 - t_fwd0)*1000:.0f}ms",
                        't_feat': f"{(t_feat - t_fwd1)*1000:.0f}ms",
                        't_wave': f"{(t_wave - t_feat)*1000:.0f}ms",
                        't_back': f"{(t_back - t_wave)*1000:.0f}ms",
                    })
                except Exception:
                    pass
            progress.set_postfix(post)


            # ç¬¬ 0 ä¸ª batch æ‰“ä¸€è¡Œ MoE è‡ªæ£€ + æ¢¯åº¦æµè¯Šæ–­
            if batch_idx == 0:
                try:
                    n_exp = getattr(encoder.moe, 'n_experts', '?')
                    top_k = getattr(encoder.moe, 'top_k', '?')
                    sm = getattr(encoder.moe, 'specialized_moe', None)
                    token_level = getattr(sm, 'use_token_level', False) if sm else False
                    tqdm.write(f"    [MoE Active] n_experts={n_exp}, top_k={top_k}, token_level={token_level}")
                except Exception:
                    pass

                # æ¢¯åº¦æµè¯Šæ–­ä¿¡æ¯
                tqdm.write(f"    ğŸ” Gradient Flow Status:")
                tqdm.write(f"       feats gradient: {'Connected' if feats_grad_connected else 'DISCONNECTED'}")
                tqdm.write(f"       teacher forcing: {tf_ratio:.3f}")
                tqdm.write(f"       wave backprop: {wave_bp_ratio:.3f}")
                tqdm.write(f"       wave computed: {'Yes' if wave_computed else 'No'}")
                if hasattr(args, 'amp'):
                    tqdm.write(f"       AMP mode: {args.amp}")

            # Health æ‰“å°ï¼šç¬¬ 0 æ­¥ä¹Ÿæ‰“ï¼›é—´éš”=max(4*log_interval, 50)
            moe_monitor_interval = max(4 * int(getattr(args, 'log_interval', 50)), 50)
            if encoder.use_moe and (batch_idx == 0 or batch_idx % moe_monitor_interval == 0):
                # åœ¨ç»•è¿‡æ¨¡å¼ä¸‹ï¼Œæ˜¾ç¤ºç»•è¿‡çŠ¶æ€ä¿¡æ¯
                if args.emergency_bypass_moe:
                    # ç»•è¿‡æ¨¡å¼ï¼šå°è¯•æ˜¾ç¤ºæ¨¡æ‹Ÿçš„å‡åŒ€åˆ†å¸ƒç»Ÿè®¡
                    if hasattr(encoder, 'moe') and encoder.moe is not None:
                        try:
                            with torch.no_grad():
                                expert_util = encoder.moe.get_expert_utilization()
                                usage_str = ', '.join([f'{util.item():.3f}' for util in expert_util])
                                tqdm.write(f"    MoE Health (BYPASS): usage=[{usage_str}] (simulated uniform distribution)")
                        except Exception:
                            n_experts = getattr(encoder.moe, 'n_experts', 4) if hasattr(encoder, 'moe') else 4
                            bypass_usage = ', '.join(['0.333'] * n_experts)
                            tqdm.write(f"    MoE Health (BYPASS): usage=[{bypass_usage}] (routing bypassed)")
                    else:
                        tqdm.write(f"    MoE Health (BYPASS): MoE module not available")
                elif moe_metrics:
                    # æ­£å¸¸æ¨¡å¼ï¼šæ˜¾ç¤ºå®Œæ•´çš„MoEå¥åº·ä¿¡æ¯
                    expert_min = moe_metrics.get('expert_usage_min', 0)
                    expert_max = moe_metrics.get('expert_usage_max', 0)
                    expert_entropy = moe_metrics.get('expert_entropy', 0)
                    expert_usage_all = moe_metrics.get('expert_usage_all', 'N/A')

                    tqdm.write(f"    MoE Health: usage=[{expert_usage_all}] entropy={expert_entropy:.3f}")

                    # æ–°å¢ï¼šç»Ÿä¸€ä¸“å®¶æ¶æ„ç›‘æ§
                    try:
                        if hasattr(encoder.moe, 'experts') and len(encoder.moe.experts) > 0:
                            first_expert = encoder.moe.experts[0]
                            if hasattr(first_expert, 'expert_id'):
                                total_params = sum(p.numel() for p in first_expert.parameters())
                                tqdm.write(f"    [Unified Architecture] Each expert: {total_params:,} params")

                                # æ˜¾ç¤ºä¸“å®¶å·®å¼‚åŒ–å­¦ä¹ è¿›åº¦
                                expert_biases = []
                                for i, expert in enumerate(encoder.moe.experts):
                                    if hasattr(expert, 'expert_bias'):
                                        bias_norm = expert.expert_bias.norm().item()
                                        expert_biases.append(f"E{i}:{bias_norm:.3f}")

                                if expert_biases:
                                    tqdm.write(f"    [Expert Differentiation] Bias norms: {', '.join(expert_biases)}")
                    except Exception:
                        pass

                    # ä¸“å®¶åˆ©ç”¨ç‡è­¦å‘Šï¼ˆä»…åœ¨ä¸¥é‡ä¸å‡è¡¡æ—¶ï¼‰
                    if expert_min < 0.1:  # æé«˜é˜ˆå€¼ä»0.15åˆ°0.1ï¼Œå‡å°‘è­¦å‘Šé¢‘ç‡
                        tqdm.write(f"    [WARNING] Expert usage imbalance detected (min={expert_min:.3f})")

                    # æ–°å¢ï¼šä¸“å®¶æ€§èƒ½åˆ†æ
                    if expert_entropy < 0.5:
                        tqdm.write(f"    [INFO] Low routing diversity - possible expert collapse")
                    elif expert_entropy > 1.0:
                        tqdm.write(f"    [INFO] High routing diversity - good expert utilization")

                    # æ¢¯åº¦è­¦å‘Šï¼šæ£€æŸ¥fargan_coreæ¢¯åº¦çŠ¶æ€
                    if hasattr(train_one_epoch, '_last_fargan_grad_norm'):
                        last_fg_norm = train_one_epoch._last_fargan_grad_norm
                        if last_fg_norm < 1e-6:
                            tqdm.write(f"    ğŸ˜¨ Warning: FARGAN core gradient very low ({last_fg_norm:.2e})")
                            tqdm.write(f"       Check: feature connectivity, wave_bp_ratio, dtype consistency")
                        # é¢å¤–çš„MoEè·¯ç”±è¯Šæ–­
                        if hasattr(encoder, 'moe') and encoder.moe is not None:
                            try:
                                with torch.no_grad():
                                    # è·å–æœ€è¿‘ä¸€æ¬¡çš„è·¯ç”±logits/probsï¼ˆå¦‚æœå¯ç”¨ï¼‰
                                    if hasattr(encoder.moe, '_last_router_logits'):
                                        logits = encoder.moe._last_router_logits
                                        tqdm.write(f"        Last router logits range: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
                                        probs = torch.softmax(logits, dim=-1)
                                        tqdm.write(f"        Last router probs mean: {probs.mean(dim=0).tolist()}")
                            except Exception as e:
                                tqdm.write(f"        MoE diagnosis failed: {e}")

                    if expert_entropy < 0.7:  # æé«˜é˜ˆå€¼ä»0.8åˆ°0.7
                        tqdm.write(f"    âš ï¸  Warning: Low expert entropy detected ({expert_entropy:.3f})")

                # ç›´æµé€šè·¯ç›‘æ§è¾“å‡º - æ¶æ„çº§ç»•è¿‡æ”¯æŒ
                if hasattr(encoder, 'moe') and hasattr(encoder.moe, 'get_performance_stats'):
                    pathway_stats = encoder.moe.get_performance_stats()
                    if pathway_stats:
                        bypass_weight = pathway_stats.get('bypass_weight', 0.0)
                        expert_weight = pathway_stats.get('expert_weight', 1.0)
                        performance_ratio = pathway_stats.get('performance_ratio', 1.0)
                        pathway_mode = pathway_stats.get('pathway_mode', 'unknown')
                        stage1_equivalent = pathway_stats.get('stage1_equivalent', False)
                        complexity_ratio = pathway_stats.get('complexity_ratio', 1.0)

                        # æ ¼å¼åŒ–æƒé‡æ˜¾ç¤º
                        weight_display = f"direct={bypass_weight:.2f}, expert={expert_weight:.2f}"
                        ratio_display = f"perf_ratio={performance_ratio:.3f}"
                        mode_display = f"mode={pathway_mode}, complexity={complexity_ratio:.1f}x"

                        tqdm.write(f"    Pathway Balance: {weight_display}, {ratio_display}")
                        tqdm.write(f"    System Mode: {mode_display}")

                        # æ¶æ„çº§ç»•è¿‡çŠ¶æ€æ˜¾ç¤º
                        if stage1_equivalent:
                            tqdm.write(f"    ğŸŸ¢ Architectural Bypass: Stage1-equivalent mode active")
                        elif pathway_mode == 'mixed':
                            tqdm.write(f"    ğŸŸ¡ Mixed Mode: Transitioning to expert system")
                        elif pathway_mode == 'pure_expert':
                            tqdm.write(f"    ğŸ”µ Pure Expert Mode: Full MoE active")

                        # æ€§èƒ½åˆ†æ - é’ˆå¯¹ç®€åŒ–ä¸“å®¶æ¶æ„
                        if pathway_mode == 'architectural_bypass':
                            tqdm.write(f"    [PATHWAY] Training in Stage1-equivalent mode for stability")
                        elif pathway_mode == 'mixed':
                            # æ›´è¯¦ç»†çš„æ€§èƒ½åˆ†æ
                            if performance_ratio > 3.0:
                                tqdm.write(f"    [PERFORMANCE] Expert system significantly underperforming (ratio={performance_ratio:.3f})")
                                tqdm.write(f"    [ANALYSIS] Unified experts may need more training time")
                            elif performance_ratio > 1.5:
                                tqdm.write(f"    [PERFORMANCE] Direct pathway outperforming (ratio={performance_ratio:.3f})")
                                tqdm.write(f"    [ANALYSIS] Expert routing learning in progress")
                            elif performance_ratio < 0.8:
                                tqdm.write(f"    [SUCCESS] Expert system outperforming direct pathway (ratio={performance_ratio:.3f})")
                                tqdm.write(f"    [ANALYSIS] Unified architecture achieving specialization")
                            else:
                                tqdm.write(f"    [BALANCED] Competitive performance (ratio={performance_ratio:.3f})")

                            # ä¸“å®¶vsç›´æµé€šè·¯æŸå¤±EMAå¯¹æ¯”
                            if hasattr(encoder.moe, 'moe_system'):
                                moe_sys = encoder.moe.moe_system
                                if hasattr(moe_sys, 'expert_loss_ema') and hasattr(moe_sys, 'direct_loss_ema'):
                                    expert_ema = moe_sys.expert_loss_ema.item()
                                    direct_ema = moe_sys.direct_loss_ema.item()
                                    tqdm.write(f"    [LOSS EMA] Expert: {expert_ema:.4f}, Direct: {direct_ema:.4f}")

                        elif pathway_mode == 'pure_expert':
                            tqdm.write(f"    [PATHWAY] Pure expert mode - unified architecture active")

    # Average metrics
    for key in epoch_metrics:
        epoch_metrics[key] /= max(total_samples, 1)

    # === ğŸ” Epochè´¨é‡ç›‘æ§å’Œç»¼åˆè¯„ä¼°æŠ¥å‘Š ===
    def generate_quality_report():
        """ç”Ÿæˆè®­ç»ƒè´¨é‡ç»¼åˆæŠ¥å‘Š"""
        current_global_step = step + len(loader)

        # ç¡®å®šå½“å‰è®­ç»ƒé˜¶æ®µ
        if current_global_step < 1000:
            current_stage = "Foundation"
            stage_progress = current_global_step / 1000.0
        elif current_global_step < 5000:
            current_stage = "Balanced"
            stage_progress = (current_global_step - 1000) / 4000.0
        else:
            current_stage = "Quality"
            stage_progress = min((current_global_step - 5000) / 5000.0, 1.0)

        # é€‚é…å™¨çŠ¶æ€
        adapter_status = semantic_adapter.get_status() if 'semantic_adapter' in locals() else {}

        # æŸå¤±è¶‹åŠ¿åˆ†æ
        feat_loss_avg = epoch_metrics.get('feature_loss', 0.0)
        wave_loss_avg = epoch_metrics.get('wave_loss', 0.0)
        moe_loss_avg = epoch_metrics.get('moe_loss', 0.0)

        # è®­ç»ƒå¥åº·åº¦è¯„ä¼°
        health_score = 1.0
        health_issues = []

        if feat_loss_avg > 1.0:
            health_score *= 0.8
            health_issues.append("High feature reconstruction loss")

        if wave_loss_avg > 2.0:
            health_score *= 0.7
            health_issues.append("High audio usability loss")

        if moe_loss_avg > 0.5:
            health_score *= 0.9
            health_issues.append("High MoE loss")

        # è¯­ä¹‰æ„ŸçŸ¥æ–¹æ¡ˆçš„epochæŠ¥å‘Š
        report = [
            f"\n=== Semantic Training Report (Step {current_global_step}) ===",
            f"Stage: {current_stage} ({stage_progress:.1%} complete)",
            f"Adaptation Strength: {adapter_status.get('adaptation_strength', 0.0):.2f}",
            f"Bypass Mode: {'On' if adapter_status.get('bypass_mode', False) else 'Off'}",
        ]

        # å¦‚æœæœ‰æƒé‡å†å²ï¼Œæ˜¾ç¤ºæƒé‡åˆ†å¸ƒ
        if hasattr(train_one_epoch, '_weight_history') and train_one_epoch._weight_history:
            recent_weights = train_one_epoch._weight_history[-10:]  # æœ€è¿‘10æ¬¡æƒé‡
            try:
                avg_weights = {
                    'feat': sum(w['weights']['feat'] for w in recent_weights) / len(recent_weights),
                    'wave': sum(w['weights']['wave'] for w in recent_weights) / len(recent_weights),
                    'moe': sum(w['weights']['moe'] for w in recent_weights) / len(recent_weights),
                }
                report.append(f"Semantic Weight Balance: recon={avg_weights['feat']:.3f}, quality={avg_weights['wave']:.3f}, routing={avg_weights['moe']:.3f}")
            except (KeyError, TypeError):
                pass  # è·³è¿‡æƒé‡ç»Ÿè®¡

        # ç²¾ç®€MoEçŠ¶æ€æ±‡æ€»
        moe_summary = []
        if hasattr(encoder, 'moe') and encoder.moe is not None:
            try:
                expert_util = encoder.moe.get_expert_utilization()
                expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"]

                # æ˜¾ç¤ºå¸¦åç§°çš„ä¸“å®¶ä½¿ç”¨ç‡
                expert_usage_named = []
                for i, util in enumerate(expert_util):
                    name = expert_names[i] if i < len(expert_names) else f"E{i}"
                    expert_usage_named.append(f"{name}:{util:.3f}")

                moe_summary.append(f"Expert Usage: [{', '.join(expert_usage_named)}]")

                # æ€§èƒ½çŠ¶æ€ï¼ˆæœ€å…³é”®ï¼‰
                if hasattr(trainer.encoder.moe, 'performance_ratio'):
                    perf_ratio = trainer.encoder.moe.performance_ratio
                    status = "Learning" if perf_ratio > 1.5 else "Competitive"
                    moe_summary.append(f"vs Direct: {status} ({perf_ratio:.2f}x)")

            except Exception:
                moe_summary.append("Analysis Failed")

        report.extend([
            f"Semantic Loss Summary: recon={feat_loss_avg:.4f}, audio_quality={wave_loss_avg:.4f}, routing={moe_loss_avg:.4f}",
            f"System Health: {health_score:.1%}",
        ])

        if moe_summary:
            report.extend(["MoE Expert Summary:"] + [f"  {item}" for item in moe_summary])

        if health_issues:
            report.append(f"[WARNING] Issues: {', '.join(health_issues)}")
        else:
            report.append("[OK] No significant issues detected")

        # ä¸‹é˜¶æ®µå»ºè®®
        if current_stage == "Foundation" and stage_progress > 0.8:
            report.append("ğŸ’¡ Recommendation: Prepare for Balanced stage transition")
        elif current_stage == "Balanced" and stage_progress > 0.8:
            report.append("ğŸ’¡ Recommendation: Prepare for Quality stage transition")
        elif current_stage == "Quality":
            report.append("ğŸ’¡ Recommendation: Monitor semantic preservation metrics")

        return "\n".join(report)

    # ç”Ÿæˆå¹¶æ‰“å°è´¨é‡æŠ¥å‘Š
    if epoch_idx is not None:
        quality_report = generate_quality_report()
        print(quality_report)

        # ä¿å­˜åˆ°epoch_metricsä¸­ä¾›åç»­åˆ†æ
        epoch_metrics['training_stage'] = current_stage if 'current_stage' in locals() else "Unknown"
        epoch_metrics['stage_progress'] = stage_progress if 'stage_progress' in locals() else 0.0
        epoch_metrics['health_score'] = health_score if 'health_score' in locals() else 1.0

    return epoch_metrics, step + len(loader)


def main() -> int:
    """Stage3è®­ç»ƒä¸»å‡½æ•° - æŒ‰AETHERä»»åŠ¡æ¸…å•è¦æ±‚é…ç½®"""

    # åˆ›å»ºtqdmå…¼å®¹çš„æ‰“å°å‡½æ•°ï¼ˆé˜²æ­¢è¢«è¿›åº¦æ¡è¦†ç›–ï¼‰
    def safe_print(msg: str, flush: bool = True):
        """å®‰å…¨æ‰“å°å‡½æ•°ï¼šåœ¨è¿›åº¦æ¡å­˜åœ¨æ—¶ä½¿ç”¨tqdm.writeï¼Œå¦åˆ™ä½¿ç”¨æ™®é€šprint"""
        try:
            # å°è¯•ä½¿ç”¨tqdm.writeï¼ˆå¦‚æœtqdmæ´»è·ƒæ—¶ï¼‰
            tqdm.write(msg)
        except:
            # å›é€€åˆ°æ™®é€šprint
            print(msg)
        if flush:
            import sys
            sys.stdout.flush()
    p = argparse.ArgumentParser(description='Stage 3: MoEå¼•å…¥è®­ç»ƒ (ç¦ç”¨FiLMï¼Œå•å˜é‡éªŒè¯)')
    p.add_argument('--moe-w', type=float, default=0.05, help='sample-level MoE balance loss æƒé‡')
    p.add_argument('--moe-token-w', type=float, default=0.02, help='token-level MoE balance loss æƒé‡')
    # ç›‘ç£è·¯ç”±æš–å¯åŠ¨ï¼ˆå½“å½“å‰æ•°æ®æ¥è‡ªæŸä¸ªä¸“å®¶é›†æ—¶ï¼‰
    p.add_argument('--expert-focus', type=str, default=None,
                   help='å¯é€‰ï¼šå½“å‰è®­ç»ƒæ ·æœ¬æ¥è‡ªå“ªä¸ªä¸“å®¶é›†ï¼ˆharmonic|transient|burst_inpaint|low_snrï¼‰ã€‚ç”¨äºè·¯ç”±å™¨ç›‘ç£æš–å¯åŠ¨ã€‚')
    p.add_argument('--router-sup', type=float, default=0.0,
                   help='è·¯ç”±ç›‘ç£æŸå¤±æƒé‡ï¼ˆCrossEntropyäºsample-level logitsï¼‰ã€‚0å…³é—­ã€‚å»ºè®®æš–å¯åŠ¨æœŸ0.3~0.5ï¼ŒåæœŸé™è‡³â‰¤0.05')
    p.add_argument('--router-sup-decay-steps', type=int, default=5000,
                   help='è·¯ç”±ç›‘ç£çº¿æ€§è¡°å‡æ­¥æ•°ï¼ˆ>0æ—¶ï¼Œä»router-supè¡°å‡è‡³10%ï¼‰')
    # å¯é€‰ï¼šçŸ­æœŸèšç„¦çš„ Top-K æš–å¯åŠ¨ï¼ˆå°æ ·æœ¬éªŒè¯ä¸‹éå¸¸æœ‰æ•ˆï¼‰
    p.add_argument('--topk-warm-steps', type=int, default=0,
                   help='å‰ N ä¸ªå…¨å±€ step ä½¿ç”¨æŒ‡å®šçš„ top-k å€¼ï¼ˆ0 ç¦ç”¨ï¼‰')
    p.add_argument('--topk-warm-k', type=int, default=1, choices=[1, 2],
                   help='æš–å¯åŠ¨é˜¶æ®µä½¿ç”¨çš„ top-kï¼ˆé»˜è®¤ 1ï¼‰')
    # In combined mode, --features/--pcm are not required
    p.add_argument('--features', type=str, required=False, help='Features file path (required if not using --combined-data-root)')
    p.add_argument('--pcm', type=str, required=False, help='Audio PCM file path (required if not using --combined-data-root)')
    p.add_argument('--stage1-checkpoint', type=str, default=None, help='Optional Stage 1 checkpoint for warm start (AETHER encoder/decoder)')
    p.add_argument('--fargan-checkpoint', type=str, help='Pre-trained FARGAN checkpoint (optional)')
    p.add_argument('--resume', type=str, default=None,
                   help='Resume from a previous Stage3 checkpoint (loads encoder/decoder/optimizer)')
    p.add_argument('--output-dir', type=str, default='checkpoints_stage3')
    p.add_argument('--device', type=str, default='auto')
    p.add_argument('--epochs', type=int, default=3, help='Training epochs (task: 3 epochs)')
    p.add_argument('--batch-size', type=int, default=4)
    p.add_argument('--seq-len', type=int, default=800)
    p.add_argument('--num-workers', type=int, default=4)
    p.add_argument('--feature-dims', type=int, default=36)
    # Deprecated: dynamic weighting has replaced static alpha settings.
    # p.add_argument('--alpha-feat', type=float, default=1.0, help='Feature loss base weight (if no schedule)')
    # p.add_argument('--alpha-wave', type=float, default=1.0, help='Wave loss weight')
    p.add_argument('--alpha-sem', type=float, default=0.2, help='Semantic proxy loss weight (encoder semantic head vs priors)')
    p.add_argument('--alpha-acoustic', type=float, default=1.0, help='Acoustic path loss weight (20-dim cepstral features)')
    p.add_argument('--alpha-semantic', type=float, default=0.5, help='Semantic path loss weight (16-dim semantic features)')
    p.add_argument('--amp', type=str, default='fp16', choices=['none', 'fp16', 'bf16'], help='Mixed precision mode for CUDA (fp16 recommended for stability)')
    # Deprecated: no longer used in current Stage3 pipeline
    # p.add_argument('--wave-warmup-steps', type=int, default=3000, help='Linear warmup steps for wave loss weight (alpha_wave)')
    # p.add_argument('--wave-start-step', type=int, default=1500, help='Do not compute vocoder/wave loss before this global step')
    # p.add_argument('--preheat-frames', type=int, default=2, help='Ignore first N frames for loss (Stage1-like warm start)')
    # DataLoader knobs
    p.add_argument('--stride-frames', type=int, default=None, help='Data loader stride in frames (None=auto-adaptive)')
    p.add_argument('--semantic-source', type=str, default='fused', choices=['fused', 'ribbon', 'thread'], help='Semantic head source inside DualStream')
    p.add_argument('--split-stream-inputs', action='store_true', default=True,
                   help='å°†è¾“å…¥ç‰¹å¾æŒ‰ [0:20]â†’Ribbon(coarse), [20:36]â†’Thread(fine) åˆ†æµæ˜ å°„è¿›å…¥ DualStream')
    # Deprecated: dynamic weighting replaces alpha_feat scheduling
    # p.add_argument('--alpha-feat-start', type=float, default=None, help='Optional alpha_feat start value (overrides --alpha-feat)')
    # p.add_argument('--alpha-feat-end', type=float, default=0.0, help='alpha_feat end value')
    # p.add_argument('--alpha-feat-steps', type=int, default=1000, help='Linear anneal steps for alpha_feat')
    p.add_argument('--log-interval', type=int, default=50, help='Steps between progress updates')
    # Quick audio snapshots
    p.add_argument('--val-audio-interval', type=int, default=500,
                   help='If >0, every N steps export ~val-audio-seconds preview wavs (pred/orig)')
    p.add_argument('--val-audio-seconds', type=int, default=10,
                   help='Validation audio preview length in seconds (clamped to available)')
    p.add_argument('--val-audio-teacher', dest='val_audio_teacher', action='store_true', default=True,
                   help='Also export teacher-forced audio in quick previews (default: on)')
    p.add_argument('--no-val-audio-teacher', dest='val_audio_teacher', action='store_false',
                   help='Disable teacher-forced audio in quick previews')
    p.add_argument('--val-audio-deemph', type=float, default=0.85,
                   help='De-emphasis coefficient for preview audio (0 disables; default 0.85 to match eval tool)')
    p.add_argument('--val-audio-teacher-no-adapter', action='store_true', default=False,
                   help='Use raw GT features (no adapter) for teacher-forced preview (A/B for vocoder issues)')
    p.add_argument('--val-audio-no-preheat', action='store_true', default=True,
                   help='Do not use preheat audio in preview synthesis (match generate_10s_audio.py)')
    # p.add_argument('--profile-interval', type=int, default=0, help='If >0, include per-step timings every N steps')
    # Checkpointing controls
    p.add_argument('--save-every-epochs', type=int, default=0, help='Save a checkpoint every N epochs (0 disables)')
    p.add_argument('--save-every-steps', type=int, default=0, help='Save a checkpoint every N global steps (0 disables)')
    p.add_argument('--always-save-last', action='store_true', help='Always save a final checkpoint at the end')

    # Combined multi-expert dataset (mixed batch training)
    p.add_argument('--combined-data-root', type=str, default=None,
                   help='If set, combine four expert subsets under this root into mixed batches')
    p.add_argument('--mix-ratio', type=str, default=None,
                   help='Comma-separated ratios for [harmonic,transient,burst_inpaint,low_snr] in combined mode')

    # Stage3ç‰¹å®šé…ç½® (æŒ‰ä»»åŠ¡è¦æ±‚)
    p.add_argument('--moe', action='store_true', default=True, help='Enable MoE (Stage3 default: enabled)')
    p.add_argument('--no-moe', action='store_true', help='ğŸš¨ Disable MoE for debugging (ä¸´æ—¶è¯Šæ–­é€‰é¡¹)')
    # p.add_argument('--enable-rate', action='store_true',
    #                help='Enable rate regularizer (Stage3é»˜è®¤ç¦ç”¨ï¼Œå½“å‰å®ç°å ä½)')
    p.add_argument('--router-no-csi', action='store_true', default=True,
                   help='Routerä¸ä½¿ç”¨CSI (Stage3å•å˜é‡éªŒè¯)')
    # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
    # p.add_argument('--gradient-accumulation-steps', type=int, default=1,
    #                help='(unused placeholder)')
    p.add_argument('--use-compile', action='store_true', default=False,
                   help='Use torch.compile for model optimization (PyTorch 2.0+)')
    p.add_argument('--moe-token-warmup-steps', type=int, default=0,
                   help='Use sample-level routing only for first N steps, then enable token-level')
    # p.add_argument('--debug-nan', action='store_true', default=False,
    #                help='(unused placeholder)')
    # p.add_argument('--safe-init', action='store_true', default=True,
    #                help='(unused placeholder)')
    p.add_argument('--emergency-bypass-moe', action='store_true', default=False,
                   help='ğŸš¨ Emergency: completely bypass MoE for NaN diagnosis')
    p.add_argument('--wave-stride', type=int, default=1,
                help='Compute vocoder/audio loss every N batches (default 1 = every batch)')
    p.add_argument('--wave-loss-seconds', type=float, default=6.0,
                help='Cap the audio segment length used for wave loss (seconds, default 6.0)')
    # p.add_argument('--wave-min-bp', type=float, default=0.1,
    #             help='(unused placeholder)')
    p.add_argument('--router-jitter', type=float, default=0.01,
                help='è®­ç»ƒæ€å¯¹è·¯ç”± logits æ–½åŠ çš„é«˜æ–¯æŠ–åŠ¨å¼ºåº¦ï¼Œç”¨äºä¿ƒä½¿ä¸“å®¶æ¢ç´¢ï¼ˆé»˜è®¤ 0.01ï¼‰')
    # ç›´æµé€šè·¯ç›¸å…³å‚æ•° - æ¶æ„çº§ç»•è¿‡ä¼˜åŒ–
    p.add_argument('--enable-direct-pathway', action='store_true', default=True,
                help='å¯ç”¨MoEç›´æµé€šè·¯ï¼Œç”¨äºæ€§èƒ½å¯¹æ¯”éªŒè¯ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    p.add_argument('--disable-direct-pathway', action='store_true', default=False,
                help='ç¦ç”¨ç›´æµé€šè·¯ï¼Œä½¿ç”¨çº¯ä¸“å®¶ç³»ç»Ÿï¼ˆç”¨äºå¯¹ç…§å®éªŒï¼‰')
    p.add_argument('--initial-bypass-weight', type=float, default=0.1,
                help='ç›´æµé€šè·¯åˆå§‹æƒé‡ (0.0-1.0)ï¼Œæ§åˆ¶è®­ç»ƒå¼€å§‹æ—¶ç›´æµvsä¸“å®¶çš„æ¯”ä¾‹ï¼ŒStage3é»˜è®¤0.1å¼ºåˆ¶expertè®­ç»ƒ')
    p.add_argument('--adaptive-threshold', type=float, default=0.15,
                help='æ€§èƒ½å·®å¼‚é˜ˆå€¼ï¼Œè§¦å‘æƒé‡è°ƒæ•´ï¼ˆé»˜è®¤15%ï¼Œå¢å¤§ä»¥å‡å°‘ç›´æµé€šè·¯å¹²é¢„ï¼‰')
    p.add_argument('--pathway-warmup-steps', type=int, default=2000,
                help='ç›´æµæƒé‡warmupæ­¥æ•°ï¼Œå‰ä¸€åŠä¸ºæ¶æ„çº§ç»•è¿‡æœŸ')

    # MoE structure knobs
    p.add_argument('--n-experts', type=int, default=4, help='Number of experts (default 4)')
    p.add_argument('--expert-dropout', type=float, default=0.1, help='Expert dropout probability (default 0.1)')

    args = p.parse_args()

    # å¼ºåˆ¶Stage3é…ç½® (æŒ‰ä»»åŠ¡æ¸…å•è¦æ±‚)
    if args.no_moe:
        safe_print("ğŸš¨ DEBUG: Disabling MoE for gradient explosion diagnosis")
        args.moe = False
    elif not args.moe:
        safe_print("âš ï¸  Warning: Force enabling MoE for Stage3 (per task requirements)")
        args.moe = True

    # ç›´æµé€šè·¯é…ç½®å¤„ç†
    if args.disable_direct_pathway:
        args.enable_direct_pathway = False
        safe_print("ğŸš¨ ç›´æµé€šè·¯å·²ç¦ç”¨ - çº¯ä¸“å®¶ç³»ç»Ÿæ¨¡å¼")
    elif args.enable_direct_pathway:
        safe_print(f"âœ… ç›´æµé€šè·¯å·²å¯ç”¨ - åˆå§‹æƒé‡: {args.initial_bypass_weight:.2f}")
        safe_print(f"   è‡ªé€‚åº”é˜ˆå€¼: {args.adaptive_threshold:.3f}, Warmupæ­¥æ•°: {args.pathway_warmup_steps}")

    # è·å–Stage3é…ç½®
    stage_cfg = get_stage_config("stage3")
    # ç¡®ä¿ç¦ç”¨FiLM (å•å˜é‡éªŒè¯è¦æ±‚)
    stage_cfg.use_film = False
    stage_cfg.apply_channel = False  # ç¦ç”¨ä¿¡é“æ¨¡æ‹Ÿ

    safe_print("ğŸš€ Starting Stage3 Training - MoEå¼•å…¥ (ç¦ç”¨FiLMï¼Œå•å˜é‡éªŒè¯)")
    safe_print(f"   MoE enabled: {args.moe}")
    safe_print(f"   FiLM disabled: {not stage_cfg.use_film}")
    safe_print(f"   Channel simulation disabled: {not stage_cfg.apply_channel}")
    safe_print(f"   Router strategy: {'no-CSI' if args.router_no_csi else 'with-CSI'}")
    if args.expert_focus:
        safe_print(f"   Router supervised warmup: focus={args.expert_focus}, sup_w={args.router_sup}")
    safe_print(f"   AMP mode: {args.amp}")
    safe_print(f"   DataLoader: workers={args.num_workers} | stride_frames={args.stride_frames}")
    safe_print(f"   Performance optimizations: torch_compile={args.use_compile}")

    device = torch.device('cuda' if (args.device == 'auto' and torch.cuda.is_available()) else
                         args.device if args.device != 'auto' else 'cpu')
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Data loader (supports combined mixed-batch mode)
    if getattr(args, 'combined_data_root', None):
        # Build combined loader from four expert subsets
        train_loader, dataset = create_combined_data_loader(
            data_root=args.combined_data_root,
            sequence_length=args.seq_len,
            batch_size=args.batch_size,
            frame_size=160,
            stride_frames=args.stride_frames,
            energy_selection=True,
            feature_dims=args.feature_dims,
            num_workers=max(1, int(args.num_workers)),
        )
        # Optional custom mix ratio
        if getattr(args, 'mix_ratio', None):
            try:
                ratios = [float(x.strip()) for x in str(args.mix_ratio).split(',')]
                assert len(ratios) == 4
                import numpy as _np
                dataset.mix_ratio = _np.array(ratios, dtype=_np.float64)
                s = dataset.mix_ratio.sum()
                dataset.mix_ratio = dataset.mix_ratio / (s if s > 0 else 1.0)
                dataset.cumprob = _np.cumsum(dataset.mix_ratio)
                safe_print(f"   Combined mix ratio set to: {dataset.mix_ratio.tolist()}")
            except Exception:
                safe_print("âš ï¸  Invalid --mix-ratio format; expected 'a,b,c,d'. Using defaults.")
    else:
        train_loader, dataset = create_aether_data_loader(
            data_dir=str(Path(args.features).parent.parent) if 'lmr_export' in Path(args.features).parts else str(Path(args.features).parent),
            sequence_length=args.seq_len,
            batch_size=args.batch_size,
            max_samples=None,
            num_workers=max(1, int(args.num_workers)),
            energy_selection=True,
            test_mode=False,
            feature_spec_type='fargan',
            features_file=args.features,
            audio_file=args.pcm,
            stride_frames=args.stride_frames,  # æ–°å¢æ­¥å¹…é…ç½®
        )

    # Create models using simplified architecture
    config = {
        "d_in": args.feature_dims,
        "d_model": 128,
        "dz": 24,
        "d_csi": 10,
        "use_film": False,  # Stage3: ç¦ç”¨FiLM
        "use_moe": args.moe,  # Stage3: å¯ç”¨MoE
        "use_quantization": False,
        "latent_bits": 4,
        "n_experts": int(getattr(args, 'n_experts', 4)),
        "top_k": 2,        # Stage3: TOP-2è·¯ç”±
        "moe_router_use_csi": (not args.router_no_csi),  # Routerä¸ä½¿ç”¨CSI â†’ False
        "use_semantic_head": True,
        "semantic_dim": 6,
        "semantic_source": args.semantic_source,
        # å°†è¾“å…¥åˆ†æ´¾åˆ° DualStream ä¸¤æ¡é€šè·¯ï¼ˆå‰20â†’Ribbonï¼Œå16â†’Threadï¼‰
        "split_stream_inputs": bool(getattr(args, 'split_stream_inputs', False)),
        # ç›´æµé€šè·¯é…ç½®
        "enable_direct_pathway": args.enable_direct_pathway,
        "initial_bypass_weight": args.initial_bypass_weight,
        "adaptive_threshold": args.adaptive_threshold,
        "pathway_warmup_steps": args.pathway_warmup_steps,
        # MoE training knobs
        "expert_dropout": float(getattr(args, 'expert_dropout', 0.1)),
    }

    encoder, _ = create_aether_codec(config)
    encoder = encoder.to(device)
    if hasattr(encoder, 'moe') and hasattr(encoder.moe, 'specialized_moe'):
        try:
            encoder.moe.specialized_moe.router_jitter = float(getattr(args, 'router_jitter', 0.0))
        except Exception:
            pass
    def _cast_rnns_fp32(module: torch.nn.Module):
        for m in module.modules():
            name = m.__class__.__name__
            if name in ('LSTM', 'GRU', 'RobustLSTM', 'RobustGRU'):
                m.to(torch.float32)

    _cast_rnns_fp32(encoder)



    # ğŸš¨ ç´§æ€¥ç»•è¿‡MoEæ¨¡å¼
    if args.emergency_bypass_moe and hasattr(encoder, 'moe') and encoder.moe is not None:
        safe_print("ğŸš¨ EMERGENCY: Bypassing MoE completely for NaN diagnosis")
        encoder.moe.specialized_moe._emergency_bypass = True

    # è‡ªå®šä¹‰decoder with FARGAN
    decoder = AETHERFARGANDecoder(
        d_out=args.feature_dims,
        d_csi=10,
        enable_synth=True,
        use_film=False  # Stage3: è§£ç ç«¯ç¦ç”¨FiLM
    ).to(device)

    # åˆ›å»ºè¯­ä¹‰ç‰¹å¾æå–å™¨ï¼ˆSSL+æŠ•å½±+å¸§ç‡å¯¹é½ï¼‰
    semantic_extractor = create_semantic_extractor(
        model_name="hubert-base",  # ä½¿ç”¨HuBERTä½œä¸ºSSLæ¨¡å‹
        proj_dim=16,               # 16ç»´è¯­ä¹‰ç‰¹å¾
        device=device
    )
    safe_print(f"  âœ… Semantic extractor initialized: {semantic_extractor.ssl_model_name}")
    # ç¡®ä¿æå–å™¨åœ¨ç›®æ ‡è®¾å¤‡
    try:
        semantic_extractor.to(device)
    except Exception:
        pass
    semantic_extractor.eval()  # SSLæ¨¡å‹ä¿æŒevalæ¨¡å¼
    # å°†æå–å™¨æ³¨å…¥åˆ° train_one_epoch ç¼“å­˜ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
    try:
        train_one_epoch._semantic_extractor = semantic_extractor
    except Exception:
        pass

    # ğŸ”§ FIX 6: æ¡ä»¶æ€§mixed precisioné…ç½®ï¼Œé¿å…dtypeå†²çª
    # ä»…åœ¨éAMPæ¨¡å¼ä¸‹å¼ºåˆ¶float32ï¼ŒAMPæ¨¡å¼ä¸‹ä¿æŒä¸€è‡´æ€§
    if args.amp == 'none':
        decoder.fargan_core.float()  # éAMPæ¨¡å¼ä½¿ç”¨float32ç¡®ä¿ç¨³å®š
    # AMPæ¨¡å¼ä¸‹è®©autocastè‡ªåŠ¨ç®¡ç†dtypeï¼Œé¿å…å†²çª
    _cast_rnns_fp32(decoder)
    # å¯é€‰çš„æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
    if args.use_compile:
        try:
            safe_print("  ğŸš€ Compiling models with torch.compile...")
            encoder = torch.compile(encoder, mode='default')
            decoder = torch.compile(decoder, mode='default')
            safe_print("  âœ… Model compilation successful")
        except Exception as e:
            safe_print(f"  âš ï¸  Model compilation failed: {e}")
            safe_print("  ğŸ“‹ Continuing without compilation")

    # Optional: Load Stage 1 checkpoint (AETHER warm start)
    if args.stage1_checkpoint:
        try:
            stage1_ckpt = torch.load(args.stage1_checkpoint, map_location='cpu')
            enc_sd = stage1_ckpt.get('encoder_state_dict') or {}
            dec_sd = stage1_ckpt.get('decoder_state_dict') or {}

            # Load encoder with shape check
            enc_state = encoder.state_dict()
            enc_loaded = enc_skipped = 0
            for k, v in enc_sd.items():
                if k in enc_state and enc_state[k].shape == v.shape:
                    enc_state[k] = v
                    enc_loaded += 1
                else:
                    enc_skipped += 1
            encoder.load_state_dict(enc_state, strict=False)
            safe_print(f"âœ… Stage1: loaded encoder params: {enc_loaded} matched, {enc_skipped} skipped")

            # Partially load decoder (Stage1 AETHERDecoder â†’ AETHERFARGANDecoder common parts)
            dec_state = decoder.state_dict()
            dec_loaded = dec_skipped = 0
            for k, v in dec_sd.items():
                if k in dec_state and dec_state[k].shape == v.shape:
                    dec_state[k] = v
                    dec_loaded += 1
                else:
                    dec_skipped += 1
            decoder.load_state_dict(dec_state, strict=False)
            safe_print(f"âœ… Stage1: loaded decoder params: {dec_loaded} matched, {dec_skipped} skipped (strict=False)")
        except Exception as e:
            safe_print(f"âš ï¸  Failed to load Stage1 checkpoint: {e}")
    def force_vocoder_fp32(decoder):
        # åªæŠŠ fargan_core å¼ºåˆ¶åˆ° float32ï¼Œå…¶ä»–è§£ç å¤´ç»§ç»­è·Ÿéš AMP
        decoder.fargan_core.float()
        for m in decoder.fargan_core.modules():
            name = m.__class__.__name__
            if name in ('LSTM', 'GRU', 'RobustLSTM', 'RobustGRU'):
                m.to(torch.float32)
    # Load FARGAN weights if provided
    if args.fargan_checkpoint:
        try:
            ckpt = torch.load(args.fargan_checkpoint, map_location='cpu')
            state = ckpt.get('model_state_dict', ckpt.get('state_dict', ckpt))

            # æ›´é²æ£’çš„åŠ è½½ï¼šåŒ¹é…æ‰€æœ‰å½¢çŠ¶ä¸€è‡´çš„é”®ï¼Œè‡ªåŠ¨å‰¥ç¦»å¯èƒ½çš„å‰ç¼€
            decoder_state = decoder.state_dict()
            loaded = skipped = 0
            # é¢å¤–ï¼šå¤„ç†æ—§ç‰ˆ weight_norm (weight_v/weight_g) â†’ æ–°ç‰ˆ parametrizations.weight.original0
            # æ”¶é›†g/vå¯¹
            vg_pairs = {}
            for k in list(state.keys()):
                if k.endswith('.weight_v'):
                    base = k[:-9]
                    gk = base + '.weight_g'
                    if gk in state:
                        vg_pairs[base] = (state[k], state[gk])

            for k, v in state.items():
                cand_keys = [k]
                if k.startswith('module.'):
                    cand_keys.append(k[len('module.'):])
                if k.startswith('model.'):
                    cand_keys.append(k[len('model.'):])
                matched = False
                for kk in cand_keys:
                    if kk in decoder_state and decoder_state[kk].shape == v.shape:
                        decoder_state[kk] = v
                        loaded += 1
                        matched = True
                        break
                if not matched:
                    skipped += 1

            # å°è¯•å°†æ—§ç‰ˆ weight_norm çš„ g/v è½¬æ¢ä¸ºæ–°ç‰ˆ parametrizations æƒé‡
            import torch as _t
            for base, (v_t, g_t) in vg_pairs.items():
                # ç›®æ ‡é”®ï¼šparametrizations.weight.original0
                target_key = base + '.parametrizations.weight.original0'
                if target_key in decoder_state:
                    try:
                        v_tensor = _t.as_tensor(v_t)
                        g_tensor = _t.as_tensor(g_t)
                        # é‡å»ºæƒé‡ï¼šw = v * (g / ||v||)
                        v_norm = v_tensor.norm(dim=list(range(1, v_tensor.ndim)), keepdim=True) if v_tensor.ndim > 1 else v_tensor.abs() + 1e-8
                        scale = g_tensor / (v_norm + 1e-8)
                        while scale.ndim < v_tensor.ndim:
                            scale = scale.unsqueeze(-1)
                        w_recon = v_tensor * scale
                        if w_recon.shape == decoder_state[target_key].shape:
                            decoder_state[target_key] = w_recon
                            loaded += 1
                    except Exception:
                        continue

            # ç¬¬äºŒè½®ï¼šå°è¯•å¸¸è§å‰ç¼€æ˜ å°„ï¼ˆcondâ†’cond_net, sigâ†’sig_net, fargan_condâ†’cond_net, fargan_subâ†’sig_netï¼‰
            if skipped > 0:
                remap_rules = [
                    ('cond.', 'cond_net.'),
                    ('sig.',  'sig_net.'),
                    ('fargan_cond.', 'cond_net.'),
                    ('fargan_sub.',  'sig_net.'),
                ]
                for k, v in list(state.items()):
                    # è·³è¿‡å·²åŒ¹é…è¿‡çš„é”®
                    if any([(alt in decoder_state) and (decoder_state[alt].shape == v.shape) for alt in [k]]):
                        continue
                    for a, b in remap_rules:
                        if a in k:
                            kk = k.replace(a, b)
                            if kk in decoder_state and decoder_state[kk].shape == v.shape:
                                decoder_state[kk] = v
                                loaded += 1
                                skipped -= 1
                                break

            decoder.load_state_dict(decoder_state, strict=False)
            safe_print(f"âœ… Stage2(FARGAN): loaded {loaded} params into decoder, skipped {skipped}")
            # æ‰“å°éƒ¨åˆ†æœªåŒ¹é…é”®ï¼Œä¾¿äºè¿›ä¸€æ­¥ç²¾ç¡®æ˜ å°„
            if skipped > 0:
                try:
                    missing = []
                    for k, v in state.items():
                        found = False
                        if k in decoder_state and decoder_state[k].shape == v.shape:
                            found = True
                        else:
                            # æ£€æŸ¥å·²åº”ç”¨çš„å‰ç¼€æ›¿æ¢
                            for a, b in [('cond.', 'cond_net.'), ('sig.', 'sig_net.'), ('fargan_cond.', 'cond_net.'), ('fargan_sub.', 'sig_net.')]:
                                if a in k:
                                    kk = k.replace(a, b)
                                    if kk in decoder_state and decoder_state[kk].shape == v.shape:
                                        found = True
                                        break
                            # æ£€æŸ¥ weight_norm é‡å»ºç›®æ ‡é”®
                            if not found and k.endswith('.weight_v'):
                                base = k[:-9]
                                target_key = base + '.parametrizations.weight.original0'
                                if target_key in decoder_state and decoder_state[target_key].shape == v.shape:
                                    found = True
                        if not found:
                            missing.append(k)
                    if missing:
                        show = missing[:10]
                        safe_print(f"   âš ï¸ Unmatched keys (sample): {show}")
                except Exception:
                    pass
        except Exception as e:
            safe_print(f"âš ï¸  Failed to load FARGAN checkpoint: {e}")
        # ğŸ”§ FIX 7: åº”ç”¨ä¸€è‡´çš„mixed precisionç­–ç•¥
        if args.amp == 'none':
            force_vocoder_fp32(decoder)  # ä»…éAMPæ¨¡å¼å¼ºåˆ¶float32

    # Optimizer with differential learning rates
    lr = getattr(stage_cfg, 'learning_rate', 2e-4)

    # FARGANï¼šæ ¸å¿ƒä¸ parametrizations å•ç‹¬ param groupï¼ˆæ›´å° lrï¼Œä¸”ç¦ WDï¼‰
    decoder_params, fargan_core_params, fargan_parametrizations = [], [], []
    for name, p in decoder.named_parameters():
        if 'parametrizations' in name:
            fargan_parametrizations.append(p)
        elif 'fargan_core' in name:
            fargan_core_params.append(p)
        else:
            decoder_params.append(p)
    # åœ¨åˆ›å»º optimizer ä¹‹å‰ï¼Œæ›¿æ¢ encoder çš„åˆ†ç»„ï¼š
    enc_backbone, enc_attn = [], []
    for n, p in encoder.named_parameters():
        if any(k in n for k in ['thread_blocks', 'qkv', 'out_proj', 'mix']):
            enc_attn.append(p)       # æ³¨æ„åŠ›/æ··åˆç›¸å…³
        else:
            enc_backbone.append(p)   # å…¶å®ƒ
    param_groups = [
        {'params': enc_backbone, 'lr': lr,      'weight_decay': 1e-6},
        {'params': enc_attn,     'lr': lr*0.5,  'weight_decay': 1e-6},  # æ³¨æ„åŠ›å±‚åŠé€Ÿ,
        {'params': decoder_params,                  'lr': lr,       'weight_decay': 1e-6},
        {'params': fargan_core_params,              'lr': lr*0.1,   'weight_decay': 0.0},
        {'params': fargan_parametrizations,         'lr': lr*0.1,   'weight_decay': 0.0},
    ]


    optimizer = optim.AdamW(param_groups, weight_decay=1e-6)
    use_fp16 = (args.amp == 'fp16')
    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)
    accum_steps = max(1, int(getattr(args, 'gradient_accumulation_steps', 1)))
    # Optional resume from previous Stage3 checkpoint (encoder/decoder/optimizer)
    if getattr(args, 'resume', None):
        from pathlib import Path as _P
        _rp = _P(args.resume)
        if _rp.exists():
            try:
                _ck = torch.load(str(_rp), map_location='cpu')
                if 'encoder_state_dict' in _ck:
                    encoder.load_state_dict(_ck['encoder_state_dict'], strict=False)
                if 'decoder_state_dict' in _ck:
                    decoder.load_state_dict(_ck['decoder_state_dict'], strict=False)
                if 'optimizer_state_dict' in _ck:
                    try:
                        optimizer.load_state_dict(_ck['optimizer_state_dict'])
                    except Exception:
                        pass
                best_loss = float(_ck.get('loss', float('inf')))
                safe_print(f"ğŸ” Resumed from: {args.resume} (best_loss={best_loss:.6f})")
            except Exception as _e:
                safe_print(f"âš ï¸  Failed to resume from {args.resume}: {_e}")

    safe_print(f"ğŸ“Š Training setup:")
    safe_print(f"   Model params: Encoder={sum(p.numel() for p in encoder.parameters()):,}, "
          f"Decoder={sum(p.numel() for p in decoder.parameters()):,}")
    safe_print(f"   Learning rate: {lr}")
    safe_print(f"   Batch size: {args.batch_size}, Sequence length: {args.seq_len}")
    safe_print(f"   Total batches per epoch: {len(train_loader)}")

    # Training loop
    best_loss = float('inf')
    global_step = 0

    safe_print("\nğŸ¯ Stage3 éªŒæ”¶æ ‡å‡†:")
    safe_print("   - ç‰¹å¾é‡å»ºæŸå¤± â‰¤ 0.25")
    safe_print("   - MoEä¸“å®¶åˆ©ç”¨ç‡ > 75% (3ä¸ªä¸“å®¶å‡è¡¡ä½¿ç”¨: Harmonic, Transient, BurstInpaint)")
    safe_print("   - æ³¢å½¢RMSåç§» < 5dB")
    safe_print("   - MoEæŸå¤±æ”¶æ•›ç¨³å®š")
    safe_print("   - æ— FiLMæ¡ä»¶ä¸‹è®­ç»ƒç¨³å®š")
    safe_print("   - è·³è¿‡LowSNRExpert (æ— CSIè¾“å…¥)")
    safe_print(f"   - Rate loss: ç¦ç”¨ (Stage3é»˜è®¤ç¦ç”¨ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸)\n")

    # CUDA backend optimisations
    if device.type == 'cuda':
        try:
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.set_float32_matmul_precision('high')
        except Exception:
            pass

    # å¼ºåˆ¶flushæ‰€æœ‰é…ç½®ä¿¡æ¯ï¼Œç¡®ä¿åœ¨è¿›åº¦æ¡å‡ºç°å‰æ˜¾ç¤º
    import sys
    sys.stdout.flush()
    sys.stderr.flush()

    for epoch in range(1, args.epochs + 1):
        safe_print(f"ğŸ”„ Epoch {epoch}/{args.epochs}")

        epoch_metrics, global_step = train_one_epoch(
            encoder, decoder, train_loader, device, optimizer,
            stage_cfg, global_step, args, epoch_idx=epoch,
            scaler=scaler, 
        )

        # è¯­ä¹‰æ„ŸçŸ¥ç³»ç»ŸEpochæ€»ç»“
        safe_print(f"[Semantic Audio System] Epoch {epoch} Summary:")
        safe_print(f"   Total Loss: {epoch_metrics['total_loss']:.6f}")
        safe_print(f"   Feature Reconstruction: {epoch_metrics['feature_loss']:.6f}")
        safe_print(f"   Audio Quality (Usability): {epoch_metrics['wave_loss']:.6f}")
        safe_print(f"   Expert Routing: {epoch_metrics['moe_loss']:.6f}")
        safe_print(f"   Rate Control: {epoch_metrics['rate_loss']:.6f}")

        # è¯­ä¹‰æŸå¤±ä¿¡æ¯
        if 'semantic_loss' in epoch_metrics:
            safe_print(f"   Semantic Alignment: {epoch_metrics['semantic_loss']:.6f}")

        # MoE health check (åªåœ¨éç»•è¿‡æ¨¡å¼ä¸‹æ˜¾ç¤º)
        if args.emergency_bypass_moe:
            safe_print(f"   [WARNING] MoE Status: BYPASSED (emergency diagnosis mode)")
        elif encoder.use_moe:
            expert_min = epoch_metrics.get('expert_usage_min', 0)
            expert_max = epoch_metrics.get('expert_usage_max', 0)
            expert_entropy = epoch_metrics.get('expert_entropy', 0)
            expert_balance = 1.0 - (expert_max - expert_min)  # å‡è¡¡åº¦

            # Display individual expert usage rates if available - åŠ¨æ€ä¸“å®¶æ•°é‡
            n_experts = getattr(encoder.moe, 'n_experts', 4) if hasattr(encoder, 'moe') else 4
            expert_usage_display = []
            expert_names = ["Harmonic", "Transient", "BurstInpaint", "LowSNR"][:n_experts]  # æ ¹æ®ä¸“å®¶æ•°é‡æˆªæ–­

            # æ„å»ºå¸¦ä¸“å®¶åç§°çš„ä½¿ç”¨ç‡æ˜¾ç¤º
            expert_usage_named = []
            for i in range(n_experts):  # åŠ¨æ€ä¸“å®¶æ•°é‡
                usage_key = f'expert_{i}_usage'
                name = expert_names[i] if i < len(expert_names) else f"E{i}"
                if usage_key in epoch_metrics:
                    usage_value = f"{epoch_metrics[usage_key]:.3f}"
                    expert_usage_display.append(usage_value)
                    expert_usage_named.append(f"{name}:{usage_value}")
                else:
                    expert_usage_display.append("N/A")
                    expert_usage_named.append(f"{name}:N/A")

            safe_print(f"   MoE Health:")
            safe_print(f"     Expert Usage: [{', '.join(expert_usage_named)}]")
            for i, (name, usage) in enumerate(zip(expert_names, expert_usage_display)):
                safe_print(f"       E{i+1} {name}: {usage}")
            safe_print(f"     Expert Entropy: {expert_entropy:.3f}")
            safe_print(f"     Balance Score: {expert_balance:.3f}")

            # éªŒæ”¶æ ‡å‡†æ£€æŸ¥
            if expert_min > 0.2:  # æ¯ä¸ªä¸“å®¶>20%ä½¿ç”¨ç‡
                safe_print("     âœ… Expert utilization criterion met")
            else:
                safe_print("     âŒ Expert utilization below threshold")

            if expert_entropy > 0.8:
                safe_print("     âœ… Expert entropy criterion met")
            else:
                safe_print("     âŒ Expert entropy below threshold")

        # Periodic epoch checkpoint
        if args.save_every_epochs and args.save_every_epochs > 0 and (epoch % args.save_every_epochs == 0):
            epoch_ckpt = {
                'epoch': epoch,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': float(epoch_metrics['total_loss']),
                'metrics': epoch_metrics,
                'config': config,
                'args': vars(args)
            }
            ckpt_path = out_dir / f'stage3_epoch_{epoch}.pth'
            torch.save(epoch_ckpt, ckpt_path)
            safe_print(f"ğŸ’¾ Saved epoch checkpoint: {ckpt_path}")

        # Save best checkpoint
        if epoch_metrics['total_loss'] < best_loss:
            best_loss = epoch_metrics['total_loss']
            checkpoint = {
                'epoch': epoch,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'metrics': epoch_metrics,
                'config': config,
                'args': vars(args)
            }

            ckpt_path = out_dir / 'stage3_best.pth'
            torch.save(checkpoint, ckpt_path)
            # Also save a copy with the epoch suffix for reproducibility
            ckpt_epoch_path = out_dir / f'stage3_best_epoch_{epoch}.pth'
            try:
                torch.save(checkpoint, ckpt_epoch_path)
            except Exception as _e:
                safe_print(f"âš ï¸  Failed to save epoch-suffixed best checkpoint: {ckpt_epoch_path} ({_e})")
            safe_print(f"ğŸ’¾ Saved best checkpoint: {ckpt_path} (epoch copy: {ckpt_epoch_path})")

            # éªŒæ”¶æ ‡å‡†ç»¼åˆæ£€æŸ¥
            feature_loss_ok = epoch_metrics['feature_loss'] <= 0.25
            expert_util_ok = encoder.use_moe and expert_min > 0.2

            safe_print(f"ğŸ“‹ Stage3 éªŒæ”¶è¿›åº¦:")
            safe_print(f"   Feature Loss â‰¤ 0.25: {'âœ…' if feature_loss_ok else 'âŒ'} ({epoch_metrics['feature_loss']:.4f})")
            if encoder.use_moe:
                safe_print(f"   Expert Utilization > 20%: {'âœ…' if expert_util_ok else 'âŒ'} (min={expert_min:.3f})")

    safe_print(f"\nğŸ‰ Stage3 training completed!")
    safe_print(f"   Best loss: {best_loss:.6f}")
    safe_print(f"   Final checkpoint: {out_dir / 'stage3_best.pth'}")

    # Always save last if requested
    if args.always_save_last:
        last_ckpt = {
            'epoch': args.epochs,
            'encoder_state_dict': encoder.state_dict(),
            'decoder_state_dict': decoder.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': float(best_loss),
            'config': config,
            'args': vars(args)
        }
        ckpt_path = out_dir / 'stage3_last.pth'
        torch.save(last_ckpt, ckpt_path)
        safe_print(f"ğŸ’¾ Saved last checkpoint: {ckpt_path}")

    # æœ€ç»ˆéªŒæ”¶æŠ¥å‘Š
    if best_loss <= 0.25:
        safe_print("âœ… Stage3 training PASSED - ready for Stage4")
    else:
        safe_print("âš ï¸  Stage3 training needs improvement - consider adjusting hyperparameters")

    return 0


if __name__ == '__main__':
    raise SystemExit(main())
