#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AETHER Progressive Training - ç²¾ç®€ç‰ˆ
ç§»é™¤äº†è°ƒè¯•ã€wandbæ—¥å¿—ç­‰å†—ä½™ä»£ç ï¼Œä¸“æ³¨æ ¸å¿ƒè®­ç»ƒé€»è¾‘
"""

import argparse
import random
import math
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import traceback
import torchaudio

try:
    from torch.amp import autocast as _autocast
    from torch.amp import GradScaler as _GradScaler
    def _create_grad_scaler(enabled: bool) -> "_GradScaler":
        return _GradScaler(init_scale=64.0, growth_interval=1000, enabled=enabled)
    def _autocast_ctx(enabled: bool):
        return _autocast("cuda", enabled=enabled)
except ImportError:
    from torch.cuda.amp import autocast as _autocast
    from torch.cuda.amp import GradScaler as _GradScaler
    def _create_grad_scaler(enabled: bool) -> "_GradScaler":
        return _GradScaler(enabled=enabled)
    def _autocast_ctx(enabled: bool):
        return _autocast(enabled=enabled)

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from models.aether_encoder_decoder import AETHEREncoder, AETHERDecoder
from training.losses import compute_layered_loss
from training.f0_losses import (
    compute_enhanced_f0_loss,
    compute_f0_variance_regularization,
    audio_f0_alignment_loss,      # â† æ–°å¢
)
 # , compute_f0_constraint_loss


# -- Global constants ------------------------------------------------------- #
SAMPLE_RATE = 16000
FRAME_HOP_SAMPLES = 160  # 10 ms @ 16 kHz


# -- Cached Mel helper ----------------------------------------------------- #
_MEL_CACHE: Dict[torch.device, torchaudio.transforms.MelSpectrogram] = {}

# -- Stage2 FARGAN-only training helpers ---------------------------------- #
def load_frozen_aether_models(checkpoint_path: str, device: torch.device, feature_dim: int = 36) -> Tuple[nn.Module, nn.Module]:
    """åŠ è½½å¹¶å†»ç»“é˜¶æ®µä¸€è®­ç»ƒå¥½çš„Aetherç¼–è§£ç å™¨"""
    from models.aether_encoder_decoder import AETHEREncoder, AETHERDecoder

    print(f"Loading Stage1 Aether models from: {checkpoint_path}")

    # åˆ›å»ºç¼–è§£ç å™¨
    encoder = AETHEREncoder(feature_dim=feature_dim).to(device)
    decoder = AETHERDecoder(feature_dim=feature_dim).to(device)

    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # åŠ è½½æ¨¡å‹æƒé‡
    if 'encoder_state_dict' in checkpoint:
        encoder.load_state_dict(checkpoint['encoder_state_dict'])
    elif 'encoder' in checkpoint:
        encoder.load_state_dict(checkpoint['encoder'])
    else:
        raise KeyError("No encoder state found in checkpoint")

    if 'decoder_state_dict' in checkpoint:
        decoder.load_state_dict(checkpoint['decoder_state_dict'])
    elif 'decoder' in checkpoint:
        decoder.load_state_dict(checkpoint['decoder'])
    else:
        raise KeyError("No decoder state found in checkpoint")

    # å†»ç»“æ‰€æœ‰å‚æ•°
    for param in encoder.parameters():
        param.requires_grad = False
    for param in decoder.parameters():
        param.requires_grad = False

    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    encoder.eval()
    decoder.eval()

    print(f"Frozen Aether models loaded successfully")
    print(f"  Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,} (frozen)")
    print(f"  Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,} (frozen)")

    return encoder, decoder

def create_independent_fargan_wavehead(device: torch.device) -> nn.Module:
    """åˆ›å»ºç‹¬ç«‹çš„FARGANæ³¢å½¢å¤´"""
    from models.fargan_decoder import FARGANDecoder

    fargan_wavehead = FARGANDecoder().to(device)

    # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ
    for param in fargan_wavehead.parameters():
        param.requires_grad = True

    print(f"Independent FARGAN wavehead created")
    print(f"  FARGAN parameters: {sum(p.numel() for p in fargan_wavehead.parameters()):,} (trainable)")

    return fargan_wavehead

def train_stage2_fargan_only(
    frozen_encoder: nn.Module,
    frozen_decoder: nn.Module,
    fargan_wavehead: nn.Module,
    train_loader: Any,
    device: torch.device,
    args: Any,
    checkpoint_dir: Path
) -> Dict[str, float]:
    """
    é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒä¸»å‡½æ•°
    ä½¿ç”¨å†»ç»“çš„Aetherç¼–è§£ç å™¨ + ç‹¬ç«‹è®­ç»ƒFARGANæ³¢å½¢å¤´
    å®Œå…¨å¤ç”¨train_fargan.pyçš„è®­ç»ƒé…ç½®
    """
    print("ğŸ”§ é…ç½®é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒ")

    # å¯¼å…¥train_fargan.pyçš„æŸå¤±å‡½æ•°
    from training.fargan_losses import compute_fargan_training_loss, compute_fargan_original_style_loss

    # åˆ›å»ºä¼˜åŒ–å™¨ - å¤ç”¨train_fargan.pyçš„é…ç½®
    optimizer = optim.AdamW(
        fargan_wavehead.parameters(),
        lr=args.fargan_learning_rate,
        weight_decay=1e-5,
        eps=1e-8,
        betas=(0.8, 0.95),
    )

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - å¤ç”¨train_fargan.pyçš„é…ç½®
    scheduler = optim.lr_scheduler.LambdaLR(
        optimizer, lr_lambda=lambda s: 1.0 / (1.0 + args.fargan_lr_decay * float(s))
    )

    # è®­ç»ƒé…ç½®
    num_epochs = 50  # é»˜è®¤è®­ç»ƒè½®æ•°
    steps_per_epoch = len(train_loader)
    total_steps = num_epochs * steps_per_epoch

    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   å­¦ä¹ ç‡: {args.fargan_learning_rate}")
    print(f"   è¡°å‡ç‡: {args.fargan_lr_decay}")
    print(f"   åŸç‰ˆæŸå¤±è½®æ•°: {args.fargan_original_epochs}")
    print(f"   æ¸å˜è½®æ•°: {args.fargan_ramp_epochs}")
    print(f"   æ€»è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"   æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")

    # è®­ç»ƒæ¨¡å¼
    fargan_wavehead.train()
    frozen_encoder.eval()  # å†»ç»“ç¼–ç å™¨
    frozen_decoder.eval()  # å†»ç»“è§£ç å™¨

    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    step_count = 0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        epoch_steps = 0

        print(f"\nğŸ¯ Epoch {epoch + 1}/{num_epochs}")

        for batch_idx, (features, target_audio) in enumerate(train_loader):
            features = features.to(device)
            target_audio = target_audio.to(device)

            # === é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒæ­¥éª¤ ===
            result = stage2_fargan_only_training_step(
                batch=(features, target_audio),
                frozen_encoder=frozen_encoder,
                frozen_decoder=frozen_decoder,
                fargan_wavehead=fargan_wavehead,
                optimizer=optimizer,
                device=device,
                epoch=epoch,
                step=step_count,
                args=args
            )

            epoch_loss += result['total_loss']
            epoch_steps += 1
            step_count += 1

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()

            # æ‰“å°è¿›åº¦
            if batch_idx % 50 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"   Step {step_count}: Loss={result['total_loss']:.6f}, LR={current_lr:.2e}")

        # Epochç»“æŸç»Ÿè®¡
        avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else float('inf')
        print(f"Epoch {epoch + 1} å®Œæˆ: å¹³å‡æŸå¤±={avg_epoch_loss:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            checkpoint_path = checkpoint_dir / "stage2_fargan_best.pt"
            torch.save({
                'epoch': epoch,
                'fargan_wavehead_state_dict': fargan_wavehead.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': best_loss,
                'step_count': step_count,
                'args': args
            }, checkpoint_path)
            print(f"ğŸ’¾ å·²ä¿å­˜æœ€ä½³æ¨¡å‹: {checkpoint_path}")

    print(f"ğŸ‰ é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒå®Œæˆ! æœ€ä½³æŸå¤±: {best_loss:.6f}")
    return {'best_loss': best_loss, 'total_steps': step_count}


def stage2_fargan_only_training_step(
    batch: Tuple[torch.Tensor, torch.Tensor],
    frozen_encoder: nn.Module,
    frozen_decoder: nn.Module,
    fargan_wavehead: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device,
    epoch: int,
    step: int,
    args: Any
) -> Dict[str, float]:
    """é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒæ­¥éª¤"""
    features, target_audio = batch
    features = features.to(device, non_blocking=True)
    target_audio = target_audio.to(device, non_blocking=True)

    optimizer.zero_grad(set_to_none=True)

    loss_dict = {}

    try:
        # ä½¿ç”¨å†»ç»“çš„ç¼–è§£ç å™¨æå–FARGANç‰¹å¾
        with torch.no_grad():
            encoded = frozen_encoder(features)
            decoded_features = frozen_decoder(encoded)  # [B, T, 36] FARGANç‰¹å¾

        # å‡†å¤‡teacher forcingæ•°æ®
        nb_pre_frames = 2
        pre = target_audio[..., :nb_pre_frames * 160]

        # ä½¿ç”¨FARGANæ³¢å½¢å¤´ç”ŸæˆéŸ³é¢‘
        period, pred_audio = fargan_wavehead(decoded_features, pre=pre)
        pred_audio = pred_audio.squeeze(1)
        pred_audio = torch.cat([pre, pred_audio], dim=-1)

        # å¯¹é½éŸ³é¢‘é•¿åº¦
        min_len = min(pred_audio.size(-1), target_audio.size(-1))
        pred_audio = pred_audio[..., :min_len]
        target_audio = target_audio[..., :min_len]

        # ä½¿ç”¨train_fargan.pyçš„æŸå¤±å‡½æ•°é…ç½®
        from training.fargan_losses import (
            compute_fargan_training_loss,
            compute_fargan_original_style_loss
        )

        # æŸå¤±å‡½æ•°é€‰æ‹©å’Œæƒé‡è°ƒåº¦
        if args.fargan_original_epochs > 0:
            if epoch <= args.fargan_original_epochs:
                alpha = 0.0
            else:
                alpha = 1.0 if args.fargan_ramp_epochs <= 0 else min(
                    1.0, (epoch - args.fargan_original_epochs) / float(args.fargan_ramp_epochs)
                )

            # åŸç‰ˆæŸå¤±
            orig_loss, orig_dict = compute_fargan_original_style_loss(
                pred_audio, target_audio, device=device,
                frame_size=160, focus_start=nb_pre_frames * 160,
            )

            # è®­ç»ƒæŸå¤±
            comp_loss, comp_dict = compute_fargan_training_loss(
                pred_audio, target_audio, period, device=device
            )

            # æ··åˆæŸå¤±
            fargan_loss = (1.0 - alpha) * orig_loss + alpha * comp_loss
            loss_dict.update({f'orig_{k}': v for k, v in orig_dict.items()})
            loss_dict.update(comp_dict)
            loss_dict['alpha'] = alpha
        else:
            # ä»…ä½¿ç”¨è®­ç»ƒæŸå¤±
            fargan_loss, loss_dict = compute_fargan_training_loss(
                pred_audio, target_audio, period, device=device
            )

        fargan_loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(fargan_wavehead.parameters(), max_norm=0.5)

        optimizer.step()

        loss_dict['total_loss'] = fargan_loss.item()

    except Exception as e:
        print(f"Stage2 FARGAN training step failed: {e}")
        traceback.print_exc()
        loss_dict['total_loss'] = float('inf')

    return loss_dict
def _safe_mse(pred: torch.Tensor, target: torch.Tensor, step: int = 0) -> torch.Tensor:
    """å®‰å…¨çš„é‡å»ºæŸå¤±ï¼šå‰1000æ­¥ç”¨SmoothL1ï¼Œåç»­ç”¨MSEã€‚

    æ³¨æ„ï¼šå…ˆåœ¨åŸå§‹å¼ é‡ä¸Šè®¡ç®—æœ‰é™æ€§æ©ç ï¼Œå†è¿›è¡Œæ•°å€¼æ¸…æ´—ï¼Œé¿å…æŠŠéæœ‰é™å€¼
    å˜æˆå¤§å¹…åº¦æœ‰é™å€¼åå‚ä¸æŸå¤±ï¼Œå¯¼è‡´æŸå¤±é£™å‡ã€‚
    """
    # åœ¨æ¸…æ´—å‰è®°å½•å“ªäº›ä½ç½®æ˜¯æœ‰é™çš„
    orig_mask = torch.isfinite(pred) & torch.isfinite(target)
    # æ•°å€¼æ¸…æ´—ï¼ˆä¸æ”¹å˜æ©ç ï¼‰
    pred   = torch.nan_to_num(pred,   nan=0.0, posinf=1e4, neginf=-1e4)
    target = torch.nan_to_num(target, nan=0.0, posinf=1e4, neginf=-1e4)
    mask = orig_mask
    if not mask.any():
        return pred.new_zeros(())

    pred_masked = pred[mask]
    target_masked = target[mask]

    # æ—©æœŸä½¿ç”¨SmoothL1ï¼Œæ›´ç¨³å®šï¼›åæœŸåˆ‡å›MSE
    if step < 1000:
        return F.smooth_l1_loss(pred_masked, target_masked, beta=0.5)
    else:
        diff = pred_masked - target_masked
        return (diff * diff).mean()
def _finite_scalar(x: torch.Tensor, name: str, step: int) -> torch.Tensor:
    """ç¡®ä¿æ ‡é‡æœ‰é™ï¼›å¦åˆ™æ‰“å°å¹¶ç½®é›¶ã€‚"""
    if not isinstance(x, torch.Tensor):
        x = torch.tensor(float(x), device='cuda' if torch.cuda.is_available() else 'cpu')
    if not torch.isfinite(x).all():
        try:
            val = float(x.detach().float().mean().cpu())
        except Exception:
            val = '<?>'
        print(f"âš ï¸ Step {step}: éæœ‰é™æŸå¤±é¡¹ -> {name}={val} ; å·²ç½®é›¶")
        return torch.zeros((), device=x.device, dtype=x.dtype)
    return x

def _sanitize_f0_losses(f0_losses: dict, step: int) -> dict:
    """é€é¡¹å‡€åŒ– f0 æŸå¤±å­—å…¸ï¼Œè¿”å›åŒåæ–°å­—å…¸ã€‚"""
    clean = {}
    for k, v in f0_losses.items():
        if isinstance(v, torch.Tensor):
            if not torch.isfinite(v).all():
                try:
                    val = float(v.detach().float().mean().cpu())
                except Exception:
                    val = '<?>'
                print(f"âš ï¸ Step {step}: f0_losses['{k}'] éæœ‰é™={val} -> ç½®é›¶")
                v = torch.zeros((), device=v.device, dtype=v.dtype)
            clean[k] = v
        else:
            clean[k] = v
    return clean

def _clean_tensor(x: torch.Tensor,
                  clip: float = 1e4) -> torch.Tensor:
    """
    æ•°å€¼æ¸…æ´—ï¼šæŠŠ NaN/Â±Inf å˜æˆæœ‰é™å€¼ï¼Œå¹¶åšä¸€æ¬¡å¯é€‰è½¯é™å¹…ï¼Œé¿å…æç«¯å€¼æŠŠæŸå¤±æ‹‰çˆ†ã€‚
    """
    x = torch.nan_to_num(x, nan=0.0, posinf=clip, neginf=-clip)
    if clip is not None and clip > 0:
        x = x.clamp(min=-clip, max=clip)
    return x


def _get_mel_transform(device: torch.device) -> torchaudio.transforms.MelSpectrogram:
    """Create or fetch a cached 80-bin Mel transform on the requested device."""
    transform = _MEL_CACHE.get(device)
    if transform is None:
        transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=SAMPLE_RATE,
            n_fft=1024,
            win_length=400,
            hop_length=FRAME_HOP_SAMPLES,
            f_min=50.0,
            f_max=7600.0,
            n_mels=80,
            window_fn=torch.hann_window,
            center=True,
            pad_mode="reflect",
            power=2.0,
            norm="slaney",
            mel_scale="slaney",
        ).to(device)
        _MEL_CACHE[device] = transform
    else:
        transform = transform.to(device)
    return transform


def _logmel_80(x: torch.Tensor) -> torch.Tensor:
    """
    Compute log-mel features pooled over time, producing shape [B, 80].
    Accepts [B, T], [B, 1, T] or [T].
    """
    if x.dim() == 1:
        x = x.unsqueeze(0)
    if x.dim() == 3:
        x = x.squeeze(1)

    # ç¡®ä¿è¾“å…¥æ˜¯2Dæ³¢å½¢ [B, T]
    if x.dim() != 2:
        raise ValueError(f"Expected 2D input [B, T], got {x.shape}")

    device = x.device
    mel_transform = _get_mel_transform(device)

    try:
        mel = mel_transform(x)  # [B, 80, Frames]

        # éªŒè¯melè¾“å‡ºç»´åº¦
        if mel.dim() != 3 or mel.size(1) != 80:
            # å¦‚æœmel_transformè¾“å‡ºä¸æ˜¯[B, 80, T]æ ¼å¼ï¼Œå¼ºåˆ¶ä¿®å¤
            print(f"âš ï¸ Mel transformè¾“å‡ºå¼‚å¸¸: {mel.shape}, é¢„æœŸ: [B, 80, T]")
            # ä½¿ç”¨STFTåæ‰‹åŠ¨è®¡ç®—melè°±
            stft = torch.stft(x, n_fft=1024, hop_length=FRAME_HOP_SAMPLES,
                            win_length=400, center=True, return_complex=True)
            magnitude = torch.abs(stft)  # [B, freq_bins, time]

            # å¦‚æœé¢‘ç‡binsä¸æ˜¯513ï¼Œè£å‰ªæˆ–å¡«å……åˆ°åˆç†èŒƒå›´
            if magnitude.size(1) != 513:
                magnitude = F.interpolate(magnitude.unsqueeze(1),
                                       size=(513, magnitude.size(-1)),
                                       mode='bilinear', align_corners=False).squeeze(1)

            # ä½¿ç”¨ç°æœ‰çš„mel_transformçš„filter bankçŸ©é˜µ
            mel_filters = mel_transform.mel_scale.fb
            if mel_filters.size(0) != 80 or mel_filters.size(1) != magnitude.size(1):
                # é‡æ–°åˆ›å»ºæ­£ç¡®çš„mel filter bank
                from torchaudio.functional import melscale_fbanks
                mel_filters = melscale_fbanks(
                    n_freqs=magnitude.size(1), f_min=50.0, f_max=7600.0,
                    n_mels=80, sample_rate=SAMPLE_RATE
                ).to(device)

            mel = torch.matmul(mel_filters, magnitude)  # [B, 80, time]

        mel = (mel + 1e-8).log()
        mel_pooled = mel.mean(dim=-1)  # [B, 80]
        mel_norm = F.layer_norm(mel_pooled, mel_pooled.shape[-1:])
        return mel_norm

    except Exception as e:
        print(f"âš ï¸ Mel spectrogramè®¡ç®—å¤±è´¥: {e}")
        # å›é€€ï¼šè¿”å›é›¶å‘é‡
        return torch.zeros(x.size(0), 80, device=device, dtype=x.dtype)

# ğŸš€ GPUä¼˜åŒ–çš„è½»é‡MR-STFT Loss (é¢„çƒ­ä¸“ç”¨)
# --- progressive_train_clean.py ---

import torch
import torch.nn as nn
import torch.nn.functional as F
from collections.abc import Sequence

# --- BEGIN PATCH: robust MR-STFT loss ----------------------------------------
class MRSTFTLoss(nn.Module):
    """
    å…¼å®¹ä¸¤ç§æ„é€ æ–¹å¼ï¼š
      1) MRSTFTLoss(cfgs=[(n_fft, hop, win), ...])
      2) MRSTFTLoss(fft_sizes=(...), hop_sizes=(...), win_sizes=(...), alpha_l1=..., alpha_mag=..., alpha_sc=...)
    å¹¶åšæ•°å€¼ç¨³å®šå¤„ç†ï¼Œé¿å… NaN/Infã€‚
    """
    def __init__(
        self,
        cfgs: Optional[Sequence[Tuple[int, int, int]]] = None,
        *,
        fft_sizes: Sequence[int] = (256, 512, 1024),
        hop_sizes: Sequence[int] = (64, 128, 256),
        win_sizes: Optional[Sequence[int]] = None,
        alpha_l1: float = 0.0,
        alpha_mag: float = 1.0,
        alpha_sc: float = 0.08,
        center: bool = False,
        power_mag: float = 1.0,
        lightweight: bool = False,  # å…¼å®¹å¤šä½™å‘½åå‚æ•°
        **kwargs
    ):
        super().__init__()
        if cfgs is not None:
            # å…è®¸ cfgs ä¸­åªç»™ (n_fft, hop)ï¼›æœªç»™ win æ—¶é»˜è®¤ win=n_fft
            _cfgs = []
            for it in cfgs:
                if len(it) == 2:
                    n_fft, hop = int(it[0]), int(it[1])
                    _cfgs.append((n_fft, hop, n_fft))
                else:
                    n_fft, hop, win = int(it[0]), int(it[1]), int(it[2])
                    _cfgs.append((n_fft, hop, win))
            self.cfgs = _cfgs
        else:
            if win_sizes is None:
                win_sizes = fft_sizes
            assert len(fft_sizes) == len(hop_sizes) == len(win_sizes)
            self.cfgs = [(int(n), int(h), int(w)) for n, h, w in zip(fft_sizes, hop_sizes, win_sizes)]

        self.alpha_l1 = float(alpha_l1)
        self.alpha_mag = float(alpha_mag)
        self.alpha_sc = float(alpha_sc)
        self.center = bool(center)
        self.power_mag = float(power_mag)

    def _stft_mag(self, x: torch.Tensor, n_fft: int, hop: int, win: int) -> torch.Tensor:
        window = torch.hann_window(win, device=x.device, dtype=x.dtype)
        spec = torch.stft(
            x, n_fft=n_fft, hop_length=hop, win_length=win,
            window=window, center=self.center, return_complex=True
        )
        mag = spec.abs().pow(self.power_mag)
        # æ•°å€¼ç¨³å®šï¼šé¿å… log(0) å’Œé™¤é›¶
        return torch.clamp(mag, min=1e-7)

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        # è¾“å…¥é€šå¸¸æ˜¯ [B,1,T]ï¼Œå‹æ‰é€šé“ç»´
        if pred.dim() == 3 and pred.size(1) == 1:
            pred = pred[:, 0, :]
        if target.dim() == 3 and target.size(1) == 1:
            target = target[:, 0, :]

        total = pred.new_tensor(0.0)
        count = 0

        for (n_fft, hop, win) in self.cfgs:
            if pred.shape[-1] < win or target.shape[-1] < win:
                continue
            p = self._stft_mag(pred, n_fft, hop, win)
            t = self._stft_mag(target, n_fft, hop, win)

            # å¹…åº¦ L1
            l_mag = F.l1_loss(p, t)

            # è°±æ”¶æ•›ï¼ˆæŒ‰å¸¸è§å®šä¹‰ï¼‰ï¼š ||P-T||_F / ||T||_F
            num = torch.linalg.vector_norm(p - t, ord=2, dim=(-2, -1))
            den = torch.linalg.vector_norm(t,     ord=2, dim=(-2, -1)).clamp_min(1e-7)
            l_sc = (num / den).mean()

            total = total + (self.alpha_mag * l_mag + self.alpha_sc * l_sc)
            count += 1

        if count == 0:
            # å›é€€åˆ°æ—¶åŸŸ L1ï¼Œç¡®ä¿æ¢¯åº¦ä¸ä¸­æ–­
            total = F.l1_loss(pred, target)
        else:
            total = total / count

        if self.alpha_l1 > 0:
            total = total + self.alpha_l1 * F.l1_loss(pred, target)

        # æœ€ç»ˆç¡®ä¿æ—  NaN/Inf
        return torch.nan_to_num(total, nan=0.0, posinf=0.0, neginf=0.0)




from training.config import TrainConfig
from training.losses import l1_stft_loss, rate_loss
from training.fargan_losses import compute_fargan_original_style_loss
from utils.real_data_loader import create_aether_data_loader, AETHERRealDataset
from training.advanced_film_scheduler import AdvancedFiLMScheduler, create_film_parameter_groups
from utils.audio_validation_generator import integrate_audio_validation, export_validation_audio
from models.utils import extract_acoustic_priors
from utils.feature_spec import get_default_feature_spec
from models.feature_adapter import get_fargan_feature_spec


def get_feature_spec(feature_spec_type: str = "aether"):
    """æ ¹æ®ç‰¹å¾è§„èŒƒç±»å‹è·å–å¯¹åº”çš„ç‰¹å¾è§„èŒƒ"""
    if feature_spec_type == "fargan":
        return get_fargan_feature_spec()
    else:
        return get_default_feature_spec()


@dataclass
class ProgressiveStage:
    """æ¸è¿›å¼è®­ç»ƒé˜¶æ®µé…ç½®"""
    name: str
    description: str

    steps: Optional[int] = None
    batches: Optional[int] = None
    epochs: Optional[float] = None

    use_film: bool = False
    use_moe: bool = False
    use_quantization: bool = False
    apply_channel: bool = False

    channel_type: str = "clean"
    layered_loss: bool = False

    film_warmup_steps: int = 0
    film_start_ratio: float = 1.0
    film_beta_scale_start: float = 1.0

    learning_rate: float = 2e-4
    lambda_rate: float = 0.0
    lambda_balance: float = 0.0
    lambda_cons: float = 0.0

    min_convergence_rate: float = 5.0
    max_final_loss: float = 2.0
    early_stop_loss: float = 0.01

    enable_audio_quality: bool = False
    min_snr_db: float = 5.0
    min_mel_cos: float = 0.85
    max_mel_l2: float = 0.15
    max_spectral_distortion: float = 0.65
    max_rms_delta_db: float = 3.0

    use_advanced_scheduler: bool = False

    min_final_film_ratio: float = 0.0
    max_recovery_events: int = 999
    max_spikes_last_50: int = 999
    wave_start_step: int = 0
    wave_full_start_step: int = 0
    wave_lowpass_weight: float = 0.5
    wave_full_weight: float = 1.0
    wave_lowpass_schedule: List[Tuple[int, float]] = field(default_factory=list)
    wave_full_schedule: List[Tuple[int, float]] = field(default_factory=list)
    train_wave_head_only: bool = False
    target_kbps: float = 0.0
    max_kbps_p90: float = 0.0
    preheat_mix_start_step: int = 1
    preheat_mix_end_step: int = 0
    preheat_chunk_frames: int = 0

    def calculate_steps(self, total_batches: int) -> int:
        """æ ¹æ®é…ç½®è®¡ç®—å®é™…è®­ç»ƒæ­¥æ•°"""
        if self.steps is not None:
            return max(1, self.steps)
        elif self.batches is not None:
            return max(1, self.batches)
        elif self.epochs is not None:
            return max(1, int(self.epochs * total_batches))
        else:
            return total_batches


def configure_stage_model(encoder: nn.Module, decoder: nn.Module, stage: ProgressiveStage) -> None:
    """æ ¹æ®é˜¶æ®µé…ç½®æ¨¡å‹çŠ¶æ€"""
    encoder.set_stage("C" if stage.use_film or stage.use_moe else "A")


def _rg(t, digits=6):
    """å®‰å…¨æ¢¯åº¦èŒƒå›´æ˜¾ç¤º"""
    if t is None or not isinstance(t, torch.Tensor):
        return "N/A"
    try:
        if t.requires_grad and t.grad is not None:
            return f"{float(t.grad.min().cpu()):.{digits}e}~{float(t.grad.max().cpu()):.{digits}e}"
        return "no_grad"
    except:
        return "err"


def _grad_ok(modules: List[nn.Module], max_norm: float = 1000.0, debug: bool = False) -> bool:
    """æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸ - ä»…æ£€æŸ¥NaN/Infï¼Œå…è®¸å¤§æ¢¯åº¦"""
    try:
        for i, module in enumerate(modules):
            for name, param in module.named_parameters():
                if param.grad is not None:
                    # åªæ£€æŸ¥NaNå’ŒInfï¼Œä¸é™åˆ¶æ¢¯åº¦å¤§å°
                    if torch.isnan(param.grad).any():
                        if debug:
                            print(f"âš ï¸ NaNæ¢¯åº¦åœ¨æ¨¡å—{i} {name}")
                        return False
                    if torch.isinf(param.grad).any():
                        if debug:
                            print(f"âš ï¸ Infæ¢¯åº¦åœ¨æ¨¡å—{i} {name}")
                        return False
        return True
    except Exception as e:
        if debug:
            print(f"âš ï¸ æ¢¯åº¦æ£€æŸ¥å¼‚å¸¸: {e}")
        return False


def compute_si_snr(pred: torch.Tensor, target: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """Calculate SI-SNR for batch tensors [B, T]."""
    if pred.dim() == 3 and pred.size(1) == 1:
        pred = pred[:, 0]
    if target.dim() == 3 and target.size(1) == 1:
        target = target[:, 0]
    if pred.dim() == 1:
        pred = pred.unsqueeze(0)
    if target.dim() == 1:
        target = target.unsqueeze(0)

    pred_zm = pred - pred.mean(dim=-1, keepdim=True)
    target_zm = target - target.mean(dim=-1, keepdim=True)

    dot = torch.sum(pred_zm * target_zm, dim=-1, keepdim=True)
    target_power = torch.sum(target_zm.pow(2), dim=-1, keepdim=True) + eps
    proj = dot / target_power * target_zm
    noise = pred_zm - proj

    ratio = (proj.pow(2).sum(dim=-1) + eps) / (noise.pow(2).sum(dim=-1) + eps)
    return 10 * torch.log10(ratio + eps)


def calculate_audio_quality(y_hat_feats, y_orig_feats, wave_head, original_audio, csi_dict=None):
    """è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡ - ä»åŸç‰ˆæå–çš„æ ¸å¿ƒé€»è¾‘"""
    try:
        device = y_hat_feats.device
        eps = 1e-12

        # ç”Ÿæˆé‡å»ºéŸ³é¢‘
        frame_count = y_hat_feats.size(1) if y_hat_feats.dim() >= 2 else y_hat_feats.size(-1)
        target_len = int(frame_count * FRAME_HOP_SAMPLES)

        audio_aligned = None
        if original_audio is not None:
            audio_aligned = original_audio.clone()
            while audio_aligned.dim() > 2:
                audio_aligned = audio_aligned.squeeze(1)
            if audio_aligned.dim() == 1:
                audio_aligned = audio_aligned.unsqueeze(0)
            current_len = audio_aligned.size(-1)
            if current_len < target_len:
                pad = target_len - current_len
                audio_aligned = F.pad(audio_aligned, (0, pad))
            elif current_len > target_len:
                audio_aligned = audio_aligned[..., :target_len]
        else:
            return {'error': 'No original audio provided'}

        with torch.no_grad():
            if getattr(wave_head, '_is_exciter', False) and csi_dict is not None:
                try:
                    y_hat_audio = wave_head(y_hat_feats, target_len=target_len, csi_dict=csi_dict)
                except:
                    y_hat_audio = wave_head(y_hat_feats, target_len=target_len)
            else:
                y_hat_audio = wave_head(y_hat_feats, target_len=target_len)

        # éŸ³é¢‘å¯¹é½ï¼ˆé•¿åº¦ + æ—¶é—´åç§»æ ¡æ­£ï¼‰
        y_target = audio_aligned
        min_len = min(y_hat_audio.size(-1), y_target.size(-1))
        y_hat_audio = y_hat_audio[..., :min_len]
        y_target = y_target[..., :min_len]

        # é€šè¿‡äº’ç›¸å…³ä¼°è®¡å›ºå®šå»¶è¿Ÿå¹¶è¡¥å¿ï¼ˆæå‡ SNR/SI-SNR çš„å¯ä¿¡åº¦ï¼‰
        def _xcorr_align(pred: torch.Tensor, ref: torch.Tensor, max_shift: int = 640):
            # è¾“å…¥å½¢çŠ¶ [B, 1, T] æˆ– [B, T]
            if pred.dim() == 3:
                pred = pred.squeeze(1)
            if ref.dim() == 3:
                ref = ref.squeeze(1)
            B, T = pred.size(0), pred.size(-1)
            max_shift = min(max_shift, T - 1) if T > 1 else 0
            if max_shift <= 0:
                return pred, torch.zeros(B, dtype=torch.long, device=pred.device)

            # å½’ä¸€åŒ–é¿å…å¹…åº¦ä¸»å¯¼
            pred_n = pred - pred.mean(dim=-1, keepdim=True)
            ref_n = ref - ref.mean(dim=-1, keepdim=True)

            # é€šè¿‡æœ‰é™èŒƒå›´æ»‘åŠ¨è®¡ç®—åˆ†æ®µç›¸å…³ï¼ˆæ•ˆç‡è¶³å¤Ÿï¼‰
            best_shifts = []
            aligned = []
            for b in range(B):
                p = pred_n[b]
                r = ref_n[b]
                best_score = -1e9
                best_k = 0
                for k in range(-max_shift, max_shift + 1):
                    if k < 0:
                        # pred æå‰ -> å‘å³ç§»
                        s = (p[-k:] * r[: T + k]).sum()
                    elif k > 0:
                        s = (p[: T - k] * r[k:]).sum()
                    else:
                        s = (p * r).sum()
                    if s > best_score:
                        best_score = s
                        best_k = k
                best_shifts.append(best_k)
                # åº”ç”¨ç§»ä½
                if best_k < 0:
                    # pred å‘å³ç§» |k|
                    pad = torch.zeros(-best_k, device=pred.device, dtype=pred.dtype)
                    aligned_pred = torch.cat([pad, pred[b, : T + best_k]], dim=0)
                elif best_k > 0:
                    aligned_pred = torch.cat([pred[b, best_k:], torch.zeros(best_k, device=pred.device, dtype=pred.dtype)], dim=0)
                else:
                    aligned_pred = pred[b]
                aligned.append(aligned_pred.unsqueeze(0))
            return torch.cat(aligned, dim=0), torch.tensor(best_shifts, device=pred.device)

        y_hat_audio_aligned, _shifts = _xcorr_align(y_hat_audio, y_target, max_shift=640)
        y_hat_audio = y_hat_audio_aligned

        # è®¡ç®—å¢ç›Šå¯¹é½
        pred_energy = y_hat_audio.pow(2).mean(dim=-1, keepdim=True)
        tgt_energy = y_target.pow(2).mean(dim=-1, keepdim=True)
        gain = torch.sqrt((tgt_energy + eps) / (pred_energy + eps))
        gain = torch.clamp(gain, 0.1, 3.0)
        y_hat_aligned = gain * y_hat_audio

        # SNRè®¡ç®—
        signal_power = torch.mean(y_target.pow(2), dim=-1)
        noise_power = torch.mean((y_hat_aligned - y_target).pow(2), dim=-1)
        snr_linear = (signal_power + eps) / (noise_power + eps)
        snr_db = 10.0 * torch.log10(snr_linear + eps)

        # SI-SNRè®¡ç®—
        target_norm = y_target - torch.mean(y_target, dim=-1, keepdim=True)
        pred_norm = y_hat_aligned - torch.mean(y_hat_aligned, dim=-1, keepdim=True)

        # æŠ•å½±
        dot_product = torch.sum(pred_norm * target_norm, dim=-1, keepdim=True)
        target_energy = torch.sum(target_norm.pow(2), dim=-1, keepdim=True)
        projection = (dot_product / (target_energy + eps)) * target_norm

        # SI-SNR
        signal_power_si = torch.sum(projection.pow(2), dim=-1)
        noise_power_si = torch.sum((pred_norm - projection).pow(2), dim=-1)
        si_snr_linear = (signal_power_si + eps) / (noise_power_si + eps)
        si_snr_db = 10.0 * torch.log10(si_snr_linear + eps)

        # æ­£ç¡®çš„80-bin MelæŒ‡æ ‡
        mel_pred = _logmel_80(y_hat_aligned)
        mel_target = _logmel_80(y_target)

        mel_cos = F.cosine_similarity(mel_pred, mel_target, dim=-1).mean()
        mel_l2 = F.mse_loss(mel_pred, mel_target)

        # PESQ-like score (0-5èŒƒå›´ï¼ŒåŸºäº Mel ç›¸ä¼¼åº¦ä¸ SNR)
        pesq_like = 1.0 + 4.0 * torch.sigmoid(0.1 * snr_db.mean() + 2.0 * mel_cos)

        # Melè°±å¤±çœŸ (å‡æ–¹æ ¹è¯¯å·®)
        spectral_distortion = torch.sqrt(mel_l2 + eps)

        # ç‰¹å¾å±‚é¢ç›¸å…³æ€§
        y_hat_flat = y_hat_feats.flatten()
        y_orig_flat = y_orig_feats.flatten()
        feature_correlation = torch.corrcoef(torch.stack([y_hat_flat, y_orig_flat]))[0, 1]
        feature_correlation = torch.nan_to_num(feature_correlation, nan=0.0)

        return {
            'snr_db': float(snr_db.mean().cpu()),
            'si_snr_db': float(si_snr_db.mean().cpu()),
            'pesq_like': float(pesq_like.cpu()),
            'spectral_distortion': float(spectral_distortion.cpu()),
            'mel_cos': float(mel_cos.cpu()),
            'mel_l2': float(mel_l2.cpu()),
            'feature_correlation': float(feature_correlation.cpu()),
            'pred_rms_db': float(20 * torch.log10(torch.sqrt(torch.mean(y_hat_aligned.pow(2))) + eps).cpu()),
            'target_rms_db': float(20 * torch.log10(torch.sqrt(torch.mean(y_target.pow(2))) + eps).cpu())
        }

    except Exception as e:
        return {'error': str(e)}


def check_energy_anomaly(pred_audio, step, anomaly_state=None):
    """RMSèƒ½é‡å“¨å…µ - æ£€æµ‹é™éŸ³/çˆ†å™ªå¼‚å¸¸"""
    if anomaly_state is None:
        anomaly_state = {'low_energy_count': 0, 'high_energy_count': 0, 'last_warning_step': -999}

    eps = 1e-12
    rms = torch.sqrt(torch.mean(pred_audio.pow(2)) + eps)
    rms_db = 20 * torch.log10(rms + eps)

    # æ£€æµ‹æŒç»­ä½èƒ½é‡ (é™éŸ³)
    if rms_db < -35.0:
        anomaly_state['low_energy_count'] += 1
        if anomaly_state['low_energy_count'] >= 5 and (step - anomaly_state['last_warning_step']) > 50:
            print(f"âš ï¸ æ­¥éª¤ {step}: æ£€æµ‹åˆ°æŒç»­ä½èƒ½é‡ (é™éŸ³) - RMS: {rms_db:.1f}dB, è¿ç»­{anomaly_state['low_energy_count']}æ¬¡")
            anomaly_state['last_warning_step'] = step
            return 'low_energy'
    else:
        anomaly_state['low_energy_count'] = max(0, anomaly_state['low_energy_count'] - 1)

    # æ£€æµ‹èƒ½é‡è¿‡é«˜ (çˆ†å™ª)
    if rms_db > -5.0:
        anomaly_state['high_energy_count'] += 1
        if anomaly_state['high_energy_count'] >= 3 and (step - anomaly_state['last_warning_step']) > 50:
            print(f"âš ï¸ æ­¥éª¤ {step}: æ£€æµ‹åˆ°è¿‡é«˜èƒ½é‡ (çˆ†å™ª) - RMS: {rms_db:.1f}dB, è¿ç»­{anomaly_state['high_energy_count']}æ¬¡")
            anomaly_state['last_warning_step'] = step
            return 'high_energy'
    else:
        anomaly_state['high_energy_count'] = max(0, anomaly_state['high_energy_count'] - 1)

    return None


def apply_energy_rescue(wave_head, anomaly_type, step):
    """èƒ½é‡å¼‚å¸¸è‡ªæ•‘æœºåˆ¶"""
    if anomaly_type == 'low_energy':
        print(f"ğŸ”§ æ­¥éª¤ {step}: åº”ç”¨ä½èƒ½é‡è‡ªæ•‘ - æ·»åŠ å°å¹…åç½®")
        # ä¸ºæœ€åä¸€å±‚æ·»åŠ å°åç½®
        for name, param in wave_head.named_parameters():
            if 'bias' in name and param.dim() == 1:
                with torch.no_grad():
                    param.data += 0.01 * torch.randn_like(param.data)
                break
        return True
    elif anomaly_type == 'high_energy':
        print(f"ğŸ”§ æ­¥éª¤ {step}: åº”ç”¨é«˜èƒ½é‡è‡ªæ•‘ - æƒé‡æŠ‘åˆ¶")
        # è½»å¾®æŠ‘åˆ¶æƒé‡
        for param in wave_head.parameters():
            if param.dim() > 1:  # åªå¤„ç†æƒé‡çŸ©é˜µ
                with torch.no_grad():
                    param.data *= 0.95
                break
        return True
    return False


def monitor_f0_health(f0_pred, f0_target, step, threshold_corr=0.95, threshold_unique=0.01):
    """ç›‘æ§F0å¥åº·çŠ¶æ€ï¼Œè‡ªåŠ¨è°ƒæ•´æƒé‡"""
    try:
        # è®¡ç®—ç›¸å…³æ€§
        f0_pred_flat = f0_pred.flatten()
        f0_target_flat = f0_target.flatten()

        # è¿‡æ»¤æ‰æ— æ•ˆå€¼
        valid_mask = torch.isfinite(f0_pred_flat) & torch.isfinite(f0_target_flat)
        if valid_mask.sum() < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆå€¼
            return False

        f0_pred_valid = f0_pred_flat[valid_mask]
        f0_target_valid = f0_target_flat[valid_mask]

        # è®¡ç®—ç›¸å…³æ€§
        try:
            f0_corr = torch.corrcoef(torch.stack([f0_pred_valid, f0_target_valid]))[0, 1].item()
            # æ£€æŸ¥NaNå¹¶ä½¿ç”¨NumPyä½œä¸ºfallback
            if not torch.isfinite(torch.tensor(f0_corr)):
                import numpy as np
                pred_np = f0_pred_valid.detach().cpu().numpy()
                target_np = f0_target_valid.detach().cpu().numpy()
                f0_corr = float(np.corrcoef(pred_np, target_np)[0, 1])
        except:
            f0_corr = 0.0

        # è®¡ç®—å”¯ä¸€å€¼æ¯”ä¾‹
        f0_rounded = f0_pred_valid.round(decimals=2)
        unique_ratio = len(torch.unique(f0_rounded)) / f0_rounded.numel()

        # æ£€æµ‹å¡Œç¼©
        f0_collapsed = (f0_corr < threshold_corr) and (unique_ratio < threshold_unique)

        if f0_collapsed and step % 100 == 0:
            print(f"âš ï¸ Step {step}: F0 collapse detected! corr={f0_corr:.3f}, unique_ratio={unique_ratio:.3f}")
            return True  # éœ€è¦è°ƒæ•´æƒé‡

        # æ¯500æ­¥æŠ¥å‘Šå¥åº·çŠ¶æ€
        if step % 500 == 0:
            print(f"ğŸ“Š Step {step}: F0 health - corr={f0_corr:.3f}, unique_ratio={unique_ratio:.3f}")

        return False

    except Exception as e:
        print(f"âš ï¸ F0ç›‘æ§å¤±è´¥: {e}")
        return False


def train_progressive_stage(
    stage: ProgressiveStage,
    encoder: nn.Module,
    decoder: nn.Module,
    wave_head: nn.Module,
    wave_loss: nn.Module,
    train_loader: DataLoader,
    train_dataset: Optional[AETHERRealDataset],
    device: torch.device,
    checkpoint_dir: Path,
    current_stage_index: int,
    total_stages: int,
    checkpoint_every: int = 500,
    feature_spec_type: str = "fargan",
    decoder_type: str = "aether",
    disable_f0_loss: bool = False,
) -> Dict[str, Any]:
    """è®­ç»ƒå•ä¸ªæ¸è¿›é˜¶æ®µ"""

    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹é˜¶æ®µ {current_stage_index+1}/{total_stages}: {stage.name}")
    print(f"ğŸ“ æè¿°: {stage.description}")
    print(f"{'='*60}")

    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    total_batches = len(train_loader)
    actual_steps = stage.calculate_steps(total_batches)
    planned_epochs = actual_steps / total_batches
    effective_batches_per_epoch = int(total_batches)

    print(f"æœ€ç»ˆé…ç½®: {planned_epochs:.2f} epochs, æ¯epoch {effective_batches_per_epoch} batches, æ€»æ­¥æ•° {actual_steps}, å­¦ä¹ ç‡: {stage.learning_rate}")

    # é…ç½®æ¨¡å‹
    configure_stage_model(encoder, decoder, stage)

    # è®¾ç½®å‚æ•°æ¢¯åº¦
    if getattr(stage, 'train_wave_head_only', False):
        # å†»ç»“ç¼–ç å™¨
        for param in encoder.parameters():
            param.requires_grad = False

        # å¤„ç†è§£ç å™¨ï¼šFARGANéœ€è¦ç‰¹æ®Šå¤„ç†
        if decoder_type == 'aether_fargan' and hasattr(decoder, 'fargan_core'):
            # AETHER-FARGAN: å†»ç»“é™¤FARGANæ ¸å¿ƒå¤–çš„æ‰€æœ‰è§£ç å™¨å‚æ•°
            for param in decoder.parameters():
                param.requires_grad = False
            # åªå¯ç”¨FARGANæ ¸å¿ƒå‚æ•°
            for param in decoder.fargan_core.parameters():
                param.requires_grad = True
            print(f"FARGANé¢„çƒ­æ¨¡å¼: å¯ç”¨ fargan_core å‚æ•°ï¼Œå†»ç»“å…¶ä»–è§£ç å™¨å‚æ•°")
        else:
            # å…¶ä»–è§£ç å™¨ç±»å‹ï¼šå†»ç»“æ‰€æœ‰è§£ç å™¨å‚æ•°
            for param in decoder.parameters():
                param.requires_grad = False

        # å¯ç”¨æ³¢å½¢å¤´å‚æ•°
        for param in wave_head.parameters():
            param.requires_grad = True
    else:
        for param in encoder.parameters():
            param.requires_grad = True
        for param in decoder.parameters():
            param.requires_grad = True
        for param in wave_head.parameters():
            param.requires_grad = True

    # åˆ›å»ºä¼˜åŒ–å™¨ - å¤„ç†EmbeddedSynthHeadçš„å‚æ•°é‡å¤é—®é¢˜
    all_param_groups = []

    # æ£€æŸ¥æ˜¯å¦æ˜¯EmbeddedSynthHead (é›¶å‚æ•°åŒ…è£…å™¨)
    is_embedded_synth = hasattr(wave_head, 'decoder') and wave_head.decoder is decoder

    if stage.use_advanced_scheduler and stage.use_film:
        param_groups = create_film_parameter_groups(
            encoder, decoder, wave_head,
            base_lr=stage.learning_rate,
            film_lr_scale=2.0,
            decoder_lr_scale=0.8,
            wave_lr_scale=0.2
        )
        all_param_groups.extend(param_groups)
    else:
        if not getattr(stage, 'train_wave_head_only', False):
            encoder_params = [p for p in encoder.parameters() if p.requires_grad]
            decoder_params = [p for p in decoder.parameters() if p.requires_grad]
            if encoder_params:
                all_param_groups.append({'params': encoder_params, 'lr': stage.learning_rate, 'name': 'encoder'})
            if decoder_params:
                all_param_groups.append({'params': decoder_params, 'lr': stage.learning_rate * 0.8, 'name': 'decoder'})
        else:
            # train_wave_head_only=Trueçš„æƒ…å†µ
            if decoder_type == 'aether_fargan':
                # AETHER-FARGAN: æ£€æŸ¥wave_headåŒ…è£…å™¨ä¸­çš„è§£ç å™¨
                actual_decoder = wave_head.decoder if hasattr(wave_head, 'decoder') else decoder
                if hasattr(actual_decoder, 'fargan_core'):
                    # åªè®­ç»ƒFARGANæ ¸å¿ƒåˆæˆå™¨
                    fargan_core_params = [p for p in actual_decoder.fargan_core.parameters() if p.requires_grad]
                    if fargan_core_params:
                        all_param_groups.append({'params': fargan_core_params, 'lr': stage.learning_rate, 'name': 'fargan_core'})
                        print(f"FARGANé¢„çƒ­æ¨¡å¼: åªè®­ç»ƒ fargan_coreï¼Œå‚æ•°æ•°é‡: {len(fargan_core_params)}")
                    else:
                        # å›é€€ï¼šè®­ç»ƒæ•´ä¸ªwave_head
                        wave_head_params = [p for p in wave_head.parameters() if p.requires_grad]
                        if wave_head_params:
                            all_param_groups.append({'params': wave_head_params, 'lr': stage.learning_rate, 'name': 'wave_head'})
                            print(f"å›é€€: è®­ç»ƒæ•´ä¸ª wave_headï¼Œå‚æ•°æ•°é‡: {len(wave_head_params)}")
                else:
                    # å›é€€ï¼šè®­ç»ƒæ•´ä¸ªwave_head
                    wave_head_params = [p for p in wave_head.parameters() if p.requires_grad]
                    if wave_head_params:
                        all_param_groups.append({'params': wave_head_params, 'lr': stage.learning_rate, 'name': 'wave_head'})
                        print(f"å›é€€: è®­ç»ƒæ•´ä¸ª wave_headï¼Œå‚æ•°æ•°é‡: {len(wave_head_params)}")
            elif is_embedded_synth:
                # EmbeddedSynthHead: åªè®­ç»ƒdecoderçš„synthéƒ¨åˆ†
                synth_params = [p for n, p in decoder.named_parameters() if 'synth' in n and p.requires_grad]
                if synth_params:
                    all_param_groups.append({'params': synth_params, 'lr': stage.learning_rate, 'name': 'decoder_synth'})
            else:
                # ç‹¬ç«‹çš„wave_head
                wave_head_params = [p for p in wave_head.parameters() if p.requires_grad]
                if wave_head_params:
                    all_param_groups.append({'params': wave_head_params, 'lr': stage.learning_rate, 'name': 'wave_head'})

        # å¦‚æœä¸æ˜¯embedded synthæˆ–è€…ä¸æ˜¯wave_head_onlyæ¨¡å¼ï¼Œæ·»åŠ ç‹¬ç«‹çš„wave_headå‚æ•°
        if not is_embedded_synth and not getattr(stage, 'train_wave_head_only', False):
            wave_head_params = [p for p in wave_head.parameters() if p.requires_grad]
            if wave_head_params:
                all_param_groups.append({'params': wave_head_params, 'lr': stage.learning_rate * 0.2, 'name': 'wave_head'})

    if not all_param_groups:
        raise ValueError(f"é˜¶æ®µ {stage.name}: æ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°")

    # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å‚æ•°ç»„
    print(f"ğŸ”§ ä¼˜åŒ–å™¨å‚æ•°ç»„:")
    for i, group in enumerate(all_param_groups):
        name = group.get('name', f'group_{i}')
        param_count = len(group['params'])
        lr = group['lr']
        print(f"  {name}: {param_count} å‚æ•°, lr={lr:.2e}")

    # æ£€æŸ¥å‚æ•°é‡å¤
    all_params = []
    for group in all_param_groups:
        all_params.extend(group['params'])
    unique_params = set(id(p) for p in all_params)
    if len(all_params) != len(unique_params):
        print(f"âš ï¸ æ£€æµ‹åˆ°å‚æ•°é‡å¤: æ€»å‚æ•°{len(all_params)}, å”¯ä¸€å‚æ•°{len(unique_params)}")
        # ç§»é™¤é‡å¤å‚æ•°
        seen_params = set()
        for group in all_param_groups:
            unique_group_params = []
            for p in group['params']:
                if id(p) not in seen_params:
                    unique_group_params.append(p)
                    seen_params.add(id(p))
            group['params'] = unique_group_params
        print("âœ… å·²ç§»é™¤é‡å¤å‚æ•°")

    optimizer = optim.AdamW(all_param_groups, weight_decay=1e-6)
    scaler = _create_grad_scaler(enabled=(device.type == 'cuda'))

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šramp æœŸé—´ä½¿ç”¨ LambdaLRï¼Œramp ç»“æŸåå¯åˆ‡æ¢åˆ° ReduceLROnPlateau
    from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau
    ramp_steps = 5000 if decoder_type == 'aether_fargan' else actual_steps  # FARGANæ¨¡å¼ä½¿ç”¨rampè°ƒåº¦

    # åˆå§‹é˜¶æ®µä½¿ç”¨ LambdaLRï¼ˆä½™å¼¦é€€ç«ï¼‰
    lr_scheduler = LambdaLR(
        optimizer,
        lr_lambda=lambda step: 0.5 * (1 + math.cos(math.pi * step / ramp_steps)) if step < ramp_steps else 0.1
    )

    # ç”¨äº ramp ç»“æŸåçš„ Plateau è°ƒåº¦å™¨
    plateau_scheduler = ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=500, min_lr=1e-6
    )

    scheduler_switched = False  # æ ‡è®°æ˜¯å¦å·²åˆ‡æ¢åˆ° plateau è°ƒåº¦å™¨

    # é«˜çº§FiLMè°ƒåº¦å™¨
    film_scheduler = None
    if stage.use_advanced_scheduler and stage.use_film:
        film_scheduler = AdvancedFiLMScheduler(
            encoder=encoder,
            total_steps=actual_steps,
            warmup_steps=stage.film_warmup_steps,
            start_ratio=stage.film_start_ratio,
            beta_scale_start=stage.film_beta_scale_start
        )

    # è®­ç»ƒå¾ªç¯
    encoder.train()
    decoder.train()
    wave_head.train()

    # Initialize FARGAN loss logging variable
    fargan_loss_logs = None

    # é¢„çƒ­é˜¶æ®µå…ˆå…³é—­æ½œåœ¨é‡åŒ–ï¼Œé¿å…æ—©æœŸé‡åŒ–å™ªå£°å¹²æ‰°F0
    _orig_quant_flag = getattr(encoder, 'quantize_latent', None)
    if stage.name == 'wave_preheat' and _orig_quant_flag is not None:
        try:
            encoder.quantize_latent = False
            print("ğŸ”§ é¢„çƒ­é˜¶æ®µ: å·²æš‚æ—¶å…³é—­latenté‡åŒ–")
        except Exception:
            pass

    best_loss = float('inf')
    convergence_losses = []
    step = 0

    # æ³¢å½¢é¢„çƒ­ç›¸å…³å‚æ•°
    preheat_gain = nn.Parameter(torch.tensor(3.0, device=device))
    preheat_scale = nn.Parameter(torch.tensor(0.1, device=device))

    # RMSèƒ½é‡å¼‚å¸¸ç›‘æ§çŠ¶æ€
    energy_anomaly_state = {'low_energy_count': 0, 'high_energy_count': 0, 'last_warning_step': -999}

    # å¯ç”¨æ•°å€¼å¼‚å¸¸æ£€æµ‹ (ä»…åœ¨è°ƒè¯•æ—¶)
    # torch.autograd.set_detect_anomaly(True)  # æš‚æ—¶ç¦ç”¨ä»¥æé«˜æ€§èƒ½

    # æ— é™è¿­ä»£æ•°æ®åŠ è½½å™¨
    def batch_gen():
        while True:
            for batch in train_loader:
                yield batch

    batch_iter = batch_gen()

    # F0å¥åº·å‘Šè­¦å†·å´çª—å£ï¼ˆè§¦å‘ååœ¨ä¸€å®šæ­¥æ•°å†…æå‡F0æƒé‡/é—¨æ§æ³¢å½¢æŸå¤±ï¼‰
    f0_alert_until_step = -1

    # ğŸš€ è¶…è½»é‡é¢„çƒ­ä¸“ç”¨æ³¢å½¢æŸå¤± - å¤§å¹…å‡å°‘è®¡ç®—å‹åŠ›
    wave_loss_fast = MRSTFTLoss(
        lightweight=False,
        fft_sizes=(256, 512, 1024),              # å¦‚å¤Ÿç®—åŠ›å¯æ‰©åˆ° (256,512,1024,2048)
        hop_sizes=(64, 128, 256),
        win_sizes=(256, 512, 1024),
        alpha_l1=2.0,                             # ä» 4.0 é™åˆ° 2.0
        alpha_mag=1.0,
        alpha_sc=0.08                             # æ‰“å¼€è°±æ”¶æ•›
    ).to(device)

    if stage.name == "wave_preheat":
        wave_loss_fast = MRSTFTLoss(
            lightweight=False,
            fft_sizes=(256, 512, 1024, 2048),
            hop_sizes=(64, 128, 256, 512),
            win_sizes=(256, 512, 1024, 2048),
            alpha_l1=2.0,      # ä» 4.0 ä¸‹è°ƒï¼Œç»™é¢‘åŸŸè®©è·¯
            alpha_mag=1.0,
            alpha_sc=0.08      # æ‰“å¼€è°±æ”¶æ•›
        ).to(device)


    for step in range(1, actual_steps + 1):
        batch = next(batch_iter)
        optimizer.zero_grad()

        # æ•°æ®é¢„å¤„ç† - ä½¿ç”¨æ­£ç¡®çš„é”®å
        x_gpu = batch['x'].to(device, non_blocking=True)  # è¾“å…¥ç‰¹å¾
        y_gpu = batch['y'].to(device, non_blocking=True)  # ç›®æ ‡ç‰¹å¾
        csi_dict = {k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v
                   for k, v in batch.get('csi', {}).items()}

        x = torch.nan_to_num(x_gpu, nan=0.0, posinf=1e4, neginf=-1e4)
        y = torch.nan_to_num(y_gpu, nan=0.0, posinf=1e4, neginf=-1e4)
        original_audio = batch.get('audio')
        if original_audio is not None:
            original_audio = original_audio.to(device, non_blocking=True)

        # ä¸ºåŒè·¯è®­ç»ƒä¿å­˜åŸå§‹ç›®æ ‡ç‰¹å¾
        y_original = y.clone()

        # å½“å‰epochå’Œæ­¥æ•°ä¿¡æ¯
        current_epoch = (step - 1) // effective_batches_per_epoch + 1
        epoch_step = (step - 1) % effective_batches_per_epoch + 1

        # éšæœºè£å‰ª(ä»…é¢„çƒ­é˜¶æ®µ)
        if stage.name == "wave_preheat" and getattr(stage, 'preheat_chunk_frames', 0) > 0:
            chunk_len = stage.preheat_chunk_frames
            seq_len = x.size(1)
            if seq_len > chunk_len:
                start_idx = random.randint(0, seq_len - chunk_len)
                x = x[:, start_idx:start_idx + chunk_len, :]
                y = y[:, start_idx:start_idx + chunk_len, :]
                # ä¹Ÿå¯¹åŸå§‹ç›®æ ‡ç‰¹å¾åº”ç”¨ç›¸åŒçš„è£å‰ª
                y_original = y_original[:, start_idx:start_idx + chunk_len, :]
                if original_audio is not None:
                    audio_start = start_idx * FRAME_HOP_SAMPLES
                    audio_end = audio_start + chunk_len * FRAME_HOP_SAMPLES
                    original_audio = original_audio[:, audio_start:audio_end]

        # æ··åˆç²¾åº¦è®¾ç½®
        s1_t_global = int(getattr(stage, 'preheat_mix_end_step', 0)) if hasattr(stage, 'preheat_mix_end_step') else 0
        use_amp_step = (device.type == 'cuda') and not (stage.name == "wave_preheat" and step <= s1_t_global + 300)

        with _autocast_ctx(enabled=use_amp_step):
            # ç¼–ç 
            x_fp32 = x.to(torch.float32)
            try:
                with _autocast_ctx(enabled=False):
                    z, enc_logs = encoder(x_fp32, csi_dict=csi_dict, inference=False)

                # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šè¾“å…¥ç‰¹å¾ç»Ÿè®¡
                if step <= 10 or step % 200 == 0:
                    print(f"ğŸ“Š Step {step} è¾“å…¥ç‰¹å¾ç»Ÿè®¡:")
                    print(f"   ç¼–ç è¾“å‡ºz: shape={z.shape}, mean={z.mean().item():.3f}, std={z.std().item():.3f}")
                    if not disable_f0_loss:
                        spec = get_feature_spec(feature_spec_type)
                        f0_slice = spec.get_feature_slice('f0') if hasattr(spec, 'get_feature_slice') else slice(18, 19)
                        f0_input = x[:, :, f0_slice].flatten()
                        print(f"   F0è¾“å…¥: mean={f0_input.mean().item():.3f}, std={f0_input.std().item():.3f}, range=[{f0_input.min().item():.3f}, {f0_input.max().item():.3f}]")

            except Exception as e:
                print(f"âŒ ç¼–ç å¤±è´¥ at step {step}: {e}")
                continue

            # è§£ç 
            try:
                # è®©è§£ç å™¨ä¹Ÿæ„ŸçŸ¥å£°å­¦å…ˆéªŒï¼ˆä¸ç¼–ç å™¨å¯¹é½ï¼‰
                csi_dec = dict(csi_dict)
                try:
                    csi_dec["acoustic_priors"] = extract_acoustic_priors(x).detach()
                except Exception:
                    csi_dec = csi_dict
                y_hat_raw = decoder(z, csi_dict=csi_dec)
                # è®¡ç®—é‡å»ºæŸå¤±æ—¶éœ€è¦åŸºäºåŸå§‹è¾“å‡ºçš„æœ‰é™æ€§æ©ç ï¼Œé¿å…æŠŠéæœ‰é™å€¼æ¸…æ´—åå‚ä¸æŸå¤±
                y_hat = _clean_tensor(y_hat_raw)
                y     = _clean_tensor(y)
                # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šè§£ç è¾“å‡ºç»Ÿè®¡
                if step <= 10 or step % 200 == 0:
                    if not disable_f0_loss:
                        spec = get_feature_spec(feature_spec_type)
                        f0_slice = spec.get_feature_slice('f0') if hasattr(spec, 'get_feature_slice') else slice(18, 19)
                        f0_pred = y_hat[:, :, f0_slice].flatten()
                        f0_target = y[:, :, f0_slice].flatten()
                        print(f"   F0é¢„æµ‹: mean={f0_pred.mean().item():.3f}, std={f0_pred.std().item():.3f}, range=[{f0_pred.min().item():.3f}, {f0_pred.max().item():.3f}]")
                        print(f"   F0ç›®æ ‡: mean={f0_target.mean().item():.3f}, std={f0_target.std().item():.3f}, range=[{f0_target.min().item():.3f}, {f0_target.max().item():.3f}]")

            except Exception as e:
                print(f"âŒ è§£ç å¤±è´¥ at step {step}: {e}")
                continue

            # âœ… ä¸»æŸå¤±ï¼šç‰¹å¾åŸŸé‡å»º [B,T,feature_dims] vs [B,T,feature_dims]
            if disable_f0_loss:
                try:
                    spec = get_feature_spec(feature_spec_type)
                    if feature_spec_type == "fargan":
                        f0_sl = spec.get_feature_slices().get('dnn_pitch', slice(0,0))
                    else:
                        f0_sl = spec.get_feature_slice('f0') if hasattr(spec, 'get_feature_slice') else slice(20, 21)
                except Exception:
                    f0_sl = slice(0,0)

                def _drop_slice(t: torch.Tensor, s: slice) -> torch.Tensor:
                    if (s.stop - s.start) <= 0:
                        return t
                    left = t[..., :s.start]
                    right = t[..., s.stop:]
                    return torch.cat([left, right], dim=-1)

                recon_loss = _finite_scalar(_safe_mse(_drop_slice(y_hat, f0_sl), _drop_slice(y, f0_sl), step), "recon_mse_nof0", step)
            else:
                recon_loss = _finite_scalar(_safe_mse(y_hat, y, step), "recon_mse", step)

            # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæŸå¤±åˆ†è§£åˆ†æï¼ˆå¯ç¦ç”¨F0ï¼‰
            if step <= 10 or step % 200 == 0:
                if not disable_f0_loss:
                    try:
                        spec = get_feature_spec(feature_spec_type)
                        if feature_spec_type == "fargan":
                            slices = spec.get_feature_slices()
                            f0_slice = slices.get('dnn_pitch', slice(18, 19))
                        else:
                            f0_slice = spec.get_feature_slice('f0') if hasattr(spec, 'get_feature_slice') else slice(18, 19)
                        f0_loss = F.mse_loss(y_hat[:, :, f0_slice], y[:, :, f0_slice])
                        other_features_loss = F.mse_loss(
                            y_hat[:, :, :f0_slice.start] if f0_slice.start > 0 else torch.empty(0, device=device),
                            y[:, :, :f0_slice.start] if f0_slice.start > 0 else torch.empty(0, device=device)
                        ) if f0_slice.start > 0 else 0.0
                        print(f"ğŸ’¡ Step {step} æŸå¤±åˆ†è§£:")
                        print(f"   æ€»é‡å»ºæŸå¤±: {recon_loss.item():.6f}")
                        print(f"   F0ç‰¹å¾æŸå¤±: {f0_loss.item():.6f}")
                        print(f"   å…¶ä»–ç‰¹å¾æŸå¤±: {other_features_loss if isinstance(other_features_loss, float) else other_features_loss.item():.6f}")
                    except Exception as e:
                        print(f"âš ï¸ F0æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                else:
                    print(f"ğŸ’¡ Step {step} æŸå¤±åˆ†è§£: å·²ç¦ç”¨F0; æ€»é‡å»ºæŸå¤±={recon_loss.item():.6f}")

            # === æ–°å¢ï¼šåˆ†å±‚æŸå¤±ï¼ˆä¸‰é˜¶æ®µç­–ç•¥ï¼‰ ===
            # ğŸ”§ åˆ†ç¦»æŸå¤±ä¼˜åŒ–ï¼šå»¶è¿Ÿåˆ†å±‚æŸå¤±ï¼Œç»™F0åˆ†æ”¯ç¨³å®šæ—¶é—´
            if getattr(stage, 'layered_loss', False) and step > 3000:  # å‰3000æ­¥ä¸ä½¿ç”¨åˆ†å±‚æŸå¤±
                layered_loss, ld, stage_name = compute_layered_loss(
                    y_hat, y, step, feature_spec_type, disable_f0=disable_f0_loss
                )
                recon_loss = recon_loss + layered_loss
                if step % 100 == 0:
                    print(f"[{step}] stage={stage_name} layered_loss={layered_loss.item():.4f}")
            elif getattr(stage, 'layered_loss', False) and step <= 3000:
                if step % 500 == 0:
                    print(f"[{step}] åˆ†å±‚æŸå¤±å·²ç¦ç”¨ï¼Œä¸“æ³¨F0ç¨³å®šè®­ç»ƒ")

            # ğŸ¯ Enhanced F0 Loss: å¯ç¦ç”¨
            f0_loss_applied = False
            try:
                if disable_f0_loss:
                    raise RuntimeError("F0 disabled")
                f0_losses = compute_enhanced_f0_loss(y, y_hat, spec=get_feature_spec(feature_spec_type))
                f0_losses = _sanitize_f0_losses(f0_losses, step)

                # === å½¢çŠ¶å¯¹é½å¢å¼ºï¼šé’ˆå¯¹æœ‰å£°å¸§çš„ç›¸å…³æ€§ä¸æ–œç‡ä¸€è‡´æ€§ ===
                try:
                    _spec_f0 = get_feature_spec(feature_spec_type)
                    f0_tgt = _spec_f0.extract_feature(y, 'f0') if hasattr(_spec_f0, 'extract_feature') else y[:, :, 18:19]  # [B,T,1] FARGAN: dnn_pitch
                    f0_hat = _spec_f0.extract_feature(y_hat, 'f0') if hasattr(_spec_f0, 'extract_feature') else y_hat[:, :, 18:19]    # [B,T,1]
                    # FARGANæ²¡æœ‰ç‹¬ç«‹çš„voicingç‰¹å¾ï¼Œä½¿ç”¨dnn_pitch > thresholdä½œä¸ºvoicing
                    if hasattr(_spec_f0, 'extract_feature'):
                        voi_tgt = _spec_f0.extract_feature(y, 'voicing')  # [B,T,1]
                        voi_hat = _spec_f0.extract_feature(y_hat, 'voicing')
                    else:
                        # FARGAN: ä»dnn_pitchæ¨å¯¼voicing
                        voi_tgt = (y[:, :, 18:19] > -1.0).float()      # [B,T,1]
                        voi_hat = (y_hat[:, :, 18:19] > -1.0).float()  # [B,T,1]

                    # æ©ç ç¡¬åŒ–ï¼ˆ0.3/0.7 é˜ˆå€¼ï¼‰+ å½¢æ€å­¦é—­è¿ç®—å¹³æ»‘ + é«˜é˜ˆå€¼æ»å›ç§å­
                    mask_lo = ((voi_tgt > 0.3) & (voi_hat > 0.3)).float()  # å®½æ¾æœ‰å£°
                    mask_hi = ((voi_tgt > 0.7) & (voi_hat > 0.7)).float()  # ç½®ä¿¡æœ‰å£°
                    # é—­è¿ç®—ï¼šå…ˆè†¨èƒ€å†è…èš€ï¼Œå»æ‰å°å­”æ´ä¸çŸ­å­¤ç«‹æ®µ
                    m = mask_lo.transpose(1, 2)  # [B,1,T]
                    m = F.max_pool1d(m, kernel_size=3, stride=1, padding=1)           # è†¨èƒ€
                    m = -F.max_pool1d(-m, kernel_size=3, stride=1, padding=1)         # è…èš€ï¼ˆæœ€å°æ± åŒ–ï¼‰
                    mask_closed = (m.transpose(1, 2) > 0.5).float()                    # [B,T,1]
                    # é«˜é˜ˆå€¼æ»å›ï¼šä»…ä¿ç•™ä¸é«˜é˜ˆå€¼é‚»åŸŸç›¸è¿åŒºåŸŸ
                    seed = F.max_pool1d(mask_hi.transpose(1, 2), kernel_size=3, stride=1, padding=1)
                    seed = (seed.transpose(1, 2) > 0.0).float()  # [B,T,1]
                    mask = (mask_closed > 0.5) & (seed > 0)
                    mask = mask.float()

                    # å°çª—xcorrå¯¹é½ï¼ˆÂ±2å¸§ï¼‰æé«˜F0 lossæ—¶åºä¸€è‡´æ€§
                    def _best_shift(a, b, max_k=2):
                        # a,b: [T]
                        best_k, best_s = 0, -1e9
                        for k in range(-max_k, max_k + 1):
                            if k < 0:
                                s = torch.sum(a[-k:] * b[: a.shape[0] + k])
                            elif k > 0:
                                s = torch.sum(a[: a.shape[0] - k] * b[k:])
                            else:
                                s = torch.sum(a * b)
                            if s > best_s:
                                best_s, best_k = float(s), k
                        return best_k

                    # é€æ ·æœ¬å¯¹é½ï¼ˆæ‰¹é‡è¾ƒå°æ—¶å¼€é”€å¯æ¥å—ï¼‰
                    f0_tgt_aligned = []
                    f0_hat_aligned = []
                    mask_aligned = []
                    B, T = f0_tgt.shape[0], f0_tgt.shape[1]
                    for b in range(B):
                        a = f0_hat[b, :, 0]
                        bvec = f0_tgt[b, :, 0]
                        k = _best_shift(a, bvec)
                        if k < 0:
                            ah = a[-k:]
                            bh = bvec[: T + k]
                            mh = mask[b, : T + k, 0]
                        elif k > 0:
                            ah = a[: T - k]
                            bh = bvec[k:]
                            mh = mask[b, k:, 0]
                        else:
                            ah = a
                            bh = bvec
                            mh = mask[b, :, 0]
                        # å¯¹é½åç»Ÿä¸€é•¿åº¦
                        L = min(ah.shape[0], bh.shape[0])
                        f0_hat_aligned.append(ah[:L])
                        f0_tgt_aligned.append(bh[:L])
                        mask_aligned.append(mh[:L])

                    f0_hat_cat = torch.cat([t.unsqueeze(0) for t in f0_hat_aligned], dim=0)
                    f0_tgt_cat = torch.cat([t.unsqueeze(0) for t in f0_tgt_aligned], dim=0)
                    mask_cat = torch.cat([t.unsqueeze(0) for t in mask_aligned], dim=0)

                    # ä»…åœ¨æœ‰å£°æ©ç å†…è®¡ç®—ç›¸å…³æ€§ä¸æ–œç‡
                    eps = 1e-5
                    def _masked_norm(z, m):
                        z_m = z * m
                        mu = (z_m.sum(dim=1, keepdim=True) / (m.sum(dim=1, keepdim=True) + eps))
                        zc = z_m - mu
                        var = (zc.pow(2) * m).sum(dim=1, keepdim=True) / (m.sum(dim=1, keepdim=True) + eps)
                        std = (var + eps).sqrt()
                        return (zc / std), m

                    valid_frames = int(mask_cat.sum().item())
                    if valid_frames >= 64:  # é—¨æ§›ï¼šè‡³å°‘ 64 ä¸ªæœ‰å£°å¸§
                        x, m_used = _masked_norm(f0_hat_cat, mask_cat)
                        y_, _     = _masked_norm(f0_tgt_cat, mask_cat)
                        L_corr = 1.0 - ((x * y_) * m_used).sum(dim=1) / (m_used.sum(dim=1) + eps)
                        L_corr = torch.nan_to_num(L_corr, nan=0.0, posinf=1.0, neginf=1.0).mean()

                        dx = torch.diff(f0_hat_cat, dim=1)
                        dy = torch.diff(f0_tgt_cat, dim=1)
                        mm = (mask_cat[:, 1:] * mask_cat[:, :-1])
                        denom = (mm.sum(dim=1) + eps)
                        L_delta = (torch.abs(dx - dy) * mm).sum(dim=1) / denom
                        L_delta = torch.nan_to_num(L_delta, nan=0.0, posinf=1.0, neginf=1.0).mean()
                    else:
                        L_corr  = torch.tensor(0.0, device=y_hat.device)
                        L_delta = torch.tensor(0.0, device=y_hat.device)


                    # ä»¥å°æƒé‡åŠ å…¥F0æ•´ä½“æŸå¤±ï¼ˆä»…å¢ç›Šï¼Œä¸æ›¿ä»£åŸæœ‰é¡¹ï¼‰
                    f0_losses_extra = 0.2 * L_corr + 0.1 * L_delta
                except Exception:
                    f0_losses_extra = 0.0

                if (not disable_f0_loss) and stage.name == "wave_preheat":
                    # é¢„çƒ­é˜¶æ®µï¼šè¾ƒå¼ºF0çº¦æŸ + æ–¹å·®æ­£åˆ™ + å½¢çŠ¶ä¸€è‡´æ€§
                    # f0_weight ä»2.0çº¿æ€§å›è½è‡³1.0ï¼ˆæ¥è¿‘preheat_mix_end_stepï¼‰
                    f0_weight = 2.0
                    try:
                        s2 = int(getattr(stage, 'preheat_mix_end_step', 0) or 0)
                        if s2 > 0:
                            decay_start = int(0.5 * s2)
                            if step >= decay_start:
                                p = min(1.0, (step - decay_start) / max(1, s2 - decay_start))
                                f0_weight = 2.0 - p * 1.0  # 2.0 -> 1.0
                    except Exception:
                        pass
                    var_reg = 0.0
                    try:
                        _spec_var = get_feature_spec(feature_spec_type)
                        _recon_f0 = _spec_var.extract_feature(y_hat, 'f0') if hasattr(_spec_var, 'extract_feature') else y_hat[:, :, 18:19]
                        var_reg = compute_f0_variance_regularization(_recon_f0, var_floor=0.02, weight=1.5)
                    except Exception:
                        pass
                    # F0å¥åº·é—­ç¯ï¼šå‘Šè­¦æœŸé—´ä¸´æ—¶æå‡F0æƒé‡
                    f0_boost = 2.0 if step <= f0_alert_until_step else 1.0
                    total_f0 = (
                        _finite_scalar((f0_boost * f0_weight) * f0_losses['total_f0_loss'], "f0_total", step)
                        + _finite_scalar(f0_losses['voi_loss'], "voi_loss", step)
                        + _finite_scalar(var_reg if isinstance(var_reg, torch.Tensor) else torch.tensor(var_reg, device=y_hat.device), "f0_var_reg", step)
                    )
                    if isinstance(f0_losses_extra, torch.Tensor):
                        total_f0 = total_f0 + _finite_scalar((f0_boost * f0_weight) * f0_losses_extra, "f0_shape_extra", step)
                    if step <= 10 or step % 200 == 0:
                        print(f"ğŸ’¡ Step {step} é¢„çƒ­é˜¶æ®µ: å¢å¼ºF0æŸå¤± f0_w={(f0_boost * f0_weight):.2f} total={total_f0.item():.6f}")
                else:
                    # æ­£å¸¸é˜¶æ®µï¼šå®Œæ•´F0æŸå¤±ï¼ˆåŠ å…¥å‰æœŸç¼“å¯ï¼Œé¿å…æ—©æœŸä¸»å¯¼ï¼‰
                    f0_weight = 0.1 + 1.4 * min(1.0, step / 1000.0)  # 0.1 â†’ 1.5 in first 1k steps
                    f0_boost = 2.0 if step <= f0_alert_until_step else 1.0
                    total_f0 = (f0_boost * f0_weight) * f0_losses['total_f0_loss'] + f0_losses['voi_loss']
                    if isinstance(f0_losses_extra, torch.Tensor):
                        total_f0 = total_f0 + (f0_boost * f0_weight) * f0_losses_extra

                # ğŸ§ª ç§»é™¤çº¦æŸæŸå¤±ï¼Œæµ‹è¯•åŸå§‹ä»£ç ç¨³å®šæ€§
                # constraint_loss = compute_f0_constraint_loss(pred_f0, target_f0, weight=0.3)

                recon_loss = recon_loss + total_f0
                f0_loss_applied = True

                if step % 100 == 0 and stage.name != "wave_preheat":
                    print(
                        f"[{step}] F0 losses: base={f0_losses['f0_base'].item():.4f} "
                        f"slope={f0_losses['f0_slope'].item():.4f} "
                        f"mean={f0_losses['f0_mean'].item():.4f} "
                        f"std={f0_losses['f0_std'].item():.4f} "
                        f"voi={f0_losses['voi_loss'].item():.4f} "
                        f"core={f0_losses['f0_core'].item():.4f}"
                    )

                # ğŸ” F0å¥åº·ç›‘æ§
                if step % 50 == 0:
                    feature_spec = get_feature_spec(feature_spec_type)
                    try:
                        orig_f0 = feature_spec.extract_feature(y, 'f0') if hasattr(feature_spec, 'extract_feature') else y[:, :, 18:19]
                        recon_f0 = feature_spec.extract_feature(y_hat, 'f0') if hasattr(feature_spec, 'extract_feature') else y_hat[:, :, 18:19]
                        needs_adjustment = monitor_f0_health(recon_f0, orig_f0, step)
                        if needs_adjustment:
                            print(f"ğŸš¨ Step {step}: F0å¡Œç¼©æ£€æµ‹åˆ° -> ä¸´æ—¶æå‡F0æƒé‡ & é—¨æ§æ³¢å½¢æŸå¤±")
                            f0_alert_until_step = max(f0_alert_until_step, step + 150)
                        # ä»…å¯¹æœ‰å£°å¸§å¢åŠ æœ€å°æ–¹å·®æ­£åˆ™ï¼Œåå¡Œé™·
                        try:
                            voi = feature_spec.extract_feature(y_hat, 'voicing')
                            voi_mask = (voi > 0.6).float()
                            # æŒ‰æ ·æœ¬ç»Ÿè®¡æœ‰å£°å¸§stdï¼Œç›®æ ‡â‰¥0.25
                            std_per_utt = ((recon_f0 * voi_mask).std(dim=1, unbiased=False) + 1e-6)
                            var_floor_pen = (0.25 - std_per_utt).clamp_min(0.0).mean()
                            recon_loss = recon_loss + 0.5 * var_floor_pen
                        except Exception:
                            pass
                    except Exception:
                        pass
            except Exception as e:
                if not disable_f0_loss:
                    print(f"âš ï¸ F0æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                    if step <= 10 or step % 200 == 0:
                        print(f"âš ï¸ Step {step}: F0æŸå¤±è¢«è·³è¿‡ (è®¡ç®—å¤±è´¥)")

            # è½»é‡åé™æ€æ­£åˆ™ï¼šæŠ‘åˆ¶æ•´æ®µå¸¸æ•°ç‰¹å¾ï¼ˆé¿å…F0/å‚æ•°å¡Œç¼©ï¼‰
            try:
                # æ—¶é—´ç»´åº¦ä¸Šæ–¹å·®è¿‡å°æ—¶æ–½åŠ æƒ©ç½š - å¢å¼ºæƒé‡
                var_t = y_hat.float().var(dim=1).mean()
                anti_static_loss = _finite_scalar((1.0 / (var_t + 1e-3)).clamp(max=1e3), "anti_static", step)
                anti_static_weight = 2e-3 if stage.name == "wave_preheat" else 1e-4  # é¢„çƒ­æœŸæ›´å¼ºæŠ‘åˆ¶é™æ€
                recon_loss = recon_loss + anti_static_weight * anti_static_loss
            except Exception:
                pass

            # ç ç‡æŸå¤±
            rate_loss_val = _finite_scalar(rate_loss(enc_logs.get('latent_continuous', z), stage.lambda_rate), "rate_loss", step)

            # âœ… è¾…åŠ©æŸå¤±ï¼šæ³¢å½¢åŸŸéªŒæ”¶/å¼•å¯¼ (ä»…å½“æœ‰éŸ³é¢‘æ—¶)
            # æµç¨‹ï¼šdecoder(z) -> [B,T,48] -> wave_head([B,T,48]) -> [B,1,T_audio]
            wave_loss_val = torch.tensor(0.0, device=device)

            # å…¨å±€ MR-STFT æƒé‡å¢å¼ºç­–ç•¥
            if decoder_type == 'aether_fargan':
                ramp_steps = 5000
                if step > ramp_steps * 0.6:
                    if step <= ramp_steps * 0.8:
                        mr_stft_boost = 1.5
                    else:
                        mr_stft_boost = 2.0  # ç»¼åˆæœŸç»´æŒå¼ºæƒé‡
                else:
                    mr_stft_boost = 1.0
            else:
                mr_stft_boost = 1.0  # é»˜è®¤ä¸åŠ å¼º
            if stage.enable_audio_quality and original_audio is not None:
                # é¢„çƒ­é˜¶æ®µçš„æ··åˆç­–ç•¥ - åŸå§‹ç‰¹å¾ â†” é‡å»ºç‰¹å¾æ¸è¿›è¿‡æ¸¡
                if stage.name == "wave_preheat":
                    # ç»Ÿä¸€TFæ—¶é—´è¡¨ï¼šä»s1åˆ°s2çº¿æ€§è¡°å‡ï¼Œä¸fallbackåˆ°æ€»æ­¥æ•°ä¸€åŠ
                    s1 = int(getattr(stage, 'preheat_mix_start_step', 0) or 0)
                    s2 = int(getattr(stage, 'preheat_mix_end_step', 0) or 0)
                    if s2 <= s1:
                        teacher_ratio = 1.0 if step <= s1 else 0.0
                    else:
                        if step <= s1:
                            teacher_ratio = 1.0
                        elif step >= s2:
                            teacher_ratio = 0.0
                        else:
                            prog = (step - s1) / max(1, s2 - s1)
                            teacher_ratio = 1.0 - prog
                    teacher_ratio = float(torch.clamp(torch.tensor(teacher_ratio, device=device), 0.0, 1.0).item())
                    mix_ratio = 1.0 - teacher_ratio

                    # åŒè·¯ç‰¹å¾æ··åˆ - ä»åŸå§‹ç›®æ ‡ç‰¹å¾æ¸å˜åˆ°ç¼–è§£ç å™¨é‡å»ºç‰¹å¾
                    y_hat_features = (1.0 - mix_ratio) * y_original + mix_ratio * y_hat
                    preheat_mix_ratio = mix_ratio
                else:
                    y_hat_features = y_hat
                    preheat_mix_ratio = 1.0

                # æ³¢å½¢ç”Ÿæˆ - ç¡®ä¿target_lenä¸chunkedç‰¹å¾å¯¹é½
                feature_frames = y_hat_features.size(1)
                target_len = feature_frames * FRAME_HOP_SAMPLES

                if original_audio is not None:
                    while original_audio.dim() > 2:
                        original_audio = original_audio.squeeze(1)
                    if original_audio.dim() == 1:
                        original_audio = original_audio.unsqueeze(0)
                    current_len = original_audio.size(-1)
                    if current_len < target_len:
                        pad = target_len - current_len
                        original_audio = F.pad(original_audio, (0, pad))
                    elif current_len > target_len:
                        original_audio = original_audio[..., :target_len]

                # å‡†å¤‡ç›®æ ‡éŸ³é¢‘å¼ é‡ä¾› FARGAN teacher forcing ä½¿ç”¨ï¼ˆç»Ÿä¸€ä¸º [B, L]ï¼‰
                y_target_audio = None
                if original_audio is not None:
                    y_target_audio = original_audio.squeeze(1) if original_audio.dim() == 3 else original_audio
                    if stage.name == "wave_preheat" and getattr(stage, 'preheat_chunk_frames', 0) > 0:
                        chunk_len = stage.preheat_chunk_frames
                        expected_audio_len = chunk_len * FRAME_HOP_SAMPLES
                        if y_target_audio.size(-1) > expected_audio_len:
                            y_target_audio = y_target_audio[..., :expected_audio_len]

                detach_wave = getattr(stage, 'train_wave_head_only', False)

                # === é€šé“çº§TFå–‚ç»™æ³¢å½¢å¤´ï¼šé¢„çƒ­æœŸå¼ºåˆ¶ä½¿ç”¨çœŸå€¼F0ï¼Œå…¶ä»–é€šé“æŒ‰mixæ··åˆ ===
                y_in = y_hat_features
                if stage.name == "wave_preheat":
                    try:
                        spec = get_feature_spec(feature_spec_type)
                        f0_slice = spec.get_feature_slice('f0') if hasattr(spec, 'get_feature_slice') else slice(18, 19)
                        y_in = y_hat_features.clone()
                        y_in[..., f0_slice] = y_original[..., f0_slice]
                    except Exception:
                        pass

                y_in = torch.nan_to_num(y_in, nan=0.0, posinf=1e3, neginf=-1e3).clamp(-6.0, 6.0)

                # æ³¢å½¢ç”Ÿæˆ(ç¦ç”¨æ—©æœŸAMPé¿å…ä¸‹æº¢)
                use_amp_wave = (device.type == 'cuda') and not (stage.name == "wave_preheat" and step <= (int(getattr(stage, 'preheat_mix_end_step', 0) or 0) + 300))
                with _autocast_ctx(enabled=use_amp_wave):
                    bypass_condition = stage.name == "wave_preheat" and step <= int(getattr(stage, 'preheat_mix_end_step', 0) or 0) + 300

                    # FARGAN Teacher Forcing: ä½¿ç”¨ pre å‚æ•°è¿›è¡Œæ—©æœŸç¨³å®š
                    fargan_pre = None
                    if (decoder_type == 'aether_fargan' and hasattr(decoder, 'fargan_core') and
                        stage.name == "wave_preheat" and 'teacher_ratio' in locals()):
                        # åœ¨teacher forcingæœŸé—´ä½¿ç”¨ç›®æ ‡éŸ³é¢‘çš„å‰å‡ å¸§ä½œä¸ºpre
                        if teacher_ratio > 0.1:  # åªæœ‰åœ¨teacher forcingè¾ƒå¼ºæ—¶æ‰ä½¿ç”¨
                            try:
                                # ä¸ºFARGANæä¾›å‰åºéŸ³é¢‘å¸§ä½œä¸ºç¨³å®šå¼•å¯¼
                                pre_frames = min(2, y_target_audio.size(-1) // 160)  # ä¸train_farganä¿æŒä¸€è‡´ï¼ˆ2å¸§ï¼‰
                                if pre_frames > 0:
                                    fargan_pre = y_target_audio[..., :pre_frames * 160]
                                    # æ·»åŠ åˆ°csi_dictç”¨äºä¼ é€’ç»™FARGAN
                                    if csi_dict is None:
                                        csi_dict = {}
                                    csi_dict['fargan_pre'] = fargan_pre
                            except Exception as e:
                                if step % 100 == 0:
                                    print(f"   âš ï¸ FARGAN teacher forcing setupå¤±è´¥: {e}")

                    if getattr(wave_head, '_is_exciter', False):
                        try:
                            setattr(wave_head.exciter, "_bypass_output_tanh", bypass_condition)
                            y_hat_audio = wave_head(y_in, target_len=target_len, csi_dict=csi_dict)
                        except Exception:
                            y_hat_audio = wave_head(y_in, target_len=target_len)
                    else:
                        try:
                            y_hat_audio = wave_head(y_in, target_len=target_len, csi_dict=csi_dict)
                        except Exception:
                            y_hat_audio = wave_head(y_in, target_len=target_len)


                    # é¢„çƒ­é˜¶æ®µå¹…åº¦æå‡
                    if getattr(stage, 'train_wave_head_only', False):
                        y_hat_audio = preheat_gain * y_hat_audio

                # æ—©æœŸé¢„çƒ­é˜¶æ®µæ³¨å…¥å°å™ªå£°é¿å…æ­»åŒº
                if stage.name == "wave_preheat" and step <= 5000:
                    noise = 1e-4 * torch.randn_like(y_hat_audio)
                    y_hat_audio = y_hat_audio + noise

                raw_wave = y_hat_audio
                y_hat_audio = torch.nan_to_num(raw_wave, nan=0.0, posinf=1.0, neginf=-1.0)

                # RMSèƒ½é‡å¼‚å¸¸ç›‘æ§å’Œè‡ªæ•‘
                anomaly_type = check_energy_anomaly(y_hat_audio, step, energy_anomaly_state)
                if anomaly_type is not None:
                    rescue_applied = apply_energy_rescue(wave_head, anomaly_type, step)
                    if rescue_applied and anomaly_type == 'low_energy':
                        # é‡æ–°ç”Ÿæˆæ³¢å½¢
                        try:
                            if getattr(wave_head, '_is_exciter', False):
                                y_hat_audio = wave_head(y_in, target_len=target_len, csi_dict=csi_dict)
                            else:
                                y_hat_audio = wave_head(y_in, target_len=target_len)
                            y_hat_audio = torch.nan_to_num(y_hat_audio, nan=0.0, posinf=1.0, neginf=-1.0)
                        except:
                            pass

                # ç›®æ ‡éŸ³é¢‘å·²æå‰æ„é€ ä¸º y_target_audio
                min_len = min(y_hat_audio.size(-1), y_target_audio.size(-1))
                y_hat_audio = y_hat_audio[..., :min_len]
                y_target_audio = y_target_audio[..., :min_len]

                # è®¡ç®—å¯¹é½å¢ç›Š
                eps = 1e-12
                pred_energy = y_hat_audio.pow(2).mean(dim=-1, keepdim=True)
                tgt_energy = y_target_audio.pow(2).mean(dim=-1, keepdim=True)
                gain_pure = torch.sqrt((tgt_energy + eps) / (pred_energy + eps))
                gain_pure = torch.nan_to_num(gain_pure, nan=1.0, posinf=3.0, neginf=0.25).clamp(0.25, 3.0)
                y_hat_pure_aligned = gain_pure * y_hat_audio

                # ç›‘å¬æ··åˆ(ä»…ç”¨äºç›‘å¬ï¼Œä¸å‚ä¸loss)
                y_hat_play = y_hat_audio.clone()
                if stage.name == "wave_preheat" and preheat_mix_ratio < 0.5:
                    with torch.no_grad():
                        y_hat_play = preheat_scale.detach() * y_target_audio + y_hat_play

                # âœ… æ˜ç¡®åˆ†ç¦»ï¼šæ³¢å½¢æŸå¤±åªå¤„ç†æ³¢å½¢ï¼Œç‰¹å¾æŸå¤±åªå¤„ç†ç‰¹å¾
                # å¼ºæ ¡éªŒï¼šç¡®ä¿æ˜¯æ³¢å½¢è€Œéç‰¹å¾
                def validate_waveform_tensor(tensor, name):
                    if tensor.dim() != 2:  # æœŸæœ› [B, T_audio]
                        raise ValueError(f"{name} åº”è¯¥æ˜¯æ³¢å½¢ [B, T_audio]ï¼Œå®é™…å½¢çŠ¶: {tensor.shape}")
                    return tensor

                pred_wav = validate_waveform_tensor(y_hat_audio.squeeze() if y_hat_audio.dim() > 2 else y_hat_audio, "pred_wav")
                target_wav = validate_waveform_tensor(y_target_audio.squeeze() if y_target_audio.dim() > 2 else y_target_audio, "target_wav")

                # æ³¢å½¢é•¿åº¦å¯¹é½
                min_len = min(pred_wav.size(-1), target_wav.size(-1))
                min_batch = min(pred_wav.size(0), target_wav.size(0))
                pred = pred_wav[:min_batch, :min_len]
                target = target_wav[:min_batch, :min_len]

                # èƒ½é‡è‡ªä¸¾ - ä»…åœ¨æ—©æœŸæä¾›è½»åº¦å¢ç›Šï¼Œä¹‹åå›è½ä¸º1
                pred_rms = torch.sqrt(torch.mean(pred.pow(2), dim=-1) + eps)
                tgt_rms = torch.sqrt(torch.mean(target.pow(2), dim=-1) + eps)
                warmup = 500
                if step <= warmup:
                    with torch.no_grad():
                        energy_scale = (tgt_rms / (pred_rms + eps)).clamp(0.8, 1.25).unsqueeze(-1)
                else:
                    energy_scale = 1.0
                pred_for_loss = pred * energy_scale

                use_fast_loss = wave_loss_fast is not None and step > 200
                wl_obj = wave_loss_fast if use_fast_loss else wave_loss
                if stage.name == "wave_preheat" and step <= 20000:
                    with _autocast_ctx(enabled=False):
                        mrstft_loss = mr_stft_boost * wl_obj(pred_for_loss.float(), target.float())
                        # + Mel é¢‘è°± L1ï¼ˆlogåŸŸï¼Œ80-binï¼Œä¸è¯„ä¼°ç®¡çº¿ä¸€è‡´ï¼‰
                        mel_tf = _get_mel_transform(device)
                        def _logmel_wave(x_2d):          # x_2d: [B, T]
                            M = mel_tf(x_2d.float())
                            return (M.clamp_min(1e-8)).log()

                        mel_pred = _logmel_wave(pred_for_loss)
                        mel_tgt  = _logmel_wave(target)
                        mel_l1   = torch.mean(torch.abs(mel_pred - mel_tgt))

                        # å°æƒé‡èåˆåˆ°å½“å‰ mrstft_loss
                        mrstft_loss = mrstft_loss + 0.20 * mel_l1
                        # + éŸ³é¢‘çº§ F0 å¯¹é½ï¼ˆä»…åœ¨æœ‰å£°å¸§ä¸Šç»Ÿè®¡ï¼‰
                        _spec = get_feature_spec(feature_spec_type)
                        f0_tgt = _spec.extract_feature(y, 'f0') if hasattr(_spec, 'extract_feature') else y[:, :, 18:19]  # [B,T,1] (ç‰¹å¾åŸŸ)
                        if hasattr(_spec, 'extract_feature'):
                            voi_tgt = _spec.extract_feature(y, 'voicing')   # [B,T,1]
                        else:
                            voi_tgt = (y[:, :, 18:19] > -1.0).float()  # FARGAN: ä»dnn_pitchæ¨å¯¼
                        # ä¸åˆæˆå™¨ä¸€è‡´çš„ Hz æ˜ å°„
                        if not disable_f0_loss:
                            f0_tgt_hz = (SAMPLE_RATE * torch.pow(2.0, f0_tgt.squeeze(-1) - 6.5)).unsqueeze(-1)
                            f0_wave_l = audio_f0_alignment_loss(
                                pred_for_loss.unsqueeze(1), target.unsqueeze(1),
                                f0_tgt_hz, sr=SAMPLE_RATE, hop=FRAME_HOP_SAMPLES, v_mask=voi_tgt
                            )
                            mrstft_loss = mrstft_loss + 0.10 * f0_wave_l


                else:
                    with _autocast_ctx(enabled=use_amp_step):
                        with _autocast_ctx(enabled=False):
                            mrstft_loss = mr_stft_boost * wl_obj(pred_for_loss.float(), target.float())
                        # + Mel é¢‘è°± L1ï¼ˆlogåŸŸï¼Œ80-binï¼Œä¸è¯„ä¼°ç®¡çº¿ä¸€è‡´ï¼‰
                        mel_tf = _get_mel_transform(device)
                        def _logmel_wave(x_2d):          # x_2d: [B, T]
                            M = mel_tf(x_2d.float())
                            return (M.clamp_min(1e-8)).log()

                        mel_pred = _logmel_wave(pred_for_loss)
                        mel_tgt  = _logmel_wave(target)
                        mel_l1   = torch.mean(torch.abs(mel_pred - mel_tgt))

                        # å°æƒé‡èåˆåˆ°å½“å‰ mrstft_loss
                        mrstft_loss = mrstft_loss + 0.20 * mel_l1
                        # + éŸ³é¢‘çº§ F0 å¯¹é½ï¼ˆä»…åœ¨æœ‰å£°å¸§ä¸Šç»Ÿè®¡ï¼‰
                        _spec = get_feature_spec(feature_spec_type)
                        f0_tgt = _spec.extract_feature(y, 'f0') if hasattr(_spec, 'extract_feature') else y[:, :, 18:19]  # [B,T,1] (ç‰¹å¾åŸŸ)
                        if hasattr(_spec, 'extract_feature'):
                            voi_tgt = _spec.extract_feature(y, 'voicing')   # [B,T,1]
                        else:
                            voi_tgt = (y[:, :, 18:19] > -1.0).float()  # FARGAN: ä»dnn_pitchæ¨å¯¼
                        # ä¸åˆæˆå™¨ä¸€è‡´çš„ Hz æ˜ å°„
                        if not disable_f0_loss:
                            f0_tgt_hz = (SAMPLE_RATE * torch.pow(2.0, f0_tgt.squeeze(-1) - 6.5)).unsqueeze(-1)
                            f0_wave_l = audio_f0_alignment_loss(
                                pred_for_loss.unsqueeze(1), target.unsqueeze(1),
                                f0_tgt_hz, sr=SAMPLE_RATE, hop=FRAME_HOP_SAMPLES, v_mask=voi_tgt
                            )
                            mrstft_loss = mrstft_loss + 0.10 * f0_wave_l



                # RMSå¯¹é½æŸå¤± - å…³é”®çš„èµ·æŒ¯è¾…åŠ©
                def rms_db(x):
                    return 20 * torch.log10(x.pow(2).mean(dim=-1).clamp_min(1e-8).sqrt() + 1e-8)

                rms_loss = (rms_db(pred) - rms_db(target)).abs().mean()

                # åŠ¨æ€æƒé‡ï¼šå‰3000æ­¥é‡ç‚¹å­¦å“åº¦ï¼Œä¹‹åè½¬å‘ç»†èŠ‚
                warmup_steps = 3000
                if step <= warmup_steps:
                    lambda_rms = 2.0 * (1.0 - step / warmup_steps) + 0.2
                    lambda_stft = 0.5 + 0.5 * (step / warmup_steps)
                else:
                    lambda_rms = 0.2
                    lambda_stft = 1.0

                # é¢å¤–æŸå¤±é¡¹
                log_rms_diff = torch.abs((pred_rms + eps).log() - (tgt_rms + eps).log()).mean()
                l_time = torch.abs(pred - target).mean()
                floor_level = 10 ** (-45.0 / 20.0)
                floor_penalty = torch.relu(floor_level - pred_rms).mean()
                dc_penalty = pred.mean(dim=-1).abs().mean()
                l_wav_l1 = F.l1_loss(pred_for_loss, target)
                l_rms = F.l1_loss(pred_rms, tgt_rms)
                si_snr_loss = 0.0
                if stage.name != "wave_preheat" or step > 400:
                    si_snr_vals = compute_si_snr(pred_for_loss, target)
                    si_snr_loss = -si_snr_vals.mean()

                # â€” RMSé—¨æ§ä¸F0å¥åº·é—­ç¯æ§åˆ¶ â€”
                # é¢„çƒ­æ—©æœŸæˆ–å¼±éŸ³æ—¶ï¼Œæš‚æ—¶å…³é—­STFT/Si-SNRï¼Œä»…ä¿ç•™å°æƒé‡çš„æ—¶åŸŸL1/RMS
                try:
                    pred_rms_db_mean = float((20.0 * torch.log10(torch.clamp(pred_rms.mean(), min=1e-8))).detach().cpu().item())
                except Exception:
                    pred_rms_db_mean = -100.0

                f0_alert_active = False
                if 'f0_alert_until_step' in locals():
                    f0_alert_active = step <= f0_alert_until_step

                # åŠ¨æ€é—¨æ§é˜ˆå€¼ï¼š-35 dB @ step=0 çº¿æ€§å‡åˆ° -28 dB @ stepâ‰ˆ1000
                thr_db = -35.0 + 7.0 * min(1.0, step / 1000.0)
                gating_active = (stage.name == "wave_preheat") and ((pred_rms_db_mean < thr_db) or (step < 400) or f0_alert_active)

                if gating_active:
                    wave_loss_val = (
                        0.10 * l_time +
                        0.10 * l_rms +
                        0.02 * dc_penalty
                    )
                else:
                    # åº”ç”¨åŠ¨æ€æƒé‡çš„æŸå¤±ç»„åˆï¼ˆå®Œæ•´ï¼‰
                    wave_loss_val = (
                        lambda_stft * mrstft_loss +      # åŠ¨æ€STFTæƒé‡
                        lambda_rms * rms_loss +          # åŠ¨æ€RMSå¯¹é½æƒé‡
                        0.5 * log_rms_diff +             # åŸæœ‰RMSæŸå¤±
                        0.2 * l_time +                   # L1æŸå¤±
                        2.0 * floor_penalty +            # é™éŸ³æƒ©ç½š
                        0.05 * dc_penalty +              # ç›´æµæƒ©ç½š
                        0.05 * l_wav_l1 +                # æ³¢å½¢å¹…åº¦çº¦æŸ
                        0.02 * l_rms +                   # RMSå¹…åº¦çº¦æŸ
                        0.5 * si_snr_loss                # SI-SNR æå‡æ—¶é—´åŸŸä¸€è‡´æ€§
                    )
                if not disable_f0_loss:
                    spec = get_feature_spec(feature_spec_type)
                    f0_tgt_hz = spec.extract_feature(y, 'f0') if hasattr(spec, 'extract_feature') else y[:, :, 18:19]
                    if hasattr(spec, 'extract_feature'):
                        voi_mask = spec.extract_feature(y, 'voicing')   # [B,T,1]
                    else:
                        voi_mask = (y[:, :, 18:19] > -1.0).float()
                    f0_align  = audio_f0_alignment_loss(
                        y_hat_audio.unsqueeze(1), y_target_audio.unsqueeze(1),
                        f0_tgt_hz, sr=16000, hop=160, v_mask=voi_mask
                    )
                    wave_loss_val = wave_loss_val + 0.02 * f0_align

                if hasattr(decoder, 'synth') and hasattr(decoder.synth, 'tilt'):
                    wave_loss_val = wave_loss_val + 1e-4 * (decoder.synth.tilt ** 2)

                # AETHERFARGANDecoder: ä½¿ç”¨ FARGAN è®­ç»ƒæŸå¤±ä½œä¸ºä¸»è¦æŸå¤±
                if decoder_type == 'aether_fargan' and hasattr(decoder, 'fargan_core'):
                    try:
                        # ä»FARGANç‰¹å¾ä¼°è®¡å‘¨æœŸï¼ˆç”¨äºFARGANè®­ç»ƒæŸå¤±ï¼‰
                        if hasattr(decoder, '_estimate_period'):
                            # ä½¿ç”¨è§£ç å™¨çš„å‘¨æœŸä¼°è®¡
                            estimated_period = decoder._estimate_period(y_hat)
                        else:
                            # å›é€€æ–¹æ¡ˆï¼šä»ç‰¹å¾æ‰‹åŠ¨ä¼°è®¡å‘¨æœŸ
                            dnn_pitch = y_hat[..., 18:19] if y_hat.size(-1) > 18 else torch.zeros_like(y_hat[..., :1])
                            period_raw = 256.0 / torch.pow(2.0, dnn_pitch + 1.5)
                            estimated_period = torch.round(torch.clamp(period_raw, 32.0, 255.0)).long().squeeze(-1)

                        # è®¡ç®— FARGAN è®­ç»ƒæŸå¤± (comprehensive training loss)
                        from training.fargan_losses import compute_fargan_training_loss
                        fargan_train_loss, fargan_train_details = compute_fargan_training_loss(
                            y_hat_audio.squeeze(1), y_target_audio.squeeze(1), estimated_period,
                            frame_size=160, subframe_size=40, device=device
                        )

                        # è®¡ç®— FARGAN åŸç‰ˆæŸå¤± (ä½œä¸ºè¾…åŠ©æŸå¤±)ï¼Œèšç„¦äº pre ä¹‹åçš„é¦–å¸§
                        focus_start = 0
                        if 'fargan_pre' in (csi_dict or {}) and csi_dict['fargan_pre'] is not None:
                            focus_start = int(csi_dict['fargan_pre'].size(-1))
                        orig_total, orig_details = compute_fargan_original_style_loss(
                            y_hat_audio.squeeze(1) if y_hat_audio.dim() == 3 else y_hat_audio,
                            y_target_audio.squeeze(1) if y_target_audio.dim() == 3 else y_target_audio,
                            device=device, frame_size=160, focus_start=focus_start
                        )

                        # æƒé‡è°ƒåº¦ï¼šFARGANè®­ç»ƒæŸå¤±ä¸ºä¸»ï¼ŒåŸç‰ˆæŸå¤±ä¸ºè¾…
                        ramp_steps = 5000  # rampæœŸé•¿åº¦
                        if step < ramp_steps:
                            # æ¸è¿›å¢åŠ FARGANè®­ç»ƒæŸå¤±æƒé‡
                            t = step / ramp_steps
                            smooth_t = 3 * t * t - 2 * t * t * t
                            fargan_train_weight = 0.5 + 0.5 * smooth_t  # 0.5 -> 1.0
                            orig_weight = 0.3 * (1.0 - smooth_t)  # 0.3 -> 0.0
                        else:
                            fargan_train_weight = 1.0
                            orig_weight = 0.1  # ä¿ç•™å°‘é‡åŸç‰ˆæŸå¤±

                        # ç»„åˆæŸå¤±ï¼šä»¥FARGANè®­ç»ƒæŸå¤±ä¸ºä¸»
                        combined_fargan_loss = (
                            fargan_train_weight * fargan_train_loss +
                            orig_weight * orig_total
                        )

                        # MR-STFTå¢å¼ºä¿æŒä¸å˜
                        mr_stft_boost = 1.0
                        if step > ramp_steps * 0.6:
                            if step <= ramp_steps * 0.8:
                                mr_stft_boost = 1.5
                            else:
                                mr_stft_boost = 2.0

                        # æœ€ç»ˆæ³¢å½¢æŸå¤±ï¼šMR-STFT + FARGANç»„åˆæŸå¤±
                        wave_loss_val = mr_stft_boost * wave_loss_val + combined_fargan_loss

                        # è®°å½•ç”¨äºæ—¥å¿—
                        fargan_loss_logs = {
                            'fargan_train_weight': fargan_train_weight,
                            'fargan_train_loss': fargan_train_loss.item(),
                            'orig_weight': orig_weight,
                            'orig_loss': orig_total.item(),
                            'combined_fargan': combined_fargan_loss.item(),
                            'mr_stft_boost': mr_stft_boost,
                            'l1': fargan_train_details.get('l1', torch.tensor(0.0)).item(),
                            'pitch_consistency': fargan_train_details.get('pitch_consistency', torch.tensor(0.0)).item(),
                            'subframe_alignment': fargan_train_details.get('subframe_alignment', torch.tensor(0.0)).item(),
                            'ramp_progress': step / ramp_steps if step < ramp_steps else 1.0
                        }
                    except Exception as e:
                        if step % 100 == 0:
                            print(f"âš ï¸ FARGANæŸå¤±è®¡ç®—å¤±è´¥ at step {step}: {e}")

                wave_loss_val = torch.nan_to_num(wave_loss_val, nan=0.0, posinf=1e4, neginf=1e4)


        # æ€»æŸå¤±
        if stage.name == "clean_baseline":
            total_loss = _finite_scalar(recon_loss, "recon_total", step)
        elif stage.name == "wave_preheat":
            if stage.enable_audio_quality and original_audio is not None:
                # æ”¾å¤§æ³¢å½¢æŸå¤±å¹…åº¦ï¼Œç¡®ä¿åˆæˆå¤´å¾—åˆ°è¶³å¤Ÿå¤§æ¢¯åº¦ï¼›ä¿ç•™å°‘é‡F0å¼•å¯¼
                total_f0_for_mix = total_f0 if 'total_f0' in locals() else torch.tensor(0.0, device=device)
                total_loss = _finite_scalar(30.0 * wave_loss_val, "wave_loss_scaled", step) \
                           + _finite_scalar(0.1 * total_f0_for_mix, "f0_mix_scaled", step)
            else:
                total_loss = _finite_scalar(recon_loss, "recon_total", step)
        else:
            total_loss = _finite_scalar(recon_loss, "recon_total", step) \
                       + _finite_scalar(rate_loss_val, "rate_loss", step) \
                       + _finite_scalar(wave_loss_val, "wave_loss", step)

        # é«˜æŸå¤±æ‰¹æ¬¡çš„è½¯è·³è¿‡æœºåˆ¶
        if not torch.isfinite(total_loss).all() or total_loss.item() > 50.0:
            if total_loss.item() > 50.0:
                print(f"âš ï¸ æ­¥éª¤ {step}: æŸå¤±è¿‡å¤§ ({total_loss.item():.2f}), è·³è¿‡æ­¤æ‰¹æ¬¡")
                optimizer.zero_grad(set_to_none=True)
                # ä¸æ‰§è¡Œ scaler.update()ï¼šæœ¬æ­¥æœªè¿›è¡Œç¼©æ”¾/åä¼ ï¼Œé¿å… torch.amp çš„ _scale æ–­è¨€
                continue

        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if not torch.isfinite(total_loss).all():
            try:
                spec = get_feature_spec(feature_spec_type)
                if feature_spec_type == "fargan":
                    names = ["ceps", "dnn_pitch", "frame_corr", "lpc"]
                else:
                    names = ["ceps","f0","voicing","enhanced","lpc","prosodic"]
                flags = []
                for n in names:
                    if hasattr(spec, 'get_feature_slice'):
                        sl = spec.get_feature_slice(n)
                    else:
                        # FARGAN hard-coded slices
                        slice_map = {"ceps": slice(0, 18), "dnn_pitch": slice(18, 19), "frame_corr": slice(19, 20), "lpc": slice(20, 36)}
                        sl = slice_map.get(n, slice(0, 1))
                    p, t = y_hat[..., sl], y[..., sl]
                    flags.append(f"{n}:pred_bad={torch.isnan(p).any().item() or torch.isinf(p).any().item()},"
                                f"tgt_bad={torch.isnan(t).any().item() or torch.isinf(t).any().item()}")
                print("âš ï¸ Loss NaN å®¡è®¡: " + " | ".join(flags))
            except Exception as e:
                print(f"âš ï¸ å®¡è®¡å¤±è´¥: {e}")
            print(f"âš ï¸ æ­¥éª¤ {step}: æ£€æµ‹åˆ°æŸå¤±å¼‚å¸¸ (NaN/Inf), è·³è¿‡æ­¤æ­¥")
            continue


        # åå‘ä¼ æ’­ - å¢å¼ºçš„æ•°å€¼ç¨³å®šæ€§
        try:
            if scaler is not None:
                scaler.scale(total_loss).backward()

                # æ£€æŸ¥æ¢¯åº¦å‰éœ€è¦å…ˆunscale
                scaler.unscale_(optimizer)
                grad_ok = _grad_ok([encoder, decoder, wave_head], debug=(step <= 10))

                # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ¢¯åº¦ç»Ÿè®¡ï¼ˆAMP åˆ†æ”¯ï¼‰
                if step <= 10 or step % 200 == 0:
                    # å…ˆæ”¶é›†åŸºç¡€æ¢¯åº¦ï¼Œé¿å…æœªå®šä¹‰å˜é‡
                    encoder_grads = [p.grad for p in encoder.parameters() if p.grad is not None]
                    decoder_grads = [p.grad for p in decoder.parameters() if p.grad is not None]
                    # å†æ”¶é›† F0 ç›¸å…³æ¢¯åº¦
                    f0_encoder_grads = [
                        p.grad for n, p in encoder.named_parameters()
                        if 'f0_encoder' in n and p.grad is not None
                    ]
                    f0_decoder_grads = [
                        p.grad for n, p in decoder.named_parameters()
                        if ('f0' in n or 'voic' in n) and p.grad is not None
                    ]
                    # æ‰“å°ç»Ÿè®¡
                    if encoder_grads:
                        enc_grad_norm = torch.stack([g.norm() for g in encoder_grads]).mean().item()
                        print(f"ğŸ”§ Step {step} æ¢¯åº¦ç»Ÿè®¡:")
                        print(f"   ç¼–ç å™¨æ¢¯åº¦èŒƒæ•°: {enc_grad_norm:.6f}")
                    if decoder_grads:
                        dec_grad_norm = torch.stack([g.norm() for g in decoder_grads]).mean().item()
                        print(f"   è§£ç å™¨æ¢¯åº¦èŒƒæ•°: {dec_grad_norm:.6f}")
                    if f0_encoder_grads:
                        f0_enc_norm = torch.stack([g.norm() for g in f0_encoder_grads]).mean().item()
                        print(f"   ğŸ¯ F0ç¼–ç å™¨æ¢¯åº¦èŒƒæ•°: {f0_enc_norm:.6f}")
                    if f0_decoder_grads:
                        f0_dec_norm = torch.stack([g.norm() for g in f0_decoder_grads]).mean().item()
                        print(f"   ğŸ¯ F0è§£ç å™¨æ¢¯åº¦èŒƒæ•°: {f0_dec_norm:.6f}")


                if grad_ok:
                    # å…ˆæŒ‰å€¼åŸŸè£å‰ªï¼Œå†æŒ‰èŒƒæ•°è£å‰ªï¼ŒåŒé‡ç¨³å¥
                    torch.nn.utils.clip_grad_value_(
                        [p for group in optimizer.param_groups for p in group['params']],
                        clip_value=1.0
                    )
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        [p for group in optimizer.param_groups for p in group['params']],
                        max_norm=1.0
                    )
                    if step <= 10 or step % 200 == 0:
                        print(f"   è£å‰ªå‰æ¢¯åº¦èŒƒæ•°: {grad_norm.item():.6f}")

                # âœ… åªæœ‰åœ¨æ¢¯åº¦æœ‰é™æ—¶æ‰æ‰§è¡Œ stepï¼›æ— è®ºå¦‚ä½•éƒ½è¦ update()
                if grad_ok:
                    scaler.step(optimizer)
                else:
                    print(f"âš ï¸ æ­¥éª¤ {step}: æ£€æµ‹åˆ°æ¢¯åº¦å¼‚å¸¸ï¼Œå·²è·³è¿‡æœ¬æ¬¡å‚æ•°æ›´æ–°")
                    optimizer.zero_grad(set_to_none=True)
                
                scaler.update()

            else:
                total_loss.backward()
                grad_ok = _grad_ok([encoder, decoder, wave_head], debug=(step <= 10))

                # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ¢¯åº¦ç»Ÿè®¡ (éAMPæ¨¡å¼)
                if step <= 10 or step % 200 == 0:
                    encoder_grads = [p.grad for p in encoder.parameters() if p.grad is not None]
                    decoder_grads = [p.grad for p in decoder.parameters() if p.grad is not None]
                    if encoder_grads:
                        enc_grad_norm = torch.stack([g.norm() for g in encoder_grads]).mean().item()
                        print(f"ğŸ”§ Step {step} æ¢¯åº¦ç»Ÿè®¡ (éAMP):")
                        print(f"   ç¼–ç å™¨æ¢¯åº¦èŒƒæ•°: {enc_grad_norm:.6f}")
                    if decoder_grads:
                        dec_grad_norm = torch.stack([g.norm() for g in decoder_grads]).mean().item()
                        print(f"   è§£ç å™¨æ¢¯åº¦èŒƒæ•°: {dec_grad_norm:.6f}")
                    print(f"   æ¢¯åº¦æ£€æŸ¥é€šè¿‡: {grad_ok}")

                if grad_ok:
                    # åŒé‡æ¢¯åº¦è£å‰ªï¼šå…ˆå€¼åŸŸè£å‰ªï¼Œå†èŒƒæ•°è£å‰ªï¼ˆAMPåˆ†æ”¯ï¼‰
                    all_params = [p for group in optimizer.param_groups for p in group['params']]
                    torch.nn.utils.clip_grad_value_(all_params, clip_value=1.0)
                    grad_norm = torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)
                    if step <= 10 or step % 200 == 0:
                        print(f"   è£å‰ªå‰æ¢¯åº¦èŒƒæ•°: {grad_norm.item():.6f}")
                    optimizer.step()
                else:
                    print(f"âš ï¸ æ­¥éª¤ {step}: æ£€æµ‹åˆ°æ¢¯åº¦å¼‚å¸¸ï¼Œå·²è·³è¿‡æœ¬æ¬¡å‚æ•°æ›´æ–°")
                    optimizer.zero_grad(set_to_none=True)
                    continue
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"âš ï¸ æ­¥éª¤ {step}: GPUå†…å­˜ä¸è¶³, è·³è¿‡æ­¤æ­¥")
                torch.cuda.empty_cache()
                continue
            else:
                print(f"âš ï¸ æ­¥éª¤ {step}: åå‘ä¼ æ’­å¼‚å¸¸: {e}")
                continue

        # é«˜çº§FiLMè°ƒåº¦
        if film_scheduler is not None:
            film_scheduler.step()

        # å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°
        if decoder_type == 'aether_fargan':
            if step < ramp_steps and not scheduler_switched:
                # ramp æœŸé—´ä½¿ç”¨ LambdaLR
                lr_scheduler.step()
            elif step >= ramp_steps and not scheduler_switched:
                # ramp ç»“æŸï¼Œåˆ‡æ¢åˆ° ReduceLROnPlateau
                print(f"ğŸ”„ Step {step}: åˆ‡æ¢å­¦ä¹ ç‡è°ƒåº¦å™¨ä» LambdaLR åˆ° ReduceLROnPlateau")
                scheduler_switched = True
            elif scheduler_switched:
                # ä½¿ç”¨ ReduceLROnPlateauï¼ŒåŸºäºéªŒè¯æŸå¤±è°ƒæ•´
                plateau_scheduler.step(current_loss)
        else:
            # é FARGAN æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†è°ƒåº¦
            lr_scheduler.step()

        # è®°å½•æœ€ä½³æŸå¤±
        current_loss = float(total_loss.detach().cpu())
        convergence_losses.append(current_loss)
        if current_loss < best_loss:
            best_loss = current_loss

        # ä¸­é—´æ£€æŸ¥ç‚¹ä¿å­˜
        if step % checkpoint_every == 0:
            intermediate_checkpoint_path = checkpoint_dir / f"stage_{current_stage_index}_{stage.name}_step_{step}.pth"
            torch.save({
                'stage_index': current_stage_index,
                'stage_name': stage.name,
                'encoder_state_dict': encoder.state_dict(),
                'decoder_state_dict': decoder.state_dict(),
                'wave_head_state_dict': wave_head.state_dict(),
                'step': step,
                'best_loss': best_loss,
                'current_loss': current_loss,
                'optimizer_state_dict': optimizer.state_dict()
            }, intermediate_checkpoint_path)
            print(f"  ğŸ’¾ ä¸­é—´æ£€æŸ¥ç‚¹å·²ä¿å­˜: {intermediate_checkpoint_path.name}")

        # æ—¥å¿—è¾“å‡º
        if step % 50 == 0 or step <= 10:
            film_info = ""
            if stage.use_film and hasattr(encoder, 'film_ratio'):
                film_info = f" film={encoder.film_ratio:.3f}"

            pred_rms_db = 20 * torch.log10(torch.sqrt(torch.mean(pred.pow(2)) + 1e-12) + 1e-12).item() if 'pred' in locals() else 0

            # æ˜¾ç¤ºF0æŸå¤±ä¿¡æ¯
            f0_loss_str = ""
            if 'f0_losses' in locals() and f0_losses is not None:
                f0_total = f0_losses.get('total_f0_loss', torch.tensor(0.0))
                if f0_total.item() > 0:
                    f0_loss_str = f" f0_loss={f0_total.item():.6f}"

            # FARGANæŸå¤±ç›‘æ§
            fargan_loss_str = ""
            if fargan_loss_logs is not None and decoder_type == 'aether_fargan':
                logs = fargan_loss_logs
                if logs:
                    fargan_loss_str = (f" fargan_train={logs.get('fargan_train_loss', 0):.4f} "
                                     f"(w={logs.get('fargan_train_weight', 0):.2f}) "
                                     f"orig={logs.get('orig_loss', 0):.4f} "
                                     f"(w={logs.get('orig_weight', 0):.2f}) "
                                     f"combined={logs.get('combined_fargan', 0):.4f} "
                                     f"mr_boost={logs.get('mr_stft_boost', 1.0):.1f} "
                                     f"pitch={logs.get('pitch_consistency', 0):.4f} "
                                     f"subframe={logs.get('subframe_alignment', 0):.4f} "
                                     f"ramp={logs.get('ramp_progress', 0):.1%}")

            print(f"  æ­¥éª¤ {step}/{actual_steps} (epoch {current_epoch:.1f}): "
                  f"loss={current_loss:.6f} recon={recon_loss.item():.6f}{f0_loss_str}{fargan_loss_str} "
                  f"best={best_loss:.6f}{film_info} pred_rms={pred_rms_db:.1f}dB")

            # è¿½åŠ æ—¶åºè¯Šæ–­ï¼šå‚æ•°æ—¶é—´æ–¹å·®ä¸F0é€šé“å˜åŒ–
            try:
                with torch.no_grad():
                    tvar = y_hat.float().std(dim=1).mean().item()
                    msg = f"     â€¢ param_t.stdâ‰ˆ{tvar:.4f}"

                    # ä½¿ç”¨FeatureSpecæå–F0å’ŒéŸµå¾‹ç‰¹å¾è¿›è¡Œè¯Šæ–­
                    feature_spec = get_feature_spec(feature_spec_type)
                    if hasattr(feature_spec, 'extract_feature'):
                        f0_block = feature_spec.extract_feature(y_hat, 'f0')
                        prosodic_block = feature_spec.extract_feature(y_hat, 'prosodic')
                    else:
                        # FARGAN
                        f0_block = y_hat[:, :, 18:19]  # dnn_pitch
                        prosodic_block = y_hat[:, :, 19:20]  # frame_corr

                    f0_tvar = f0_block.float().std(dim=1).mean().item()
                    f0_mean = f0_block.float().mean().item()
                    f0_range = f0_block.float().max().item() - f0_block.float().min().item()

                    prosodic_tvar = prosodic_block.float().std(dim=1).mean().item()
                    prosodic_range = prosodic_block.float().max().item() - prosodic_block.float().min().item()

                    msg += f" | f0_t.stdâ‰ˆ{f0_tvar:.4f} meanâ‰ˆ{f0_mean:.3f} rangeâ‰ˆ{f0_range:.3f}"
                    msg += f" | prosodic_t.stdâ‰ˆ{prosodic_tvar:.4f} rangeâ‰ˆ{prosodic_range:.3f}"
                    print(msg)
            except Exception:
                pass

    # è®­ç»ƒåå¤„ç†
    encoder.eval()
    decoder.eval()
    wave_head.eval()

    # æ”¶æ•›æ€§æ£€æŸ¥
    if len(convergence_losses) >= 10:
        recent_avg = sum(convergence_losses[-10:]) / 10
        early_avg = sum(convergence_losses[:10]) / 10 if len(convergence_losses) >= 10 else recent_avg
        convergence_rate = max(0.0, early_avg - recent_avg)
    else:
        convergence_rate = 0.0

    # é˜¶æ®µéªŒæ”¶åˆ¤å®š
    passed = True
    fail_reasons = []
    audio_quality = {}

    # åŸºç¡€æŸå¤±éªŒæ”¶
    if stage.max_final_loss > 0 and best_loss > stage.max_final_loss:
        passed = False
        fail_reasons.append(f"æœ€ç»ˆæŸå¤± {best_loss:.6f} > {stage.max_final_loss}")

    if stage.min_convergence_rate > 0 and convergence_rate < stage.min_convergence_rate:
        passed = False
        fail_reasons.append(f"æ”¶æ•›ç‡ {convergence_rate:.6f} < {stage.min_convergence_rate}")

    # éŸ³é¢‘è´¨é‡éªŒæ”¶ - å…³é”®çš„PESQ/SNR Gate
    if stage.enable_audio_quality:
        print(f"\nğŸµ æ‰§è¡ŒéŸ³é¢‘è´¨é‡éªŒæ”¶...")
        try:
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            prev_encoder_mode = encoder.training
            prev_decoder_mode = decoder.training
            prev_wave_mode = wave_head.training
            encoder.eval()
            decoder.eval()
            wave_head.eval()

            # å–ä¸€ä¸ªéªŒè¯æ‰¹æ¬¡
            eval_batch = next(batch_iter)
            x_eval = eval_batch['x'].to(device, non_blocking=True)
            y_eval = eval_batch['y'].to(device, non_blocking=True)
            original_audio = eval_batch.get('audio')
            if original_audio is not None:
                original_audio = original_audio.to(device, non_blocking=True)
            csi_dict = {k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v
                       for k, v in eval_batch.get('csi', {}).items()}

            x_eval = torch.nan_to_num(x_eval, nan=0.0, posinf=1e4, neginf=-1e4)
            y_eval = torch.nan_to_num(y_eval, nan=0.0, posinf=1e4, neginf=-1e4)

            with torch.no_grad():
                # ç¼–è§£ç ï¼ˆè§£ç ç«¯åŒæ ·æ³¨å…¥acoustic_priorsï¼Œé¿å…CSIç»´åº¦ä¸ä¸€è‡´ï¼‰
                z_eval, _ = encoder(x_eval, csi_dict=csi_dict, inference=True)
                csi_dec_eval = dict(csi_dict)
                try:
                    csi_dec_eval["acoustic_priors"] = extract_acoustic_priors(x_eval).detach()
                except Exception:
                    pass
                y_hat_eval = decoder(z_eval, csi_dict=csi_dec_eval)

            # è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡
            csi_eval = dict(csi_dict)
            try:
                csi_eval["acoustic_priors"] = extract_acoustic_priors(x_eval).detach()
            except Exception:
                pass

            audio_quality = calculate_audio_quality(
                y_hat_eval, y_eval, wave_head, original_audio, csi_dict=csi_eval
            )

            if 'error' not in audio_quality:
                print(f"    SNR: {audio_quality['snr_db']:.2f} dB")
                print(f"    SI-SNR: {audio_quality['si_snr_db']:.2f} dB")
                print(f"    Mel Cosine: {audio_quality['mel_cos']:.3f}")
                print(f"    Mel L2: {audio_quality['mel_l2']:.4f}")
                print(f"    å…‰è°±ç›¸å…³æ€§: {audio_quality['feature_correlation']:.3f}")
                print(f"    å…‰è°±å¤±çœŸ: {audio_quality['spectral_distortion']:.4f}")
                print(f"    é¢„æµ‹RMS: {audio_quality['pred_rms_db']:.1f} dB")
                print(f"    ç›®æ ‡RMS: {audio_quality['target_rms_db']:.1f} dB")

                # éªŒæ”¶é—¨æ§›æ£€æŸ¥
                audio_passed = True
                if audio_quality['snr_db'] < stage.min_snr_db:
                    audio_passed = False
                    fail_reasons.append(f"SNRä¸è¶³: {audio_quality['snr_db']:.2f} < {stage.min_snr_db}")
                if audio_quality['mel_cos'] < stage.min_mel_cos:
                    audio_passed = False
                    fail_reasons.append(
                        f"Mel Cosineä¸è¶³: {audio_quality['mel_cos']:.3f} < {stage.min_mel_cos}"
                    )
                if audio_quality['mel_l2'] > stage.max_mel_l2:
                    audio_passed = False
                    fail_reasons.append(
                        f"Mel L2è¶…é™: {audio_quality['mel_l2']:.4f} > {stage.max_mel_l2}"
                    )
                if stage.max_spectral_distortion > 0.0 and audio_quality['spectral_distortion'] > stage.max_spectral_distortion:
                    audio_passed = False
                    fail_reasons.append(
                        f"è°±å¤±çœŸè¶…é™: {audio_quality['spectral_distortion']:.4f} > {stage.max_spectral_distortion}"
                    )
                if stage.max_rms_delta_db > 0.0:
                    rms_delta = abs(audio_quality['pred_rms_db'] - audio_quality['target_rms_db'])
                    if rms_delta > stage.max_rms_delta_db:
                        audio_passed = False
                        fail_reasons.append(
                            f"RMSå·®å€¼è¶…é™: {rms_delta:.2f} dB > {stage.max_rms_delta_db} dB"
                        )

                if audio_passed:
                    print(f"    âœ… éŸ³é¢‘è´¨é‡éªŒæ”¶é€šè¿‡")

                    if train_dataset is not None:
                        prev_modes = (
                            encoder.training,
                            decoder.training,
                            wave_head.training
                        )
                        encoder.eval()
                        decoder.eval()
                        wave_head.eval()
                        try:
                            stage_models = {stage.name: (encoder, decoder, wave_head)}
                            summary_stage = integrate_audio_validation(
                                dataset=train_dataset,
                                trained_models=stage_models,
                                device=device,
                                output_dir=str(checkpoint_dir)
                            )
                            print(f"    ğŸ¨ é˜¶æ®µ {stage.name} éŸ³é¢‘å¯è§†åŒ–å·²ç”Ÿæˆ (audio_validation/{stage.name})")
                            if summary_stage:
                                print(summary_stage)
                        except Exception as viz_e:
                            print(f"    âš ï¸ é˜¶æ®µ {stage.name} éŸ³é¢‘å¯è§†åŒ–å¤±è´¥: {viz_e}")
                        finally:
                            encoder.train(prev_modes[0])
                            decoder.train(prev_modes[1])
                            wave_head.train(prev_modes[2])
                else:
                    passed = False
                    print(f"    âŒ éŸ³é¢‘è´¨é‡éªŒæ”¶å¤±è´¥")

                # å¯¼å‡ºéªŒè¯éŸ³é¢‘å’Œå¯è§†åŒ– (æ— è®ºæ˜¯å¦é€šè¿‡éƒ½å¯¼å‡ºï¼Œç”¨äºè¯Šæ–­)
                export_validation_audio(
                    stage_name=stage.name,
                    y_hat_feats=y_hat_eval,
                    y_orig_feats=y_eval,
                    wave_head=wave_head,
                    original_audio=original_audio,
                    output_dir=checkpoint_dir.parent,
                    csi_dict=csi_dict
                )

            else:
                print(f"    âš ï¸ éŸ³é¢‘è´¨é‡è¯„ä¼°å¤±è´¥: {audio_quality['error']}")
                # ä¸å› è¯„ä¼°å¤±è´¥è€Œé˜»æ­¢é˜¶æ®µé€šè¿‡ï¼Œä½†è®°å½•è­¦å‘Š
                fail_reasons.append("éŸ³é¢‘è´¨é‡è¯„ä¼°å¤±è´¥")

            # æ¢å¤è®­ç»ƒæ¨¡å¼
            encoder.train(prev_encoder_mode)
            decoder.train(prev_decoder_mode)
            wave_head.train(prev_wave_mode)

        except Exception as e:
            print(f"    âš ï¸ éŸ³é¢‘è´¨é‡è¯„ä¼°å¼‚å¸¸: {e}")
            traceback.print_exc()
            audio_quality = {'error': str(e)}

    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_path = checkpoint_dir / f"stage_{current_stage_index}_{stage.name}.pth"
    checkpoint = {
        'stage_index': current_stage_index,
        'stage_name': stage.name,
        'encoder_state_dict': encoder.state_dict(),
        'decoder_state_dict': decoder.state_dict(),
        'wave_head_state_dict': wave_head.state_dict(),
        'best_loss': best_loss,
        'convergence_rate': convergence_rate,
        'step': step,
        'passed': passed,
        'audio_quality': audio_quality,  # æ·»åŠ éŸ³é¢‘è´¨é‡ä¿¡æ¯
        'fail_reasons': fail_reasons
    }
    torch.save(checkpoint, checkpoint_path)

    result = {
        'passed': passed,
        'best_loss': best_loss,
        'convergence_rate': convergence_rate,
        'final_step': step,
        'fail_reasons': fail_reasons,
        'checkpoint_path': checkpoint_path,
        'audio_quality': audio_quality
    }

    status = "âœ… é€šè¿‡" if passed else "âŒ æœªé€šè¿‡"
    print(f"\nğŸ é˜¶æ®µ {stage.name} å®Œæˆ: {status}")
    print(f"   æœ€ä½³æŸå¤±: {best_loss:.6f}")
    print(f"   æ”¶æ•›ç‡: {convergence_rate:.6f}")
    if audio_quality and 'error' not in audio_quality:
        print(
            f"   éŸ³é¢‘æŒ‡æ ‡: PESQ-like={audio_quality['pesq_like']:.3f}, "
            f"SNR={audio_quality['snr_db']:.2f} dB, "
            f"MelCos={audio_quality['mel_cos']:.3f}, "
            f"RMS={audio_quality['pred_rms_db']:.1f} dB"
        )
    if fail_reasons:
        print(f"   å¤±è´¥åŸå› : {'; '.join(fail_reasons)}")

    # æ¢å¤latenté‡åŒ–å¼€å…³
    try:
        if stage.name == 'wave_preheat' and _orig_quant_flag is not None:
            encoder.quantize_latent = _orig_quant_flag
            print("ğŸ” å·²æ¢å¤latenté‡åŒ–ä¸ºé˜¶æ®µå‰è®¾ç½®")
    except Exception:
        pass

    return result


def create_progressive_stages() -> List[ProgressiveStage]:
    """åˆ›å»ºæ¸è¿›å¼è®­ç»ƒé˜¶æ®µåºåˆ—"""
    return [
        ProgressiveStage(
            name="clean_baseline",
            description="æ¸…æ´åŸºçº¿ - æ— ä¿¡é“å¹²æ‰°ä¸‹çš„ç‰¹å¾é‡å»º",
            epochs=1.0,
            use_film=False,
            use_moe=False,
            use_quantization=False,
            apply_channel=False,
            channel_type="clean",
            layered_loss=False,
            learning_rate=5e-5,
            lambda_rate=1e-5,
            min_convergence_rate=0.5,
            max_final_loss=0.2,
            early_stop_loss=0.01
        ),

        ProgressiveStage(
            name="wave_preheat",
            description="æ³¢å½¢å¤´é¢„çƒ­ - è®­ç»ƒæ³¢å½¢è§£ç å™¨",
            epochs=2.0,
            use_film=False,
            use_moe=False,
            use_quantization=False,
            apply_channel=False,
            channel_type="clean",
            layered_loss=True,
            enable_audio_quality=True,
            learning_rate=1e-4,
            lambda_rate=0.0,
            min_convergence_rate=-20.0,
            max_final_loss=3.0,  # æ”¾å®½ä»1.5åˆ°3.0
            early_stop_loss=0.0,
            min_snr_db=-5.0,  # å¤§å¹…æ”¾å®½SNRé—¨æ§›ï¼ˆå¦‚æœå¯ç”¨çš„è¯ï¼‰
            min_mel_cos=0.60,  # æ”¾å®½Melä½™å¼¦ç›¸ä¼¼åº¦é—¨æ§›
            max_mel_l2=0.40,   # æ”¾å®½Mel L2è¯¯å·®é—¨æ§›
            max_spectral_distortion=1.20,  # æ”¾å®½è°±å¤±çœŸé—¨æ§›
            max_rms_delta_db=8.0,  # æ”¾å®½RMSå·®å€¼é—¨æ§›
            wave_start_step=0,
            wave_full_start_step=1200,
            wave_lowpass_weight=0.9,
            wave_full_weight=1.0,
            wave_lowpass_schedule=[(0,1.2),(800,0.8),(1200,0.6)],
            wave_full_schedule=[(0,0.2),(600,0.5),(1200,0.8),(1800,1.0)],
            train_wave_head_only=True,  # ğŸ”§ FARGANé¢„çƒ­é˜¶æ®µå†»ç»“ç¼–è§£ç å™¨ï¼Œåªè®­ç»ƒæ³¢å½¢åˆæˆ
            preheat_mix_start_step=0,
            preheat_mix_end_step=15000,  # ğŸ”§ è¿›ä¸€æ­¥å»¶ç¼“teacher-forcingè¡°å‡ï¼Œç»™F0æ›´å¤šç¨³å®šæ—¶é—´
            preheat_chunk_frames=128,
        ),

        ProgressiveStage(
            name="channel_adapt",
            description="ä¿¡é“é€‚åº”ï¼ˆå«FiLMè°ƒåº¦ï¼‰",
            epochs=3.0,
            use_film=True,
            use_moe=True,
            use_quantization=False,
            apply_channel=True,
            channel_type="awgn",
            layered_loss=True,
            learning_rate=3e-4,
            lambda_rate=0.1,
            min_convergence_rate=1.0,
            max_final_loss=1.0,  # é€‚ä¸­é—¨æ§›
            early_stop_loss=0.01,
            enable_audio_quality=True,  # å¼€å§‹å¯ç”¨éŸ³é¢‘è´¨é‡é—¨æ§›
            min_snr_db=3.0,  # ä»åˆç†çš„SNRå¼€å§‹
            min_mel_cos=0.75,  # é€‚ä¸­çš„Melç›¸ä¼¼åº¦è¦æ±‚
            max_mel_l2=0.30,   # é€‚ä¸­çš„Melè¯¯å·®
            max_spectral_distortion=0.90,  # é€‚ä¸­çš„è°±å¤±çœŸ
            max_rms_delta_db=5.0,  # é€‚ä¸­çš„RMSè¦æ±‚
            use_advanced_scheduler=True,
            film_warmup_steps=500,
            film_start_ratio=0.1,
            film_beta_scale_start=0.1
        ),

        ProgressiveStage(
            name="full_optimization",
            description="å®Œæ•´ä¼˜åŒ– - ç«¯åˆ°ç«¯è®­ç»ƒ",
            epochs=5.0,
            use_film=True,
            use_moe=True,
            use_quantization=True,
            apply_channel=True,
            channel_type="fading",
            layered_loss=True,
            learning_rate=2e-4,
            lambda_rate=0.2,
            lambda_balance=0.1,
            lambda_cons=0.05,
            min_convergence_rate=0.5,
            max_final_loss=1.0,
            early_stop_loss=0.01,
            enable_audio_quality=True,
            min_snr_db=10.0,
            min_mel_cos=0.90,
            max_mel_l2=0.12,
            max_spectral_distortion=0.60,
            max_rms_delta_db=2.5,
            target_kbps=1.2,
            max_kbps_p90=1.6
        )
    ]


def main():
    parser = argparse.ArgumentParser(description='AETHERæ¸è¿›å¼è®­ç»ƒ(ç²¾ç®€ç‰ˆ)')

    # æ•°æ®å‚æ•°
    parser.add_argument('--features', type=str, required=True, help='ç‰¹å¾æ–‡ä»¶è·¯å¾„(.f32)')
    parser.add_argument('--pcm', type=str, required=True, help='PCMéŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--seq-len', type=int, default=400, help='åºåˆ—é•¿åº¦(å¸§)')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--num-workers', type=int, default=4, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--limit-seqs', type=int, default=None, help='é™åˆ¶åºåˆ—æ•°é‡')
    parser.add_argument('--feature-dims', type=int, default=36, help='ç‰¹å¾ç»´åº¦ (36 for FARGAN, 48 for AETHER)')
    parser.add_argument('--feature-spec-type', type=str, default='fargan', choices=['fargan', 'aether'], help='ç‰¹å¾è§„èŒƒç±»å‹')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--output-dir', type=str, default='checkpoints', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡(auto/cpu/cuda)')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--wave-head-type', type=str, default='conv', choices=['conv', 'exciter'], help='æ³¢å½¢å¤´ç±»å‹')
    parser.add_argument('--checkpoint-every', type=int, default=500, help='æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”(æ­¥æ•°)')
    # é¢„çƒ­å¯è°ƒå‚æ•°ï¼ˆå¯è¦†ç›–é˜¶æ®µé»˜è®¤å€¼ï¼‰
    parser.add_argument('--preheat-mix-end', type=int, default=None, help='è¦†ç›–wave_preheatçš„teacher-forcingç»“æŸæ­¥æ•°')
    parser.add_argument('--preheat-chunk-frames', type=int, default=None, help='è¦†ç›–wave_preheatçš„chunkå¸§æ•°')

    # é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒå‚æ•°
    parser.add_argument('--stage2-fargan-only', action='store_true', help='é˜¶æ®µäºŒæ¨¡å¼ï¼šå†»ç»“ç¼–è§£ç å™¨ï¼Œåªè®­ç»ƒç‹¬ç«‹FARGANæ³¢å½¢å¤´')
    parser.add_argument('--stage1-checkpoint', type=str, default=None, help='é˜¶æ®µä¸€Aetheræ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--fargan-learning-rate', type=float, default=1e-4, help='FARGANè®­ç»ƒå­¦ä¹ ç‡')
    parser.add_argument('--fargan-lr-decay', type=float, default=2e-5, help='FARGANå­¦ä¹ ç‡è¡°å‡ç‡')
    parser.add_argument('--fargan-original-epochs', type=int, default=0, help='FARGANåŸç‰ˆæŸå¤±è®­ç»ƒè½®æ•°')
    parser.add_argument('--fargan-ramp-epochs', type=int, default=10, help='FARGANæŸå¤±æ··åˆæ¸å˜è½®æ•°')

    # é˜¶æ®µæ§åˆ¶
    parser.add_argument('--stages', type=str, default='all', help='è®­ç»ƒé˜¶æ®µ(all/stage1,stage2ç­‰)')
    parser.add_argument('--start-stage', type=int, default=1, help='å¼€å§‹é˜¶æ®µ(1-based)')
    parser.add_argument('--end-stage', type=int, default=None, help='ç»“æŸé˜¶æ®µ(1-based,åŒ…å«)')
    parser.add_argument('--skip-passed', action='store_true', help='è·³è¿‡å·²é€šè¿‡çš„é˜¶æ®µ')

    # è§£ç å™¨ç±»å‹
    parser.add_argument('--decoder-type', type=str, default='aether', choices=['aether', 'aether_fargan'],
                        help='è§£ç å™¨ç±»å‹: aether(ä»…ç‰¹å¾é‡å»º), aether_fargan(ç‰¹å¾+FARGANæ³¢å½¢åˆæˆ)')
    parser.add_argument('--disable-f0-loss', action='store_true', help='ç¦ç”¨æ‰€æœ‰ä¸F0ç›¸å…³çš„æŸå¤±ä¸å¯¹é½')

    args = parser.parse_args()

    # è®¾å¤‡è®¾ç½®
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")

    # å½“ä½¿ç”¨ FARGAN è§£ç è·¯å¾„æ—¶ï¼Œé»˜è®¤ç¦ç”¨ F0 æŸå¤±ï¼ˆæŒ‰ä½ çš„éœ€æ±‚ï¼‰
    if args.decoder_type == 'aether_fargan':
        args.disable_f0_loss = True

    # é¢„åˆå§‹åŒ–Melç¼“å­˜ï¼ˆä¾¿äºåç»­è¯„ä¼°é˜¶æ®µç›´æ¥å¤ç”¨ï¼‰
    _get_mel_transform(device)

    # éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    if device.type == 'cuda':
        torch.cuda.manual_seed(args.seed)
    torch.set_num_threads(1)
    if hasattr(torch, "set_num_interop_threads"):
        torch.set_num_interop_threads(1)
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")

    # è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ•°æ®åŠ è½½å™¨ - è‡ªåŠ¨ä»å‚æ•°è·¯å¾„æ¨æ–­æ•°æ®ç›®å½•
    # ä»featuresè·¯å¾„æ¨æ–­æ•°æ®ç›®å½• (/path/to/data_cn/lmr_export/features_48_complete.f32 -> /path/to/data_cn)
    features_path = Path(args.features)
    if "lmr_export" in features_path.parts:
        # ä» lmr_export ç›®å½•å‘ä¸Šæ‰¾åˆ° data_cn
        data_dir = None
        for i, part in enumerate(features_path.parts):
            if part == "lmr_export" and i > 0:
                data_dir = Path(*features_path.parts[:i])
                break
        if data_dir is None:
            data_dir = features_path.parent.parent  # fallback
    else:
        data_dir = Path("/home/bluestar/FARGAN/opus/data_cn")  # default fallback

    print(f"ğŸ—‚ï¸ æ¨æ–­æ•°æ®ç›®å½•: {data_dir}")
    # ğŸ”¥ å…³é”®CPUä¼˜åŒ–ï¼šé™ä½DataLoaderå‹åŠ›é˜²æ­¢å¡æ­»
    loader_workers = max(1, min(4, args.num_workers))
    train_loader, dataset = create_aether_data_loader(
        data_dir=str(data_dir),
        sequence_length=args.seq_len,
        batch_size=args.batch_size,
        max_samples=args.limit_seqs,
        num_workers=loader_workers,
        energy_selection=True,
        test_mode=False,
        feature_spec_type=args.feature_spec_type,
        features_file=args.features,
        audio_file=args.pcm
    )
    print(f"ğŸ—‚ï¸ æ•°æ®åŠ è½½å™¨å°±ç»ª: {len(train_loader)} batches")

    # æ¨¡å‹åˆå§‹åŒ–
    cfg = TrainConfig()

    # é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒæ¨¡å¼
    if args.stage2_fargan_only:
        print("=== STAGE2 FARGAN-ONLY TRAINING MODE ===")

        # éªŒè¯é˜¶æ®µä¸€checkpoint
        if not args.stage1_checkpoint:
            raise ValueError("--stage1-checkpoint is required for --stage2-fargan-only mode")

        if not os.path.exists(args.stage1_checkpoint):
            raise FileNotFoundError(f"Stage1 checkpoint not found: {args.stage1_checkpoint}")

        # åŠ è½½å†»ç»“çš„Aetherç¼–è§£ç å™¨
        frozen_encoder, frozen_decoder = load_frozen_aether_models(
            args.stage1_checkpoint, device, args.feature_dims
        )

        # åˆ›å»ºç‹¬ç«‹çš„FARGANæ³¢å½¢å¤´
        fargan_wavehead = create_independent_fargan_wavehead(device)

        # è®¾ç½®æ¨¡å‹å˜é‡ (ä¿æŒå…¼å®¹æ€§)
        encoder = frozen_encoder
        decoder = frozen_decoder
        wave_head = fargan_wavehead

        print(f"Stage2 FARGAN-only setup completed")
        print(f"  Total FARGAN parameters: {sum(p.numel() for p in fargan_wavehead.parameters()):,}")

    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼
        encoder = AETHEREncoder(
            d_in=args.feature_dims, d_model=cfg.d_model, dz=cfg.dz,
            gla_depth=cfg.gla_depth, n_heads=cfg.n_heads, d_csi=16,  # ç¼–ç å™¨ä½¿ç”¨16ç»´å®¹çº³acoustic_priors
            dropout=cfg.dropout, use_film=True, use_moe=False,
            n_experts=cfg.n_experts, top_k=cfg.top_k,
            latent_bits=cfg.latent_bits, frame_rate_hz=cfg.frame_rate_hz,
            quantize_latent=True, feature_spec_type=args.feature_spec_type
        ).to(device)

        # æ ¹æ®decoder_typeé€‰æ‹©è§£ç å™¨ (ä»…æ­£å¸¸è®­ç»ƒæ¨¡å¼)
        if args.decoder_type == 'aether_fargan':
            from models.aether_fargan_decoder import AETHERFARGANDecoder
            decoder = AETHERFARGANDecoder(
                dz=cfg.dz, d_out=args.feature_dims, d_hidden=cfg.d_model,
                d_csi=cfg.d_csi, decoder_heads=cfg.n_heads,
                enable_synth=True, feature_spec_type=args.feature_spec_type  # å¯ç”¨FARGANæ³¢å½¢åˆæˆ
            ).to(device)
            print(f"âœ… ä½¿ç”¨ AETHERFARGANDecoder: ç‰¹å¾é‡å»º + FARGANæ³¢å½¢åˆæˆ")
        else:
            decoder = AETHERDecoder(
                dz=cfg.dz, d_out=args.feature_dims, d_hidden=cfg.d_model,
                d_csi=cfg.d_csi, decoder_heads=cfg.n_heads,
                enable_synth=True, feature_spec_type=args.feature_spec_type  # å¯ç”¨å†…åµŒåˆæˆå™¨
            ).to(device)
            print(f"âœ… ä½¿ç”¨ AETHERDecoder: ä»…ç‰¹å¾é‡å»º")

    # æ ¹æ®è®­ç»ƒæ¨¡å¼å’Œè§£ç å™¨ç±»å‹é€‰æ‹©åˆé€‚çš„wave_headåŒ…è£…å™¨
    if args.stage2_fargan_only:
        # é˜¶æ®µäºŒæ¨¡å¼ï¼šwave_headå·²ç»åœ¨ä¸Šé¢è®¾ç½®ä¸ºç‹¬ç«‹çš„FARGANæ³¢å½¢å¤´
        print(f"é˜¶æ®µäºŒç‹¬ç«‹FARGANæ³¢å½¢å¤´å‚æ•°: {sum(p.numel() for p in wave_head.parameters()):,}")
        print("é˜¶æ®µäºŒæ¨¡å¼ï¼šä½¿ç”¨ç‹¬ç«‹FARGANæ³¢å½¢å¤´ï¼ˆä¸ä¾èµ–ç¼–è§£ç å™¨ï¼‰")
    elif args.decoder_type == "aether_fargan":
        # FARGANç‰ˆæœ¬çš„wave_headåŒ…è£…å™¨
        class FarganWaveHead(nn.Module):
            """FARGANè§£ç å™¨çš„æ³¢å½¢å¤´åŒ…è£…å™¨"""
            def __init__(self, decoder):
                super().__init__()
                self.decoder = decoder
                self._is_exciter = False

            def forward(self, decoded_feats: torch.Tensor, target_len: int = None, csi_dict=None):
                """
                Args:
                    decoded_feats: [B, T, 36] FARGANç‰¹å¾ (å·²ç»ä»decoderè¾“å‡º)
                    target_len: ç›®æ ‡æ³¢å½¢é•¿åº¦
                    csi_dict: å¯é€‰å­—å…¸ï¼›å½“åŒ…å« 'fargan_pre' æ—¶ï¼Œå°†ä½œä¸ºæ•™å¸ˆå¼ºåˆ¶çš„å‰åºéŸ³é¢‘æ®µä¼ å…¥
                Returns:
                    waveform: [B, T_audio] åˆæˆæ³¢å½¢
                """
                # ç›´æ¥ä½¿ç”¨FARGANåˆæˆå™¨ï¼›è‹¥æä¾› fargan_pre åˆ™è¿›è¡Œæ•™å¸ˆå¼ºåˆ¶
                period = self.decoder._estimate_period(decoded_feats)
                fargan_pre = None
                if isinstance(csi_dict, dict):
                    fargan_pre = csi_dict.get('fargan_pre', None)
                audio = self.decoder._generate_waveform(decoded_feats, period, target_len, fargan_pre)
                # å°† pre æ®µæ‹¼æ¥å›è¾“å‡ºï¼Œä¿æŒä¸ç‹¬ç«‹FARGANè®­ç»ƒä¸€è‡´
                if fargan_pre is not None:
                    pre_seg = fargan_pre
                    if pre_seg.dim() == 3:
                        pre_seg = pre_seg.squeeze(1)
                    if audio.dim() == 3:
                        audio = torch.cat([pre_seg.unsqueeze(1), audio], dim=-1)
                    else:
                        audio = torch.cat([pre_seg, audio], dim=-1)
                return audio.squeeze(1) if audio.dim() == 3 else audio  # [B, T_audio]

        wave_head = FarganWaveHead(decoder).to(device)
        print("Using AETHER-FARGAN end-to-end synthesis (features->waveform)")
    else:
        # AETHERç‰ˆæœ¬ä½¿ç”¨åŸæœ‰çš„EmbeddedSynthHead
        from models.maybe_useless.decoder_synth_head import EmbeddedSynthHead
        wave_head = EmbeddedSynthHead(decoder).to(device)
        setattr(wave_head, '_is_exciter', False)
        print("Using Decoder-Embedded OLA synthesis (no separate wave head)")

    wave_loss = MRSTFTLoss(
        fft_sizes=(256, 512, 1024, 2048),
        hop_sizes=(80, 160, 320, 640),
        win_sizes=(200, 400, 800, 1600),
        alpha_l1=2.0,
        alpha_mag=1.0,
        alpha_sc=0.08
    ).to(device)

    print(f"ğŸ—ï¸ æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in encoder.parameters()):,}")
    print(f"   è§£ç å™¨å‚æ•°: {sum(p.numel() for p in decoder.parameters()):,}")
    print(f"   æ³¢å½¢å¤´å‚æ•°: {sum(p.numel() for p in wave_head.parameters()):,}")

    # åˆ›å»ºè®­ç»ƒé˜¶æ®µ
    stages = create_progressive_stages()

    # é˜¶æ®µç­›é€‰
    if args.stages != 'all':
        # åˆ›å»ºé˜¶æ®µåç§°åˆ°ç´¢å¼•çš„æ˜ å°„
        stage_name_to_index = {stage.name: i for i, stage in enumerate(stages)}

        stage_indices = []
        for stage_spec in args.stages.split(','):
            stage_spec = stage_spec.strip()
            try:
                # å°è¯•è§£æä¸ºæ•°å­—ç´¢å¼•ï¼ˆ1-basedï¼‰
                idx = int(stage_spec) - 1
                stage_indices.append(idx)
            except ValueError:
                # è§£æä¸ºé˜¶æ®µåç§°
                if stage_spec in stage_name_to_index:
                    stage_indices.append(stage_name_to_index[stage_spec])
                else:
                    available_stages = ', '.join(stage_name_to_index.keys())
                    raise ValueError(f"æœªçŸ¥é˜¶æ®µåç§° '{stage_spec}'ï¼Œå¯ç”¨é˜¶æ®µ: {available_stages}")

        stages = [stages[i] for i in stage_indices if 0 <= i < len(stages)]
    elif args.start_stage is not None or args.end_stage is not None:
        # ä½¿ç”¨start-stageå’Œend-stage
        start_idx = (args.start_stage - 1) if args.start_stage else 0
        end_idx = args.end_stage if args.end_stage else len(stages)
        start_idx = max(0, min(start_idx, len(stages) - 1))
        end_idx = max(1, min(end_idx, len(stages)))
        stages = stages[start_idx:end_idx]

    # è¦†ç›–é¢„çƒ­é˜¶æ®µå¯è°ƒå‚æ•°
    if args.preheat_mix_end is not None:
        for st in stages:
            if st.name == 'wave_preheat':
                st.preheat_mix_end_step = int(args.preheat_mix_end)
                break
    if args.preheat_chunk_frames is not None:
        for st in stages:
            if st.name == 'wave_preheat':
                st.preheat_chunk_frames = int(args.preheat_chunk_frames)
                break

    print(f"ğŸ“‹ è®­ç»ƒé˜¶æ®µ: {[s.name for s in stages]}")

    # === Auto-resume: if starting from wave_preheat, try to load clean_baseline checkpoint ===
    try:
        if args.resume is None and len(stages) > 0 and stages[0].name == 'wave_preheat':
            # Search for any clean_baseline checkpoint saved by a prior run
            cand = []
            for p in output_dir.glob('stage_*_clean_baseline.pth'):
                try:
                    cand.append((p.stat().st_mtime, p))
                except Exception:
                    pass
            if cand:
                cand.sort(reverse=True)
                ckpt_path = cand[0][1]
                ckpt = torch.load(ckpt_path, map_location='cpu')
                if 'encoder_state_dict' in ckpt:
                    encoder.load_state_dict(ckpt['encoder_state_dict'], strict=False)
                if 'decoder_state_dict' in ckpt:
                    decoder.load_state_dict(ckpt['decoder_state_dict'], strict=False)
                if 'wave_head_state_dict' in ckpt:
                    try:
                        wave_head.load_state_dict(ckpt['wave_head_state_dict'], strict=False)
                    except Exception:
                        pass
                print(f"ğŸ” å·²ä»ä¸Šæ¬¡ clean_baseline æ£€æŸ¥ç‚¹æ¢å¤: {ckpt_path.name}")
            else:
                print("â„¹ï¸ æœªæ‰¾åˆ° clean_baseline æ£€æŸ¥ç‚¹ï¼›å»ºè®®å…ˆè·‘ Stage1 æˆ–ä½¿ç”¨ --resume")
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨æ¢å¤å¤±è´¥: {e}")

    # === é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒåˆ†æ”¯ ===
    if args.stage2_fargan_only:
        print("ğŸ¯ è¿›å…¥é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒæ¨¡å¼")

        # éªŒè¯å¿…è¦å‚æ•°
        if args.stage1_checkpoint is None:
            raise ValueError("é˜¶æ®µäºŒæ¨¡å¼éœ€è¦æŒ‡å®š --stage1-checkpoint å‚æ•°")

        # å¼€å§‹é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒ
        result = train_stage2_fargan_only(
            frozen_encoder=encoder,
            frozen_decoder=decoder,
            fargan_wavehead=wave_head,
            train_loader=train_loader,
            device=device,
            args=args,
            checkpoint_dir=output_dir
        )

        print(f"ğŸ‰ é˜¶æ®µäºŒç‹¬ç«‹FARGANè®­ç»ƒå®Œæˆ: {result}")
        return

    # æ¸è¿›å¼è®­ç»ƒ
    results = []
    for i, stage in enumerate(stages):
        result = train_progressive_stage(
            stage=stage,
            encoder=encoder,
            decoder=decoder,
            wave_head=wave_head,
            wave_loss=wave_loss,
            train_loader=train_loader,
            train_dataset=dataset,
            device=device,
            checkpoint_dir=output_dir,
            current_stage_index=i,
            total_stages=len(stages),
            checkpoint_every=args.checkpoint_every,
            feature_spec_type=args.feature_spec_type,
            decoder_type=args.decoder_type,
            disable_f0_loss=args.disable_f0_loss
        )
        results.append(result)

        # å¦‚æœé˜¶æ®µå¤±è´¥ä¸”ä¸æ˜¯æœ€åä¸€ä¸ªé˜¶æ®µï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
        if not result['passed'] and i < len(stages) - 1:
            print(f"\nâš ï¸ é˜¶æ®µ {stage.name} æœªé€šè¿‡éªŒæ”¶æ¡ä»¶")
            if not args.skip_passed:
                response = input("æ˜¯å¦ç»§ç»­ä¸‹ä¸€é˜¶æ®µ? (y/n): ")
                if response.lower() != 'y':
                    print("ğŸ›‘ è®­ç»ƒæå‰ç»ˆæ­¢")
                    break

    # è®­ç»ƒæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ‰ æ¸è¿›å¼è®­ç»ƒå®Œæˆ")
    print(f"{'='*60}")

    passed_stages = sum(1 for r in results if r['passed'])
    print(f"âœ… é€šè¿‡é˜¶æ®µ: {passed_stages}/{len(results)}")

    for i, (stage, result) in enumerate(zip(stages, results)):
        status = "âœ…" if result['passed'] else "âŒ"
        print(f"  {status} é˜¶æ®µ {i+1}: {stage.name} (loss: {result['best_loss']:.6f})")

    # æœ€ç»ˆéŸ³é¢‘éªŒè¯ä¸å¯è§†åŒ–
    if dataset is not None:
        try:
            encoder.eval()
            decoder.eval()
            wave_head.eval()

            final_models = {"final": (encoder, decoder, wave_head)}
            summary = integrate_audio_validation(
                dataset=dataset,
                trained_models=final_models,
                device=device,
                output_dir=str(output_dir)
            )
            print("ğŸ§ æœ€ç»ˆéŸ³é¢‘éªŒè¯å·²ç”Ÿæˆ (audio_validation/final)")
            if summary:
                print(summary)
        except Exception as e:
            print(f"âš ï¸ æœ€ç»ˆéŸ³é¢‘éªŒè¯å¤±è´¥: {e}")
        finally:
            encoder.train()
            decoder.train()
            wave_head.train()

    # ä¿å­˜æœ€ç»ˆçŠ¶æ€
    final_checkpoint = output_dir / "final_model.pth"
    torch.save({
        'encoder_state_dict': encoder.state_dict(),
        'decoder_state_dict': decoder.state_dict(),
        'wave_head_state_dict': wave_head.state_dict(),
        'training_results': results,
        'config': cfg.__dict__
    }, final_checkpoint)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_checkpoint}")


if __name__ == "__main__":
    main()
