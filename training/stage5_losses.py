#!/usr/bin/env python3
"""
Stage5 æŸå¤±å‡½æ•°æ¨¡å—

åŒ…å«:
1. å¤šç›®æ ‡ç‡å¤±çœŸæŸå¤±
2. è¯­ä¹‰ä¿æŒæŸå¤±
3. æ—¶é—´ä¸€è‡´æ€§æŸå¤±
4. åŠ¨æ€æƒé‡è°ƒåº¦
5. ç»¼åˆæŸå¤±è®¡ç®—
"""

from __future__ import annotations
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import math

# Constants for bits conversion
LOG2E = 1.0 / math.log(2.0)

# å¯¼å…¥ç°æœ‰çš„æ³¢å½¢æŸå¤± - ä¿®å¤P3: ç¡®ä¿è·¯å¾„ä¸€è‡´æ€§ï¼Œå¤±è´¥æ—¶æ˜¾å¼æŠ¥é”™
try:
    from .pipeline.wave_loss import fargan_wave_losses
    WAVE_LOSS_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Failed to import fargan_wave_losses from pipeline.wave_loss: {e}")
    try:
        # å°è¯•ä»ä¸Šçº§ç›®å½•å¯¼å…¥
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'training'))
        from pipeline.wave_loss import fargan_wave_losses
        WAVE_LOSS_AVAILABLE = True
        print("Successfully imported fargan_wave_losses from fallback path")
    except ImportError as e2:
        print(f"Critical: Wave loss import failed completely: {e2}")
        WAVE_LOSS_AVAILABLE = False
        # æŠ›å‡ºé”™è¯¯è€Œä¸æ˜¯æ‚„ç„¶é™çº§
        def fargan_wave_losses(pred_audio, target_audio, period, device):
            """é”™è¯¯ï¼šæ³¢å½¢æŸå¤±ä¸å¯ç”¨"""
            raise RuntimeError(f"Wave loss not available. Import errors: {e}, {e2}")

def _ensure_wave_loss_available():
    """ç¡®ä¿æ³¢å½¢æŸå¤±å¯ç”¨ï¼Œå¦åˆ™æŠ›å‡ºæ˜ç¡®é”™è¯¯"""
    if not WAVE_LOSS_AVAILABLE:
        raise RuntimeError("Wave loss is required but not available. Check import paths.")


# === [SEM] InfoNCE (NT-Xent) ===============================================
def info_nce_global(z1_bct, z2_bct, temperature=0.2):
    """Improved InfoNCE loss with better temperature and pooling"""
    B, D, T = z1_bct.shape

    # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ± åŒ–ç­–ç•¥ï¼šåŠ æƒå¹³å‡è€Œä¸æ˜¯ç®€å•å‡å€¼
    # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„"é‡è¦æ€§"æƒé‡
    z1_importance = torch.softmax(z1_bct.norm(dim=1), dim=-1)  # [B, T]
    z2_importance = torch.softmax(z2_bct.norm(dim=1), dim=-1)  # [B, T]

    # åŠ æƒæ± åŒ–
    z1 = (z1_bct * z1_importance.unsqueeze(1)).sum(dim=-1)  # [B, D]
    z2 = (z2_bct * z2_importance.unsqueeze(1)).sum(dim=-1)  # [B, D]

    # L2å½’ä¸€åŒ–
    z1 = F.normalize(z1, dim=-1)
    z2 = F.normalize(z2, dim=-1)

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    logits = (z1 @ z2.t()) / temperature      # [B, B]
    labels = torch.arange(B, device=z1.device)

    return F.cross_entropy(logits, labels)

def nt_xent_loss(student_bct: torch.Tensor, teacher_bct: torch.Tensor, temperature: float = 0.1,
                 current_step: int = 0, adaptive_temp: bool = True) -> torch.Tensor:
    # student/teacher: [B, D, T] å·²ç» L2-normalized
    B, D, T = student_bct.shape

    # ä¿®å¤3&9: æ¸©åº¦schedule + å‡å°‘ä¼ªè´Ÿæ ·æœ¬ç­–ç•¥

    # æ¸©åº¦schedule: cosine annealing from 0.25 -> 0.07 over 6000 steps
    if adaptive_temp and current_step > 0:
        max_steps = 6000
        progress = min(current_step / max_steps, 1.0)
        temp_start, temp_end = 0.25, 0.07
        # Cosine annealing æä¾›å¹³æ»‘è¿‡æ¸¡
        temperature = temp_end + 0.5 * (temp_start - temp_end) * (1 + math.cos(math.pi * progress))

    # 1. è®¡ç®—æ¯å¸§çš„èƒ½é‡ï¼ˆL2 normï¼‰
    s_energy = torch.norm(student_bct, dim=1)  # [B, T]
    t_energy = torch.norm(teacher_bct, dim=1)  # [B, T]

    # 2. è®¾å®šèƒ½é‡é˜ˆå€¼ï¼ˆå–25%åˆ†ä½æ•°ï¼Œè¿‡æ»¤æ‰æœ€ä½25%èƒ½é‡å¸§ï¼‰
    energy_threshold = torch.quantile(s_energy.flatten(), 0.25)  # 25%åˆ†ä½æ•°ï¼Œä¿ç•™ä¸­é«˜èƒ½é‡å¸§

    # 3. åˆ›å»ºæ©ç ï¼Œä¿ç•™ä¸­é«˜èƒ½é‡å¸§
    valid_mask = (s_energy > energy_threshold) & (t_energy > energy_threshold)  # [B, T]

    # 4. æ¸©åº¦é€‚åº”ké€‰æ‹©ï¼šæ—©æœŸç”¨å°kï¼Œé¿å…è¿‡éš¾ï¼›ä½æ¸©åº¦åç”¨å¤§kå¢åŠ è´Ÿæ ·æœ¬
    if temperature > 0.15:  # æ—©æœŸé«˜æ¸©é˜¶æ®µ
        k = max(T // 5, 6)   # ä¿®å¤3: æ›´å°kï¼Œä¸¥æ ¼é¿å…ä¼ªè´Ÿæ ·æœ¬
    else:  # ä½æ¸©é˜¶æ®µ
        k = max(T // 3, 12)  # ä¸­ç­‰kï¼Œå¹³è¡¡è´Ÿæ ·æœ¬æ•°é‡å’Œä¼ªè´Ÿæ ·æœ¬é£é™©
    s_pooled_list = []
    t_pooled_list = []
    batch_indicators = []  # è®°å½•æ¯ä¸ªæ ·æœ¬æ¥è‡ªå“ªä¸ªbatchï¼Œç”¨äºåŒè¯­å¥æ©ç 

    for b in range(B):
        if valid_mask[b].sum() >= k:
            # åœ¨æœ‰æ•ˆå¸§ä¸­é€‰Top-kï¼Œä¿®å¤ï¼šå–studentå’Œteacherèƒ½é‡çš„å¹¶é›†
            valid_indices = torch.where(valid_mask[b])[0]
            s_valid_energy = s_energy[b, valid_indices]
            t_valid_energy = t_energy[b, valid_indices]

            # åˆ†åˆ«å–studentå’Œteacherçš„Top-k/2
            half_k = max(k // 2, 4)  # ç¡®ä¿è‡³å°‘4ä¸ª
            s_top_k = valid_indices[torch.topk(s_valid_energy, min(half_k, len(valid_indices)))[1]]
            t_top_k = valid_indices[torch.topk(t_valid_energy, min(half_k, len(valid_indices)))[1]]

            # åˆå¹¶å¹¶å»é‡ï¼Œç¡®ä¿æ€»æ•°æ¥è¿‘k
            combined_indices = torch.cat([s_top_k, t_top_k])
            top_k_indices = torch.unique(combined_indices)[:k]  # å»é‡å¹¶æˆªæ–­åˆ°kä¸ª
        else:
            # å›é€€åˆ°å…¨å±€Top-kï¼ŒåŒæ ·ä½¿ç”¨å¹¶é›†ç­–ç•¥
            half_k = max(k // 2, 4)
            s_top_k = torch.topk(s_energy[b], half_k)[1]
            t_top_k = torch.topk(t_energy[b], half_k)[1]
            combined_indices = torch.cat([s_top_k, t_top_k])
            top_k_indices = torch.unique(combined_indices)[:k]

        k_actual = len(top_k_indices)
        s_pooled_list.append(student_bct[b, :, top_k_indices].transpose(0, 1))  # [k_actual, D]
        t_pooled_list.append(teacher_bct[b, :, top_k_indices].transpose(0, 1))  # [k_actual, D]
        batch_indicators.extend([b] * k_actual)  # è®°å½•batchç´¢å¼•

    # 5. é‡æ–°ç»„ç»‡ä¸º[total_samples, D]æ ¼å¼
    s = torch.cat(s_pooled_list, dim=0)  # [total_samples, D]
    t = torch.cat(t_pooled_list, dim=0)  # [total_samples, D]
    batch_ids = torch.tensor(batch_indicators, device=s.device)  # [total_samples]

    # 6. è®¡ç®—å¯¹æ¯”æŸå¤± - ç”±äºå»é‡ï¼Œå®é™…æ ·æœ¬æ•°å¯èƒ½ < B*k
    total_samples = s.size(0)
    if total_samples == 0:
        return torch.tensor(0.0, device=student_bct.device, requires_grad=True)

    # ä¿®å¤9: æ¸©åº¦ä¸æœ‰æ•ˆè´Ÿæ ·æœ¬æ•°é‡è€¦åˆè°ƒæ•´
    effective_negatives = total_samples * (total_samples - 1)  # å»æ‰å¯¹è§’çº¿åçš„æœ‰æ•ˆè´Ÿæ ·æœ¬å¯¹æ•°
    if effective_negatives > 1000:  # å¤§batchæ—¶ï¼Œè½»å¾®é™ä½æ¸©åº¦å¢å¼ºå¯¹æ¯”
        batch_scale_factor = min(1.3, math.sqrt(effective_negatives / 1000.0))
        adjusted_temperature = temperature * batch_scale_factor
    else:
        adjusted_temperature = temperature

    logits = (s @ t.t()) / adjusted_temperature   # [total_samples, total_samples]

    # 7. åŒè¯­å¥æ©ç ï¼šå±è”½åŒä¸€batchå†…çš„è´Ÿæ ·æœ¬å¯¹è§’çº¿ä»¥å¤–éƒ¨åˆ†
    same_batch_mask = batch_ids.unsqueeze(0) == batch_ids.unsqueeze(1)  # [total_samples, total_samples]
    eye_mask = torch.eye(total_samples, device=logits.device, dtype=torch.bool)
    # å¯¹åŒä¸€è¯­å¥å†…çš„éå¯¹è§’çº¿ä½ç½®ï¼ˆä¼ªè´Ÿæ ·æœ¬ï¼‰æ–½åŠ å¤§è´Ÿå€¼å±è”½
    logits = logits.masked_fill(same_batch_mask & ~eye_mask, -float('inf'))

    labels = torch.arange(total_samples, device=s.device)
    return torch.nn.functional.cross_entropy(logits, labels)


def compute_temporal_consistency_loss(
    quantized_sequence: torch.Tensor,
    smoothness_weight: float = 0.1,   # æ¢å¤æ­£å¸¸æƒé‡
    max_jump_threshold: float = 2.0   # æ¢å¤åŸå§‹é˜ˆå€¼
) -> torch.Tensor:
    """
    è®¡ç®—æ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿é‡åŒ–ååºåˆ—çš„å¹³æ»‘æ€§

    Args:
        quantized_sequence: [B, D, T] é‡åŒ–åçš„ç‰¹å¾åºåˆ—
        smoothness_weight: å¹³æ»‘æ€§æƒé‡
        max_jump_threshold: æœ€å¤§è·³è·ƒé˜ˆå€¼
    """
    if quantized_sequence.size(-1) < 2:
        return torch.tensor(0.0, device=quantized_sequence.device)

    # å¯¹è¾“å…¥è¿›è¡ŒL2å½’ä¸€åŒ–é¿å…scaleé—®é¢˜
    normalized_seq = F.normalize(quantized_sequence, p=2, dim=1)

    # è®¡ç®—å¸§é—´å·®åˆ†
    diff = normalized_seq[:, :, 1:] - normalized_seq[:, :, :-1]  # [B, D, T-1]

    # L1å¹³æ»‘æŸå¤±ï¼ˆå·²ç»å½’ä¸€åŒ–ï¼Œæ•°å€¼ä¼šæ›´åˆç†ï¼‰
    l1_smooth = F.l1_loss(diff, torch.zeros_like(diff), reduction='none').mean(dim=1)  # [B, T-1]

    # å¤§è·³è·ƒæƒ©ç½šï¼ˆåŸºäºå½’ä¸€åŒ–åçš„å·®å€¼ï¼‰
    jump_magnitude = torch.norm(diff, p=2, dim=1)  # [B, T-1]
    jump_penalty = F.relu(jump_magnitude - max_jump_threshold).pow(2)

    # ç»„åˆæŸå¤±ï¼ˆæ­£å¸¸æƒé‡ï¼‰
    total_loss = smoothness_weight * l1_smooth.mean() + jump_penalty.mean()

    return total_loss

def compute_real_pesq_loss(
    model_outputs: Dict[str, Any],
    targets: Dict[str, Any],
    minimum_pesq_threshold: float = 2.5,
    current_step: int = 0,
    device: torch.device = None
) -> torch.Tensor:
    """
    ä½¿ç”¨çœŸå®PESQè®¡ç®—è´¨é‡æŸå¤±

    Args:
        model_outputs: åŒ…å«synthesized_audioçš„æ¨¡å‹è¾“å‡º
        targets: åŒ…å«target_audioçš„ç›®æ ‡æ•°æ®
        minimum_pesq_threshold: æœ€ä½PESQé˜ˆå€¼
        current_step: å½“å‰è®­ç»ƒæ­¥éª¤
        device: è®¾å¤‡
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆæˆéŸ³é¢‘
        synthesized_audio = model_outputs.get('synthesized_audio')  # [B, 1, L]
        target_audio = targets.get('target_audio')  # [B, L]

        if synthesized_audio is None or target_audio is None:
            # å¦‚æœæ²¡æœ‰éŸ³é¢‘ï¼Œè¿”å›0æŸå¤±
            if current_step % 100 == 0:
                print(f"[PESQ] Step {current_step}: No audio available, PESQ loss disabled")
            return torch.tensor(0.0, device=device)

        # å¯¼å…¥PESQï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¯åŠ¨æ—¶çš„ä¾èµ–é—®é¢˜ï¼‰
        try:
            from pesq import pesq
        except ImportError:
            if current_step % 100 == 0:
                print(f"[PESQ] Step {current_step}: PESQ package not available, using fallback")
            return torch.tensor(0.0, device=device)

        # è½¬æ¢éŸ³é¢‘æ ¼å¼
        if synthesized_audio.dim() == 3:  # [B, 1, L]
            synth_audio = synthesized_audio.squeeze(1)  # [B, L]
        else:
            synth_audio = synthesized_audio

        # ç¡®ä¿é•¿åº¦åŒ¹é…ï¼ˆå–è¾ƒçŸ­çš„é•¿åº¦ï¼‰
        min_length = min(synth_audio.shape[1], target_audio.shape[1])
        synth_audio = synth_audio[:, :min_length]
        target_audio = target_audio[:, :min_length]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„PESQ
        pesq_scores = []
        sample_rate = 16000  # å‡è®¾16kHzé‡‡æ ·ç‡

        for i in range(synth_audio.shape[0]):
            # è½¬æ¢ä¸ºnumpyå¹¶å½’ä¸€åŒ–åˆ°[-1, 1]
            ref_audio = target_audio[i].detach().cpu().numpy()
            deg_audio = synth_audio[i].detach().cpu().numpy()

            # å½’ä¸€åŒ–éŸ³é¢‘
            ref_audio = np.clip(ref_audio / np.abs(ref_audio).max().clip(1e-7, None), -1, 1)
            deg_audio = np.clip(deg_audio / np.abs(deg_audio).max().clip(1e-7, None), -1, 1)

            try:
                # è®¡ç®—PESQ (wbæ¨¡å¼ï¼Œæ”¯æŒ16kHz)
                pesq_score = pesq(sample_rate, ref_audio, deg_audio, 'wb')
                pesq_scores.append(pesq_score)
            except Exception as e:
                # PESQè®¡ç®—å¤±è´¥æ—¶ä½¿ç”¨ä½åˆ†
                pesq_scores.append(1.0)

        # è½¬æ¢ä¸ºtensor
        pesq_tensor = torch.tensor(pesq_scores, device=device, dtype=torch.float32)

        # è®¡ç®—è´¨é‡æŸå¤±ï¼šä½äºé˜ˆå€¼æ—¶æƒ©ç½š
        quality_penalty = F.relu(minimum_pesq_threshold - pesq_tensor).pow(2)

        # è°ƒè¯•è¾“å‡º
        if current_step % 50 == 0:
            print(f"[Real PESQ] Step {current_step}: PESQ range [{pesq_tensor.min():.3f}, {pesq_tensor.max():.3f}], "
                  f"threshold={minimum_pesq_threshold}, penalty={quality_penalty.mean():.6f}")

        return quality_penalty.mean()

    except Exception as e:
        # å‡ºé”™æ—¶è¿”å›0æŸå¤±ï¼Œé¿å…è®­ç»ƒä¸­æ–­
        if current_step % 100 == 0:
            print(f"[PESQ] Step {current_step}: PESQ computation failed: {e}")
        return torch.tensor(0.0, device=device)

    

class GradientAwareLossWeights:
    """åŸºäºæ¢¯åº¦æ„ŸçŸ¥çš„åŠ¨æ€æŸå¤±æƒé‡è°ƒåº¦å™¨"""

    def __init__(self, total_steps: int = 8000, adaptation_rate: float = 0.1):
        self.total_steps = total_steps
        self.adaptation_rate = adaptation_rate
        # æ¢¯åº¦å†å²è®°å½•
        self.gradient_history = {
            'feat': [],
            'wave': [],
            'semantic': [],
            'quality': [],
            'commitment': []
        }
        # å½“å‰æƒé‡ï¼ˆåªä¿ç•™æœ‰æ•ˆçš„lossé¡¹ï¼‰
        self.current_weights = {
            'feat': 0.5,        # ç‰¹å¾é‡æ„æƒé‡
            'wave': 0.6,        # æ³¢å½¢æŸå¤±æƒé‡
            'semantic': 0.4,     # è¯­ä¹‰ä¿æŒæƒé‡
            'quality': 0.3,      # è´¨é‡ä¼°è®¡æƒé‡
            'commitment': 0.05   # VQ commitmentæƒé‡
        }

    def compute_gradient_magnitudes(self, model, individual_losses: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        åŸºäºâ€œå…±äº«å¹²çº¿â€å‚æ•°é›†åˆä¼°è®¡å„æŸå¤±çš„æ¢¯åº¦èŒƒæ•°ï¼ˆGradNormé£æ ¼ï¼‰ã€‚
        å…±äº«é›†åˆé»˜è®¤åŒ…å«ï¼šencoder.*, rvq_encoder.*, decoder.refiner.*ï¼›
        æ’é™¤ï¼šfargan_core, period_estimator, teacher(_momentum), rate_controllerã€‚
        """
        grad_magnitudes: Dict[str, float] = {}

        # æ”¶é›†å…±äº«å¹²çº¿å‚æ•°
        shared_params = []
        for name, p in model.named_parameters():
            if not (p.requires_grad and p.is_leaf):
                continue
            n = name.lower()
            include = (('encoder.' in n) or ('rvq_encoder' in n) or ('decoder.refiner' in n))
            exclude = ('fargan_core' in n) or ('period_estimator' in n) or ('teacher' in n) or ('rate_controller' in n)
            if include and (not exclude):
                shared_params.append(p)
        # é€€åŒ–å¤„ç†ï¼šè‹¥æ²¡æ”¶é›†åˆ°ï¼Œä½¿ç”¨å…¨éƒ¨ requires_grad ä½œä¸ºå…±äº«é›†åˆ
        if not shared_params:
            shared_params = [p for _, p in model.named_parameters() if p.requires_grad]

        for loss_name, loss_tensor in individual_losses.items():
            try:
                if (loss_tensor is None) or (not torch.is_tensor(loss_tensor)) or (not loss_tensor.requires_grad):
                    grad_magnitudes[loss_name] = 0.0
                    continue
                grads = torch.autograd.grad(loss_tensor, shared_params,
                                            retain_graph=True, create_graph=False, allow_unused=True)
                # L2èŒƒæ•°èšåˆ
                sq = 0.0
                for g in grads:
                    if g is not None:
                        sq = sq + float(g.pow(2).sum().item())
                grad_magnitudes[loss_name] = float((sq + 1e-12) ** 0.5)
            except Exception:
                grad_magnitudes[loss_name] = float(loss_tensor.item()) if torch.is_tensor(loss_tensor) else 0.0

        return grad_magnitudes

    def update_weights_based_on_gradients(self, model, individual_losses: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """åŸºäºæ¢¯åº¦åŠ¨æ€æ›´æ–°æƒé‡"""

        # è®¡ç®—æ¢¯åº¦å¹…åº¦
        grad_mags = self.compute_gradient_magnitudes(model, individual_losses)

        # è°ƒè¯•ï¼šæ£€æŸ¥æ¢¯åº¦è®¡ç®—ç»“æœ
        if any(v > 0 for v in grad_mags.values()):
            grad_debug = ", ".join([f"{k}:{v:.2e}" for k, v in grad_mags.items() if v > 0])
        else:
            grad_debug = "all_zero"

        # æ›´æ–°æ¢¯åº¦å†å²
        for key, mag in grad_mags.items():
            if key in self.gradient_history:
                self.gradient_history[key].append(mag)
                # ä¿æŒå†å²é•¿åº¦
                if len(self.gradient_history[key]) > 20:
                    self.gradient_history[key] = self.gradient_history[key][-20:]

        # è®¡ç®—ç›¸å¯¹æ¢¯åº¦å¼ºåº¦
        total_grad = sum(grad_mags.values()) + 1e-8
        relative_grads = {k: v / total_grad for k, v in grad_mags.items()}

        # åŠ¨æ€è°ƒæ•´æƒé‡ï¼šæ¢¯åº¦è¶Šå¤§ï¼Œæƒé‡è¶Šå°ï¼ˆé˜²æ­¢ä¸»å¯¼ï¼‰
        for key in self.current_weights:
            if key in relative_grads and key in individual_losses:
                rel_grad = relative_grads[key]
                current_loss = individual_losses[key].item()

                # ç»¼åˆè€ƒè™‘æ¢¯åº¦å¤§å°å’Œlosså¤§å°
                if rel_grad > 0.3:  # æ¢¯åº¦è¿‡å¤§
                    target_weight = self.current_weights[key] * 0.8  # é™ä½æƒé‡
                elif rel_grad < 0.05:  # æ¢¯åº¦è¿‡å°
                    target_weight = self.current_weights[key] * 1.2  # å¢åŠ æƒé‡
                else:
                    target_weight = self.current_weights[key]  # ä¿æŒæƒé‡

                # æƒé‡èŒƒå›´é™åˆ¶
                target_weight = max(0.01, min(1.5, target_weight))

                # å¹³æ»‘æ›´æ–°
                self.current_weights[key] = (
                    (1 - self.adaptation_rate) * self.current_weights[key] +
                    self.adaptation_rate * target_weight
                )

        return self.current_weights.copy()

    def get_weights(self, current_step: int) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰"""
        return self.current_weights.copy()

    def get_rate_warmup_factor(self, step: int, warmup_steps: int) -> float:
        """ç ç‡æŸå¤±é¢„çƒ­å› å­"""
        if step < warmup_steps:
            return float(step) / warmup_steps
        return 1.0


class AdaptiveLossWeights:
    """åŠ¨æ€æŸå¤±æƒé‡è°ƒåº¦å™¨ï¼ˆåŸç‰ˆï¼‰"""

    def __init__(self, total_steps: int = 8000):
        self.total_steps = total_steps

    def get_weights(self, current_step: int) -> Dict[str, float]:
        """
        æ ¹æ®è®­ç»ƒæ­¥éª¤è¿”å›åŠ¨æ€æƒé‡

        ä¸‰é˜¶æ®µç­–ç•¥:
        1. ç ç‡æ¢ç´¢ (0-1000æ­¥): é‡ç‚¹å­¦ä¹ ç ç‡æ§åˆ¶
        2. è´¨é‡-ç ç‡å¹³è¡¡ (1000-3000æ­¥): å¹³è¡¡è´¨é‡å’Œç ç‡
        3. è´¨é‡ä¼˜åŒ– (3000+æ­¥): ä¼˜å…ˆä¿è¯è´¨é‡
        """
        progress = current_step / self.total_steps

        if current_step < 1000:
            # é˜¶æ®µ1ï¼šç ç‡æ¢ç´¢æœŸ
            return {
                'feat': 0.5,      # ç‰¹å¾é‡æ„
                'wave': 0.6,      # æ³¢å½¢è´¨é‡
                'semantic': 0.4,   # è¯­ä¹‰ä¿æŒ
                'quality': 0.2,    # è´¨é‡ä¼°è®¡
                'commitment': 0.3  # commitmentæƒé‡
            }
        elif current_step < 3000:
            # é˜¶æ®µ2ï¼šè´¨é‡-ç ç‡å¹³è¡¡æœŸ
            alpha = (current_step - 1000) / 2000  # 0åˆ°1çš„è¿›åº¦
            return {
                'feat': 0.5 - 0.1 * alpha,     # 0.5 -> 0.4
                'wave': 0.6 + 0.1 * alpha,     # 0.6 -> 0.7
                'semantic': 0.4,               # ä¿æŒä¸å˜
                'quality': 0.2 + 0.1 * alpha,   # 0.2 -> 0.3
                'commitment': 0.3 - 0.1 * alpha # 0.3 -> 0.2
            }
        else:
            # é˜¶æ®µ3ï¼šè´¨é‡ä¼˜åŒ–æœŸ
            return {
                'feat': 0.3,      # é€‚åº¦ç‰¹å¾é‡æ„
                'wave': 0.8,      # ä¼˜å…ˆä¿è¯è´¨é‡
                'semantic': 0.3,   # å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§
                'quality': 0.4,    # å¢å¼ºè´¨é‡ç›‘ç£
                'commitment': 0.1  # é™ä½é‡åŒ–çº¦æŸ
            }

    def get_rate_warmup_factor(self, current_step: int, warmup_steps: int = 500) -> float:
        """ç ç‡çº¦æŸé¢„çƒ­å› å­ - ç¦ç”¨warmupè®©rate lossç«‹å³ç”Ÿæ•ˆ"""
        # ç¦ç”¨warmupï¼Œè®©rate lossä»ç¬¬ä¸€æ­¥å°±æœ‰æ•ˆ
        return 1.0

class Stage5ComprehensiveLoss:
    """Stage5ç»¼åˆæŸå¤±è®¡ç®—å™¨"""

    def __init__(self, config: Dict):
        self.config = config

        # è‡ªåŠ¨è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
        total_steps = self._calculate_total_steps(config)

        # é€‰æ‹©æƒé‡è°ƒåº¦å™¨ç±»å‹
        enable_gradient_balancing = config.get('enable_gradient_balancing', True)
        if enable_gradient_balancing:
            self.weight_scheduler = GradientAwareLossWeights(
                total_steps=total_steps,
                adaptation_rate=config.get('gradient_adaptation_rate', 0.01)
            )
            self.is_gradient_aware = True
            print("âœ… Multi-task gradient balancing enabled")
            print(f"   Alpha: {config.get('gradient_balance_alpha', 0.16)}")
            print(f"   Adaptation rate: {config.get('gradient_adaptation_rate', 0.01)}")
        else:
            self.weight_scheduler = AdaptiveLossWeights(
                total_steps=total_steps
            )
            self.is_gradient_aware = False
            print("ğŸ“Š Using adaptive loss weights without gradient balancing")

        # æ³¨å…¥CLIæƒé‡è®¾ç½®ï¼ˆä¿®å¤CLIå‚æ•°æœªç”Ÿæ•ˆçš„é—®é¢˜ï¼‰
        init_w = self.config.get('initial_loss_weights')
        if isinstance(init_w, dict):
            for k, v in init_w.items():
                if k in self.weight_scheduler.current_weights and isinstance(v, (int, float)):
                    self.weight_scheduler.current_weights[k] = float(v)
                    print(f"   âœ… CLIæƒé‡æ³¨å…¥: {k}={v}")

        # æŸå¤±å†å²(ç”¨äºç¨³å®šæ€§ç›‘æ§)
        self.loss_history = {
            'rate': [],
            'quality': [],
            'stability': []
        }

        # Dual-ascent state for closed-loop bitrate control
        self._lambda_rate: float = float(self.config.get('initial_lambda_rate', 0.0))
        self._rate_ema: float = 0.0
        self._dual_eta: float = float(self.config.get('dual_eta', 1e-3))
        bounds = self.config.get('lambda_rate_bounds', (0.0, 5.0))
        self._lambda_min: float = float(bounds[0])
        self._lambda_max: float = float(bounds[1])
        self._lambda_rate_scale: float = float(self.config.get('lambda_rate_scale', 1.0))

    def _calculate_total_steps(self, config: Dict) -> int:
        """
        è‡ªåŠ¨è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
        ä¼˜å…ˆçº§ï¼š
        1. ç›´æ¥æŒ‡å®š total_steps
        2. num_epochs * steps_per_epoch
        3. num_epochs * (dataset_size / batch_size)
        4. é»˜è®¤å€¼ 8000
        """
        # ä¼˜å…ˆçº§1ï¼šç›´æ¥æŒ‡å®š
        if 'total_steps' in config and config['total_steps'] is not None:
            return config['total_steps']

        # ä¼˜å…ˆçº§2ï¼šepochs * steps_per_epoch
        if 'num_epochs' in config and 'steps_per_epoch' in config:
            total_steps = config['num_epochs'] * config['steps_per_epoch']
            print(f"Auto-calculated total_steps: {config['num_epochs']} epochs Ã— {config['steps_per_epoch']} steps = {total_steps}")
            return total_steps

        # ä¼˜å…ˆçº§3ï¼šepochs * (dataset_size / batch_size)
        if all(k in config for k in ['num_epochs', 'dataset_size', 'batch_size']):
            steps_per_epoch = max(1, config['dataset_size'] // config['batch_size'])
            total_steps = config['num_epochs'] * steps_per_epoch
            print(f"Auto-calculated total_steps: {config['num_epochs']} epochs Ã— {steps_per_epoch} steps/epoch = {total_steps}")
            print(f"  (dataset_size={config['dataset_size']}, batch_size={config['batch_size']})")
            return total_steps

        # ä¼˜å…ˆçº§4ï¼šä»è®­ç»ƒå™¨é…ç½®æ¨æ–­
        if 'num_epochs' in config:
            estimated_steps_per_epoch = config.get('estimated_steps_per_epoch', 1000)  # é»˜è®¤ä¼°è®¡
            total_steps = config['num_epochs'] * estimated_steps_per_epoch
            print(f"Estimated total_steps: {config['num_epochs']} epochs Ã— {estimated_steps_per_epoch} steps/epoch = {total_steps}")
            print(f"  (using estimated_steps_per_epoch, may not be accurate)")
            return total_steps

        # é»˜è®¤å›é€€
        default_steps = 8000
        print(f"Using default total_steps: {default_steps} (no epoch/dataset info provided)")
        return default_steps

    def compute_comprehensive_loss(
        self,
        model_outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor],
        current_step: int,
        model: Optional[torch.nn.Module] = None  # æ–°å¢ï¼šç”¨äºæ¢¯åº¦æ„ŸçŸ¥æƒé‡è°ƒæ•´
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—Stage5ç»¼åˆæŸå¤±

        Args:
            model_outputs: æ¨¡å‹è¾“å‡ºå­—å…¸
            targets: ç›®æ ‡æ•°æ®å­—å…¸
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°
        """
        # P6ä¿®å¤ï¼šç»Ÿä¸€æ–­è¨€ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§
        if 'original_features' in targets:
            target_features = targets['original_features']
            assert target_features.dim() == 3, f"target_features must be 3D [B,T,D], got {target_features.shape}"
            assert target_features.shape[-1] == 36, f"target_features last dim must be 36, got {target_features.shape[-1]}"

        if 'reconstructed_features' in model_outputs:
            recon_features = model_outputs['reconstructed_features']
            assert recon_features.dim() == 3, f"reconstructed_features must be 3D [B,T,D], got {recon_features.shape}"
            assert recon_features.shape[-1] == 36, f"reconstructed_features last dim must be 36, got {recon_features.shape[-1]}"

        device = model_outputs['quantized_latent'].device

        # === 1. è·å–åŠ¨æ€æƒé‡ ===
        weights = self.weight_scheduler.get_weights(current_step)
        recon_only = bool(self.config.get('recon_only', False))
        if recon_only:
            weights.update({'feat': 1.0, 'wave': 1.0, 'semantic': 0.0, 'quality': 0.0, 'commitment': 0.0, 'rate': 0.0})

        # === 2. åŸºç¡€é‡æ„æŸå¤±ï¼ˆåœ¨rawç©ºé—´åº¦é‡ï¼‰ ===
        y_hat_feat = model_outputs.get('recon_features_raw', model_outputs['reconstructed_features'])
        y_ref_feat = targets['original_features']
        feat_recon_loss = F.l1_loss(y_hat_feat, y_ref_feat)

        # Whitened reconstruction (per-dim) to penalize scale mismatch explicitlyï¼ˆrawç©ºé—´ï¼‰
        eps_whiten = 1e-6
        try:
            mu_tgt = y_ref_feat.mean(dim=(0, 1))              # [D]
            std_tgt = y_ref_feat.std(dim=(0, 1))               # [D]
            y_hat_w = (y_hat_feat - mu_tgt) / (std_tgt + eps_whiten)
            y_ref_w = (y_ref_feat - mu_tgt) / (std_tgt + eps_whiten)
            feat_whiten_mse = torch.nn.functional.mse_loss(y_hat_w, y_ref_w)
        except Exception:
            feat_whiten_mse = torch.tensor(0.0, device=y_hat_feat.device)

        # Predicted log-std calibration (decoder three-head) â€“ supervise typical per-dim scale
        pred_logstd = model_outputs.get('pred_logstd', None)
        if torch.is_tensor(pred_logstd):
            try:
                # Average predicted logstd over B,T to get a stable per-dim estimate
                pred_logstd_mean = pred_logstd.mean(dim=(0, 1))  # [D]
                logstd_target = (std_tgt + eps_whiten).log()     # [D]
                logstd_mse = torch.nn.functional.mse_loss(pred_logstd_mean, logstd_target)
            except Exception:
                logstd_mse = torch.tensor(0.0, device=y_hat_feat.device)
        else:
            logstd_mse = torch.tensor(0.0, device=y_hat_feat.device)

        # è®°å½•ç‰¹å¾é‡æ„æŸå¤±ç”¨äºè¯­ä¹‰å­¦ä¹ æ§åˆ¶
        self._last_feat_loss = feat_recon_loss.item()

        # === 3. æ³¢å½¢æ„ŸçŸ¥æŸå¤± ===
        synthesized = model_outputs.get('synthesized_audio')
        target = targets.get('target_audio')

        # è°ƒè¯•ï¼šæ£€æŸ¥éŸ³é¢‘æ•°æ®ï¼ˆæ¯50æ­¥ï¼‰
        if current_step % 50 == 0:
            synth_status = f"shape={synthesized.shape}, range=[{synthesized.min():.3f}, {synthesized.max():.3f}]" if synthesized is not None else "None"
            target_status = f"shape={target.shape}, range=[{target.min():.3f}, {target.max():.3f}]" if target is not None else "None"
            print(f"[Wave Debug] Step {current_step}: Synth={synth_status}, Target={target_status}")

        if synthesized is not None and target is not None:
            # ä¿®å¤éŸ³é¢‘æ ¼å¼åŒ¹é…é—®é¢˜
            # 1. ç»´åº¦å¯¹é½ï¼š[B, 1, L] -> [B, L]
            if len(synthesized.shape) == 3 and synthesized.shape[1] == 1:
                synthesized = synthesized.squeeze(1)

            # 2. é•¿åº¦å¯¹é½ï¼šè£å‰ªåˆ°ç›¸åŒé•¿åº¦
            min_len = min(synthesized.shape[-1], target.shape[-1])
            synthesized = synthesized[..., :min_len]
            target = target[..., :min_len]

            # è°ƒè¯•ï¼šéªŒè¯gainä¿®å¤æ•ˆæœ
            if current_step % 50 == 0:
                synth_max = synthesized.abs().max()
                if synth_max > 2.0:
                    print(f"[Wave Warning] Step {current_step}: Audio amplitude still high: {synth_max:.3f}")
                else:
                    print(f"[Wave OK] Step {current_step}: Audio amplitude normal: {synth_max:.3f}")

            wave_loss, wave_details = fargan_wave_losses(
                synthesized, target, targets.get('period', None), device=device
            )
            # é¢å¤–ï¼šåŠ å…¥SI-SDRä»¥å¢å¼ºå¯¹å¬æ„Ÿç›¸å…³è¯¯å·®çš„æ•æ„Ÿåº¦
            def _si_sdr(pred, ref, eps=1e-8):
                # pred/ref: [B, L]
                ref_energy = (ref ** 2).sum(dim=-1, keepdim=True) + eps
                proj = ((pred * ref).sum(dim=-1, keepdim=True) / ref_energy) * ref
                e_noise = pred - proj
                sdr = (proj ** 2).sum(dim=-1) / ((e_noise ** 2).sum(dim=-1) + eps)
                return 10 * torch.log10(sdr + eps)  # [B]

            if bool(self.config.get('enable_sisdr', True)):
                lambda_sisdr = float(self.config.get('lambda_sisdr', 0.5))
                sisdr = _si_sdr(synthesized, target)
                sisdr_loss = (-sisdr.mean())
                wave_loss = wave_loss + lambda_sisdr * sisdr_loss
                try:
                    wave_details = {**wave_details, 'sisdr_db': float(sisdr.mean().item())}
                except Exception:
                    pass

            # é¢å¤–ï¼šLSD(dB)è§‚æµ‹ä¸å¯é€‰æŸå¤±
            if bool(self.config.get('enable_lsd', True)):
                try:
                    n_fft = int(self.config.get('lsd_n_fft', 512))
                    hop = int(self.config.get('lsd_hop', 160))
                    win = int(self.config.get('lsd_win', 320))
                    window = torch.hann_window(win, device=device)
                    def _spec_db(x):
                        # x: [B,L]
                        X = torch.stft(x, n_fft=n_fft, hop_length=hop, win_length=win,
                                       window=window, return_complex=True)
                        mag = (X.abs() + 1e-8)
                        return 20.0 * torch.log10(mag)
                    Xdb = _spec_db(synthesized)
                    Ydb = _spec_db(target)
                    # LSD per frame: sqrt(mean_f (Î”dB^2)), then mean over time and batch
                    lsd_per_frame = torch.sqrt(((Xdb - Ydb) ** 2).mean(dim=1) + 1e-8)  # [B,T]
                    lsd_db = lsd_per_frame.mean()
                    # å¯é€‰åŠ å…¥åˆ°loss
                    lambda_lsd = float(self.config.get('lambda_lsd', 0.0))
                    if lambda_lsd > 0.0:
                        wave_loss = wave_loss + lambda_lsd * lsd_db
                    wave_details = {**wave_details, 'lsd_db': float(lsd_db.item())}
                except Exception:
                    pass
        else:
            wave_loss = torch.tensor(0.0, device=device)
            wave_details = {}

        # === 4. æœ‰æ•ˆç ç‡æŸå¤±ï¼ˆå¯å¾®ï¼‰ ===============================================
        # è¯»å–é…ç½®
        # ä¿¡æ¯ç“¶é¢ˆæ€»é¢„ç®—ï¼ˆVIBä¸Rateå…±äº«ï¼‰ - å¿…é¡»åœ¨æœ€æ—©å®šä¹‰
        total_info_budget = self.config.get('total_info_regularization_budget', 1.0)
        beta_vib = float(self.config.get('beta_vib', 1e-3))
        vib_warmup = int(self.config.get('vib_warmup_steps', 3000))  # ä¿®å¤3ï¼šæ‹‰é•¿warmupï¼Œè®©RVQå…ˆç«™ç¨³
        sem_tau = float(self.config.get('semantic_temperature', 0.2))
        # æ¸©åº¦é€€ç«é€»è¾‘ï¼ˆæŒ‰ç…§ç”¨æˆ·å»ºè®®ï¼‰
        if self.config.get('semantic_temp_annealing', False):
            tau_end  = float(self.config.get('semantic_temp_end', 0.07))
            tau_steps = int(self.config.get('semantic_temp_steps', 3000))
            # çº¿æ€§é€€ç«ï¼šä»0.2 -> 0.07
            t = min(max(current_step, 0), tau_steps)
            if tau_steps > 0:
                sem_tau = sem_tau + (tau_end - sem_tau) * (t / float(tau_steps))
            else:
                # If tau_steps is 0, use final temperature immediately
                sem_tau = tau_end

        rate_type = self.config.get('rate_loss_type', 'soft_entropy')  # fixed to soft_entropy
        if rate_type != 'soft_entropy':
            rate_type = 'soft_entropy'
        lambda_rate = float(self.config.get('lambda_rate', 0.5))
        target_kbps = float(self.config.get('target_kbps', 1.2))
        tol_kbps = float(self.config.get('rate_tolerance', 0.1))
        frame_rate = float(self.config.get('frame_rate', 50.0))

        # æœ‰æ•ˆç ç‡è®¡ç®—
        rate_loss = torch.tensor(0.0, device=device)
        eff_bits_per_frame = torch.tensor(0.0, device=device)
        nom_bits_per_frame = torch.tensor(0.0, device=device)

        if (not recon_only) and rate_type == 'soft_entropy' and ('rvq_soft_probs' in model_outputs):
            # Differentiable soft-entropy from soft assignments; supports gradient to encoder/codebook
            stage_sizes = model_outputs.get('rvq_stage_sizes', [1024, 512, 256])
            q_list = model_outputs.get('rvq_soft_probs', [])  # list of [B, T, K]
            gate_soft_list = model_outputs.get('stage_gate_soft', [])  # list of [B, T]

            eff_bpf = torch.tensor(0.0, device=device)

            for i, K in enumerate(stage_sizes):
                if i < len(q_list) and q_list[i] is not None and torch.is_tensor(q_list[i]):
                    q = q_list[i]  # [B, T, K]
                    # Per-frame entropy
                    ent_bT = (-(q * torch.log2(q.clamp_min(1e-12))).sum(dim=-1))  # [B,T]
                    # Apply soft gate if available
                    if i < len(gate_soft_list) and gate_soft_list[i] is not None and torch.is_tensor(gate_soft_list[i]):
                        ent_bT = ent_bT * gate_soft_list[i]
                    ent = ent_bT.mean()
                else:
                    # Fallback to nominal bits if soft probs not present
                    ent = torch.tensor(math.log2(K), device=device)

                eff_bpf = eff_bpf + ent

            eff_bits_per_frame = eff_bpf
            kbps_eff = eff_bits_per_frame * frame_rate / 1000.0

            # Dual-ascent closed-loop control: Î» * (kbps_eff - target)
            try:
                err_inst = float((kbps_eff - target_kbps).detach().item())
            except Exception:
                err_inst = 0.0
            # EMA smoothing for rate error
            self._rate_ema = 0.95 * self._rate_ema + 0.05 * err_inst
            # Update Î» with clipped EMA
            ema_clipped = max(min(self._rate_ema, 2.0), -2.0)
            self._lambda_rate += self._dual_eta * ema_clipped
            self._lambda_rate = max(min(self._lambda_rate, self._lambda_max), self._lambda_min)

            # Linear penalty keeps gradients to eff rate; detach Î»
            lambda_rate_t = torch.tensor(self._lambda_rate, device=device).detach()
            rate_loss = self._lambda_rate_scale * lambda_rate_t * (kbps_eff - target_kbps)
            # For logging: nominal bits is full sum log2(K)
            nom_bits_per_frame = torch.tensor(sum([math.log2(K) for K in stage_sizes]), device=device)

        else:
            kbps_eff = torch.tensor(0.0, device=device)

        # === 5. è¯­ä¹‰æ„ŸçŸ¥ï¼šæ”¯æŒ InfoNCE ä¸ MSE ä¸¤ç§æ¨¡å¼ =============================
        semantic_mode = self.config.get('semantic_mode', 'mse')  # 'nce' or 'mse'

        if (not recon_only) and semantic_mode == 'nce' and ('student_sem' in model_outputs) and ('teacher_sem' in model_outputs):
            # æ”¹è¿›çš„é—¨æ§ï¼šåŸºäºç æœ¬å¥åº·åº¦è€Œä¸æ˜¯commitment_loss
            codebook_healthy = False
            feat_recon_stable = False

            # 1. æ£€æŸ¥ç æœ¬å¥åº·åº¦ï¼ˆå›°æƒ‘åº¦ï¼‰
            rvq_details = model_outputs.get('rvq_details', {})
            if isinstance(rvq_details, dict) and rvq_details.get('stage_perplexities') is not None:
                perplexities = rvq_details.get('stage_perplexities')  # [num_stages] - ä¿®å¤å­—æ®µå
                codebook_sizes = model_outputs.get('rvq_stage_sizes', [512, 512, 512])  # é»˜è®¤å€¼

                # ä¿®å¤ï¼šå¤„ç†perplexitieså¯èƒ½æ˜¯listçš„æƒ…å†µ
                if isinstance(perplexities, list):
                    if len(perplexities) > 0 and len(codebook_sizes) >= len(perplexities):
                        # è½¬æ¢ä¸ºtensorå¹¶æ£€æŸ¥å›°æƒ‘åº¦
                        perp_tensor = torch.tensor(perplexities, device=device)
                        size_tensor = torch.tensor(codebook_sizes[:len(perplexities)], device=device)
                        perplexity_ratios = perp_tensor / size_tensor.float()
                        codebook_healthy = (perplexity_ratios > 0.1).all().item()  # æ‰€æœ‰stageå›°æƒ‘åº¦ > 10%
                elif torch.is_tensor(perplexities) and len(codebook_sizes) >= len(perplexities):
                    # åŸæœ‰tensorå¤„ç†é€»è¾‘
                    size_tensor = torch.tensor(codebook_sizes[:len(perplexities)], device=perplexities.device)
                    perplexity_ratios = perplexities / size_tensor.float()
                    codebook_healthy = (perplexity_ratios > 0.1).all().item()  # æ‰€æœ‰stageå›°æƒ‘åº¦ > 10%
                else:
                    # å¤‡ç”¨ï¼šä½¿ç”¨ç ç‡å¥åº·åº¦åˆ¤æ–­
                    if 'effective_rate_bpf' in rvq_details and 'expected_rate_bpf' in rvq_details:
                        eff_rate = rvq_details['effective_rate_bpf']
                        exp_rate = rvq_details['expected_rate_bpf']
                        rate_ratio = eff_rate / exp_rate.clamp_min(0.1)  # é¿å…é™¤é›¶
                        codebook_healthy = 0.5 <= rate_ratio <= 2.0  # ç ç‡åœ¨æœŸæœ›å€¼çš„50%-200%èŒƒå›´å†…

            # 2. æ£€æŸ¥ç‰¹å¾é‡å»ºç¨³å®šæ€§
            if hasattr(self, '_last_feat_loss'):
                feat_recon_stable = self._last_feat_loss < 2.0  # æ”¾å®½é˜ˆå€¼ï¼š2.0ä»¥ä¸‹è®¤ä¸ºç¨³å®š
            else:
                feat_recon_stable = False

            # 3. å¼ºåˆ¶å¯ç”¨æœºåˆ¶
            force_enable_step = int(self.config.get('semantic_force_enable_step', 1500))  # 1500æ­¥åå¼ºåˆ¶å¯ç”¨

            # è°ƒè¯•ï¼šæ¯50æ­¥æ£€æŸ¥ä¸€æ¬¡æ¡ä»¶
            if current_step % 50 == 0:
                feat_val = getattr(self, '_last_feat_loss', 'N/A')
                perp_info = ""
                if isinstance(rvq_details, dict) and rvq_details.get('stage_perplexities') is not None:
                    perp_data = rvq_details.get('stage_perplexities', [])
                    # ä¿®å¤ï¼šå®‰å…¨å¤„ç†perplexitiesçš„tolist()
                    if isinstance(perp_data, list):
                        perp_vals = perp_data
                    elif torch.is_tensor(perp_data):
                        perp_vals = perp_data.tolist()
                    else:
                        perp_vals = []
                    perp_info = f"perp={perp_vals}"
                print(f"[Semantic Debug] Step {current_step}: codebook_healthy={codebook_healthy}, "
                      f"feat_loss={feat_val}, feat_stable={feat_recon_stable}, {perp_info}")

            # æ”¹è¿›çš„é—¨æ§æ¡ä»¶ï¼šç æœ¬å¥åº· + ç‰¹å¾ç¨³å®š æˆ– å¼ºåˆ¶å¯ç”¨
            if (codebook_healthy and feat_recon_stable) or current_step >= force_enable_step:
                # RVQç›¸å¯¹ç¨³å®šï¼Œå¼€å§‹è¯­ä¹‰å¯¹æ¯”å­¦ä¹ 
                # ä½¿ç”¨æ—¶é—´ç²’åº¦NT-Xentå¢åŠ è´Ÿæ ·æœ¬æ•°é‡ï¼ˆæŒ‰ç…§ç”¨æˆ·å»ºè®®ï¼‰
                semantic_loss = nt_xent_loss(
                    F.normalize(model_outputs['student_sem'], dim=1),  # [B,D,T]
                    F.normalize(model_outputs['teacher_sem'], dim=1),
                    temperature=sem_tau
                )

                if current_step % 50 == 0:
                    print(f"[Semantic] Step {current_step}: âœ… NT-Xent ACTIVE, loss={semantic_loss:.4f}, Ï„={sem_tau:.3f}")
            else:
                # ç æœ¬ä¸å¥åº·æˆ–ç‰¹å¾ä¸ç¨³å®šï¼Œæš‚åœè¯­ä¹‰å­¦ä¹ 
                semantic_loss = torch.tensor(0.0, device=device)

                if current_step % 50 == 0:
                    print(f"[Semantic] Step {current_step}: âŒ InfoNCE DISABLED (codebook_healthy={codebook_healthy}, feat_stable={feat_recon_stable})")

            # å¯é€‰ï¼šæ·»åŠ å°‘é‡MSEè¾…åŠ©
            if self.config.get('add_mse_auxiliary', False) and 'semantic_features' in model_outputs and 'reference_semantic' in targets:
                semantic_target = targets['reference_semantic']
                if semantic_target.dim() == 3 and semantic_target.shape[1] != model_outputs['semantic_features'].shape[1]:
                    semantic_target = semantic_target.transpose(1, 2)
                pred_semantic = F.normalize(model_outputs['semantic_features'], p=2, dim=1)
                target_semantic = F.normalize(semantic_target, p=2, dim=1)
                mse_aux = F.mse_loss(pred_semantic, target_semantic)
                semantic_loss = semantic_loss + 0.1 * mse_aux

        elif 'semantic_features' in model_outputs and 'reference_semantic' in targets:
            # å›é€€ï¼šåŸå§‹MSEè¯­ä¹‰æŸå¤±
            semantic_target = targets['reference_semantic']
            if semantic_target.dim() == 3 and semantic_target.shape[1] != model_outputs['semantic_features'].shape[1]:
                semantic_target = semantic_target.transpose(1, 2)
            pred_semantic = F.normalize(model_outputs['semantic_features'], p=2, dim=1)
            target_semantic = F.normalize(semantic_target, p=2, dim=1)
            semantic_loss = F.mse_loss(pred_semantic, target_semantic)

        else:
            # æ— è¯­ä¹‰æ•°æ®å¯ç”¨
            semantic_loss = torch.tensor(0.0, device=device)

        # === 6. æ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼ˆç§»é™¤ï¼ŒRVQæœ¬èº«å°±æ˜¯ç¦»æ•£çš„ï¼‰===
        temporal_loss = torch.tensor(0.0, device=device)  # ç§»é™¤æ— æ„ä¹‰çš„temporal loss

        # === 7. çœŸå®PESQè´¨é‡æŸå¤± ===
        quality_loss = compute_real_pesq_loss(
            model_outputs=model_outputs,
            targets=targets,
            minimum_pesq_threshold=self.config.get('min_quality_threshold', 2.5),
            current_step=current_step,
            device=device
        )

        # === 8. RVQç›¸å…³æŸå¤±ï¼ˆå¢å¼ºç‰ˆï¼‰===
        commitment_loss = model_outputs.get('commitment_loss', torch.tensor(0.0, device=device))

        # æ–°å¢ï¼šRVQé‡æ„ä¸€è‡´æ€§æŸå¤±ï¼ˆç¡®ä¿ç¼–ç -è§£ç ä¸€è‡´æ€§ï¼‰
        rvq_reconstruction_loss = torch.tensor(0.0, device=device)
        if model_outputs.get('compression_ready', False) and 'rvq_details' in model_outputs:
            rvq_details = model_outputs['rvq_details']
            stage_indices = rvq_details.get('stage_indices')

            if stage_indices is not None and 'quantized_latent' in model_outputs:
                # è®¡ç®—RVQé‡æ„è¯¯å·®ï¼ˆæ¨¡æ‹Ÿè§£ç å™¨çš„é‡æ„ç²¾åº¦ï¼‰
                try:
                    # è¿™é‡Œéœ€è¦è®¿é—®æ¨¡å‹çš„RVQè§£ç å™¨
                    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™ä¸ªè®¡ç®—åº”è¯¥åœ¨æ¨¡å‹å‰å‘ä¼ æ’­ä¸­å®Œæˆ
                    original_latent = model_outputs.get('encoded_latent')  # [B, T, D]
                    if original_latent is not None:
                        # è½¬æ¢ä¸º[B, D, T]æ ¼å¼è¿›è¡ŒRVQé‡æ„è¯¯å·®è®¡ç®—
                        original_bct = original_latent.transpose(1, 2)
                        quantized_bct = model_outputs['quantized_latent'].transpose(1, 2)
                        rvq_reconstruction_loss = F.mse_loss(quantized_bct, original_bct)
                except Exception:
                    # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    pass

        # === 9. å¯é€‰ï¼šå¯¹æ¯”å­¦ä¹ æŸå¤± ===
        contrastive_loss = torch.tensor(0.0, device=device)
        if hasattr(targets, 'contrastive_pairs'):
            # å®ç°å¯¹æ¯”å­¦ä¹ (å¯é€‰)
            pass

        # === ä¿¡æ¯ç“¶é¢ˆé”™å³°è°ƒåº¦é…ç½® ===================================
        vib_phase_end = self.config.get('vib_phase_end_step', 3000)  # VIBä¸»å¯¼æœŸç»“æŸ

        # === 6. VIB-KL (é”™å³°è°ƒåº¦ç‰ˆ) ===============================================

        vib_kld = model_outputs.get('vib_kld', None)
        # Optional: PI-controlled scaling from rate controller
        vib_scale = 1.0
        try:
            if model is not None and hasattr(model, 'rate_controller'):
                knobs = model.rate_controller.get_last_controls()
                vib_scale = float(knobs.get('vib_beta_scale', 1.0))
        except Exception:
            pass
        if vib_kld is not None:
            # VIBæƒé‡ï¼šæ—©æœŸé«˜ï¼ŒåæœŸé€æ¸é™ä½ä¸ºRateè®©è·¯
            if current_step <= vib_warmup:
                vib_weight = beta_vib * float(current_step) / max(1, vib_warmup)
            elif current_step <= vib_phase_end:
                vib_weight = beta_vib  # ä¿æŒæœ€å¤§å€¼
            else:
                # åæœŸæŒ‡æ•°è¡°å‡ï¼Œä¸ºrateè®©è·¯
                decay_steps = current_step - vib_phase_end
                vib_weight = beta_vib * (0.5 ** (decay_steps / 1000))  # æ¯1000æ­¥è¡°å‡ä¸€åŠ

            # åº”ç”¨æ€»é¢„ç®—çº¦æŸï¼ˆVIBä¸Rateé”™å³°ï¼‰
            vib_budget_ratio = 0.3  # VIBå æ€»é¢„ç®—30%
            vib_loss = vib_scale * vib_weight * total_info_budget * vib_budget_ratio * vib_kld
        else:
            vib_loss = torch.tensor(0.0, device=device)

        # === 9. RVQ Diversity Loss (ç ä¹¦ä½¿ç”¨ç†µæœ€å¤§åŒ–) ==========================
        diversity_loss = torch.tensor(0.0, device=device)
        if (not recon_only) and self.config.get('enable_rvq_diversity', True):  # é»˜è®¤å¼€å¯
            diversity_weight = self.config.get('rvq_diversity_weight', 2e-3)  # å°æƒé‡
            flip_step = int(self.config.get('diversity_flip_step', 2000))
            # Early: encourage balanced usage; Late: discourage entropy to lower rate
            direction = 1.0 if current_step < flip_step else -1.0
            rvq_details = model_outputs.get('rvq_details', {})

            if rvq_details.get('stage_perplexities') is not None:
                perplexities = rvq_details.get('stage_perplexities')  # list of tensors or list
                stage_sizes = model_outputs.get('rvq_stage_sizes', [1024, 512, 256])

                # ä¿®å¤ï¼šå®‰å…¨å¤„ç†perplexitiesç±»å‹
                if isinstance(perplexities, list):
                    # å¦‚æœæ˜¯Python listï¼Œè½¬æ¢ä¸ºtensor
                    if len(perplexities) > 0:
                        perp_tensor = torch.tensor(perplexities, device=device)
                        for i, K in enumerate(stage_sizes[:len(perplexities)]):
                            perp = perp_tensor[i]
                            # é¼“åŠ±é«˜å›°æƒ‘åº¦ï¼ˆæ¥è¿‘æœ€å¤§å€¼log2(K)ï¼‰
                            max_perp = math.log2(K)
                            perp_ratio = perp / max_perp  # å½’ä¸€åŒ–åˆ°[0,1]
                        # è´Ÿç†µæŸå¤±ï¼šæ–¹å‘å¯åˆ‡æ¢ï¼ˆæ—©æœŸé¼“åŠ±é«˜å›°æƒ‘åº¦ï¼ŒåæœŸæŠ‘åˆ¶ç†µï¼‰
                        diversity_loss += direction * (-torch.log(perp_ratio.clamp_min(1e-8)))
                else:
                    # åŸæœ‰tensorå¤„ç†é€»è¾‘
                    for perp, K in zip(perplexities, stage_sizes):
                        if torch.is_tensor(perp) and perp.numel() > 0:
                            # é¼“åŠ±é«˜å›°æƒ‘åº¦ï¼ˆæ¥è¿‘æœ€å¤§å€¼log2(K)ï¼‰
                            max_perp = math.log2(K)
                            perp_ratio = perp / max_perp  # å½’ä¸€åŒ–åˆ°[0,1]
                            # è´Ÿç†µæŸå¤±ï¼šæ–¹å‘å¯åˆ‡æ¢
                            diversity_loss += direction * (-torch.log(perp_ratio.clamp_min(1e-8)))

                diversity_loss *= diversity_weight

            # === æ·»åŠ æ˜¾å¼ç†µå¥–åŠ± ===
            if rvq_details.get('stage_indices') is not None:
                for stage_idx, indices in enumerate(rvq_details.get('stage_indices')):
                    # è®¡ç®—ç¬¦å·ä½¿ç”¨ç†µ
                    flat_indices = indices.reshape(-1)
                    K = model_outputs.get('rvq_stage_sizes', [1024, 512, 256])[stage_idx]
                    counts = torch.bincount(flat_indices, minlength=K).float()
                    probs = counts / counts.sum()
                    H_usage = -(probs * torch.log2(probs + 1e-12)).sum()

                    # ç†µå¥–åŠ±/æƒ©ç½šï¼šæ–¹å‘å¯åˆ‡æ¢
                    diversity_loss += (-direction) * diversity_weight * H_usage

        # === Codebook mask sparsity (effective-K control) ===
        mask_sparsity_penalty = torch.tensor(0.0, device=device)
        if (not recon_only) and 'codebook_mask_usage' in model_outputs:
            usage_list = model_outputs['codebook_mask_usage']  # list of scalars
            if isinstance(usage_list, list) and len(usage_list) > 0:
                mask_mean = torch.stack([u if torch.is_tensor(u) else torch.tensor(float(u), device=device) for u in usage_list]).mean()
                base_w = float(self.config.get('mask_sparsity_weight', 0.0))
                # Optional PI boost from controller
                try:
                    if model is not None and hasattr(model, 'rate_controller'):
                        knobs = model.rate_controller.get_last_controls()
                        base_w = base_w + float(knobs.get('mask_sparsity_boost', 0.0))
                except Exception:
                    pass
                # If rate is under target - tol, disable mask sparsity entirely
                try:
                    if 'kbps_eff' in locals():
                        if float(kbps_eff.item()) < (target_kbps - tol_kbps):
                            base_w = 0.0
                except Exception:
                    pass
                mask_sparsity_penalty = base_w * mask_mean

        # === Stage-gate sparsity (frame-level L0 proxy on higher stages) ===
        gate_l0_penalty = torch.tensor(0.0, device=device)
        try:
            gate_list = model_outputs.get('stage_gate_soft', [])  # list of [B,T]
            if (not recon_only) and isinstance(gate_list, list) and len(gate_list) > 1:
                # Exclude stage 0; penalize average open probability of s>=1
                mean_gates = []
                for i, g in enumerate(gate_list):
                    if i == 0 or (not torch.is_tensor(g)):
                        continue
                    mean_gates.append(g.mean())
                if len(mean_gates) > 0:
                    mean_gate_val = torch.stack(mean_gates).mean()
                    base = float(self.config.get('lambda_gate_l0', 0.0))
                    over = 0.0
                    try:
                        over = max(float((kbps_eff - target_kbps).detach().item()), 0.0) / max(target_kbps, 1e-6)
                    except Exception:
                        over = 0.0
                    gain = float(self.config.get('lambda_gate_over_gain', 0.5))
                    w_gate = base + gain * over
                    gate_l0_penalty = w_gate * mean_gate_val
        except Exception:
            gate_l0_penalty = torch.tensor(0.0, device=device)

        # === Stage-gate Concrete-Bernoulli KL prior (encourage open/close by rate state) ===
        gate_kl_penalty = torch.tensor(0.0, device=device)
        try:
            gate_list = model_outputs.get('stage_gate_soft', [])
            if (not recon_only) and isinstance(gate_list, list) and len(gate_list) > 1:
                # Choose prior pi based on rate condition
                pi_under = float(self.config.get('gate_prior_pi_under', 0.8))
                pi_mid   = float(self.config.get('gate_prior_pi_mid', 0.5))
                pi_over  = float(self.config.get('gate_prior_pi_over', 0.3))
                try:
                    kbps_val = float(kbps_eff.item()) if torch.is_tensor(kbps_eff) else float(kbps_eff)
                except Exception:
                    kbps_val = target_kbps
                if kbps_val < (target_kbps - tol_kbps):
                    pi = pi_under
                elif kbps_val > (target_kbps + tol_kbps):
                    pi = pi_over
                else:
                    pi = pi_mid
                # KL(q||Bernoulli(pi)) averaged over frames, stages>=1
                kl_list = []
                pi_t = torch.tensor(pi, device=device).clamp(1e-4, 1-1e-4)
                for i, g in enumerate(gate_list):
                    if i == 0 or (not torch.is_tensor(g)):
                        continue
                    q = g.clamp(1e-6, 1-1e-6)
                    kl = (q * (q/pi_t).log() + (1-q) * ((1-q)/(1-pi_t)).log()).mean()
                    kl_list.append(kl)
                if kl_list:
                    gate_kl = torch.stack(kl_list).mean()
                    gate_kl_penalty = float(self.config.get('lambda_gate_kl', 0.05)) * gate_kl
        except Exception:
            gate_kl_penalty = torch.tensor(0.0, device=device)

        # === 9. é˜¶æ®µæ„ŸçŸ¥çš„å¤šä»»åŠ¡æƒé‡è°ƒæ•´ ==========================================
        if self.is_gradient_aware and model is not None:
            # æ”¶é›†æ‰€æœ‰æŸå¤±é¡¹ï¼ˆåŒ…æ‹¬æ–°å¢çš„VIBå’Œrate lossï¼‰
            individual_losses = {
                'feat': feat_recon_loss,
                'wave': wave_loss,
                'semantic': semantic_loss,
                'quality': quality_loss,
                'commitment': commitment_loss,
                'rate': rate_loss
            }

            # é˜¶æ®µæ„ŸçŸ¥æƒé‡è°ƒæ•´ï¼šç»“åˆé˜¶æ®µç­–ç•¥å’Œæ¢¯åº¦å¹³è¡¡
            try:
                updated_weights = self._compute_stage_aware_weights(
                    model, individual_losses, weights, current_step
                )
                # ä½¿ç”¨æ›´æ–°åçš„æƒé‡
                old_weights = weights.copy()
                weights.update(updated_weights)

                # æ¯50æ­¥æ‰“å°æƒé‡å˜åŒ–
                if current_step % 50 == 0:
                    print(f"Multi-task gradient balancing (step {current_step}):")
                    for key in ['feat', 'wave', 'semantic', 'quality', 'commitment', 'rate']:
                        old_w = old_weights.get(key, 0)
                        new_w = weights.get(key, 0)
                        change = new_w - old_w
                        print(f"  {key}: {old_w:.4f} -> {new_w:.4f} ({change:+.4f})")

            except Exception as e:
                # å¦‚æœæ¢¯åº¦è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸæƒé‡
                print(f"Warning: Multi-task gradient balancing failed: {e}")

        # === 10. ç»¼åˆæŸå¤±æ±‡æ€» ====================================================
        # ä¿®å¤A: å»é™¤VIBåŒé‡ç¼©æ”¾ - vib_losså·²åŒ…å«beta_vib*warmupï¼Œç›´æ¥ç›¸åŠ 
        # å¯é€‰ï¼šè¾“å‡ºç«¯ ceps ä»¿å°„æ­£åˆ™ï¼ˆè§£ç å™¨æš´éœ²åœ¨debug_statsä¸­ï¼‰
        calib_reg = torch.tensor(0.0, device=device)
        if isinstance(model_outputs, dict):
            dbg = model_outputs.get('debug_stats', {}) if isinstance(model_outputs, dict) else {}
            if isinstance(dbg, dict) and 'calib' in dbg:
                # ä»…ç”¨äºæ—¥å¿—å±•ç¤ºï¼›çœŸå®çš„æ­£åˆ™ä»decoderè¯»å–è¾ƒåˆé€‚ï¼Œä½†æ­¤å¤„ä¿åº•å€¼ 0.0
                pass
        # ä»æ¨¡å‹è¯»å–æ›´å¯é ï¼ˆè‹¥å¯ç”¨ï¼‰
        try:
            if (model is not None and hasattr(model, 'decoder') and
                hasattr(model.decoder, '_last_calib_reg') and model.decoder._last_calib_reg is not None):
                calib_reg = model.decoder._last_calib_reg.to(device=device)
        except Exception:
            pass
        lambda_calib = float(self.config.get('lambda_ceps_calib_reg', 1e-4))

        # Variance band regularizer + temporal difference matching
        stat_band_reg = torch.tensor(0.0, device=device)
        tv_loss = torch.tensor(0.0, device=device)
        lambda_stat = float(self.config.get('lambda_stat_reg', 0.02))
        lambda_tv = float(self.config.get('lambda_tv', 0.02))
        r_lo = float(self.config.get('stat_ratio_lo', 0.6))
        r_hi = float(self.config.get('stat_ratio_hi', 1.4))
        # Mean alignment + domain constraints (frame_corr range)
        mean_match_reg = torch.tensor(0.0, device=device)
        fc_range_penalty = torch.tensor(0.0, device=device)
        lambda_mean = float(self.config.get('lambda_mean_reg', 0.02))
        lambda_fc_range = float(self.config.get('lambda_fc_range', 0.02))
        try:
            from models.feature_adapter import FARGANFeatureSpec
            sl_ceps = FARGANFeatureSpec.get_feature_slice('ceps')
            sl_lpc = FARGANFeatureSpec.get_feature_slice('lpc')
            sl_fc = FARGANFeatureSpec.get_feature_slice('frame_corr')
            y_hat = y_hat_feat  # ä½¿ç”¨rawç©ºé—´ç»Ÿè®¡
            y_ref = y_ref_feat
            def _band_penalty(a, b):
                std_a = a.std(dim=(0, 1))
                std_b = b.std(dim=(0, 1))
                ratio = (std_a + 1e-6) / (std_b + 1e-6)
                low = torch.relu(torch.tensor(r_lo, device=a.device) - ratio)
                high = torch.relu(ratio - torch.tensor(r_hi, device=a.device))
                return (low.pow(2) + high.pow(2)).mean()
            stat_band_reg = _band_penalty(y_hat[..., sl_ceps], y_ref[..., sl_ceps])
            stat_band_reg = stat_band_reg + _band_penalty(y_hat[..., sl_lpc], y_ref[..., sl_lpc])
            stat_band_reg = stat_band_reg + _band_penalty(y_hat[..., sl_fc], y_ref[..., sl_fc])
            # Mean alignment (block-wise)
            def _mean_l2(a, b):
                mu_a = a.mean(dim=(0, 1))
                mu_b = b.mean(dim=(0, 1))
                return torch.nn.functional.mse_loss(mu_a, mu_b)
            mean_match_reg = _mean_l2(y_hat[..., sl_ceps], y_ref[..., sl_ceps])
            mean_match_reg = mean_match_reg + _mean_l2(y_hat[..., sl_lpc], y_ref[..., sl_lpc])
            mean_match_reg = mean_match_reg + _mean_l2(y_hat[..., sl_fc], y_ref[..., sl_fc])
            # FrameCorr domain: encourage |fc| <= 0.5 (target range)
            fc_hat = y_hat[..., sl_fc]
            over = torch.relu(fc_hat.abs() - 0.5)
            fc_range_penalty = (over.pow(2)).mean()
            # Temporal difference matching over all dims
            if y_hat.size(1) > 1 and y_ref.size(1) > 1:
                dy_hat = y_hat[:, 1:, :] - y_hat[:, :-1, :]
                dy_ref = y_ref[:, 1:, :] - y_ref[:, :-1, :]
                tv_loss = torch.nn.functional.mse_loss(dy_hat, dy_ref)
        except Exception:
            stat_band_reg = torch.tensor(0.0, device=device)
            tv_loss = torch.tensor(0.0, device=device)
            mean_match_reg = torch.tensor(0.0, device=device)
            fc_range_penalty = torch.tensor(0.0, device=device)

        lambda_whiten = float(self.config.get('lambda_whiten', 0.5))
        lambda_logstd = float(self.config.get('lambda_logstd', 0.2))

        total_loss = (weights['feat'] * feat_recon_loss
                      + weights['wave'] * wave_loss
                      + weights['semantic'] * semantic_loss
                      + weights['quality'] * quality_loss
                      + weights['commitment'] * commitment_loss
                      + vib_loss  # ç›´æ¥ç›¸åŠ ï¼Œä¸å†ä¹˜weights['vib']
                      + rate_loss
                      + lambda_calib * calib_reg
                      + diversity_loss
                      + mask_sparsity_penalty
                      + lambda_whiten * feat_whiten_mse
                      + lambda_logstd * logstd_mse
                      + lambda_stat * stat_band_reg
                      + lambda_tv * tv_loss
                      + lambda_mean * mean_match_reg
                      + lambda_fc_range * fc_range_penalty
                      + gate_l0_penalty
                      + gate_kl_penalty)

        # === 11. é¢å¤–çš„ç¨³å®šæ€§çº¦æŸ ===
        if self.config.get('enable_stability_loss', True):
            stability_loss = self.compute_stability_loss(model_outputs, current_step)
            total_loss += 0.1 * stability_loss
        else:
            stability_loss = torch.tensor(0.0, device=device)

        # === 12. æŸå¤±è¯¦æƒ…ï¼ˆç®€åŒ–ç‰ˆï¼‰===
        loss_details = {
            'total': total_loss.item(),
            # ç»¼åˆç‰¹å¾è¯¯å·®ï¼ˆæ›´è´´è¿‘å¬æ„Ÿï¼‰ï¼šL1(raw) + è¾…åŠ©é¡¹
            'feat': float((feat_recon_loss
                           + lambda_whiten * (feat_whiten_mse if torch.is_tensor(feat_whiten_mse) else 0.0)
                           + lambda_logstd * (logstd_mse if torch.is_tensor(logstd_mse) else 0.0)
                           + lambda_stat   * (stat_band_reg if torch.is_tensor(stat_band_reg) else 0.0)
                           + lambda_tv     * (tv_loss if torch.is_tensor(tv_loss) else 0.0)).item() if torch.is_tensor(feat_recon_loss) else 0.0),
            'feat_recon': feat_recon_loss.item(),
            'feat_whiten_mse': feat_whiten_mse.item() if torch.is_tensor(feat_whiten_mse) else 0.0,
            'logstd_mse': logstd_mse.item() if torch.is_tensor(logstd_mse) else 0.0,
            'stat_band': stat_band_reg.item() if torch.is_tensor(stat_band_reg) else 0.0,
            'tv_loss': tv_loss.item() if torch.is_tensor(tv_loss) else 0.0,
            'mean_match': mean_match_reg.item() if torch.is_tensor(mean_match_reg) else 0.0,
            'fc_range': fc_range_penalty.item() if torch.is_tensor(fc_range_penalty) else 0.0,
            'gate_l0': float(gate_l0_penalty.item()) if torch.is_tensor(gate_l0_penalty) else 0.0,
            'gate_kl': float(gate_kl_penalty.item()) if torch.is_tensor(gate_kl_penalty) else 0.0,
            'wave': wave_loss.item(),
            'wave_mrstft': float(wave_details.get('mrstft', 0.0)) if isinstance(wave_details, dict) else 0.0,
            'wave_l1': float(wave_details.get('l1', 0.0)) if isinstance(wave_details, dict) else 0.0,
            'sisdr_db': float(wave_details.get('sisdr_db', 0.0)) if isinstance(wave_details, dict) else 0.0,
            'semantic': semantic_loss.item(),
            'semantic_loss': semantic_loss.item(),  # å…¼å®¹æ€§
            'semantic_mode': semantic_mode,
            'quality': quality_loss.item(),
            'commitment': commitment_loss.item(),
            'vib_kld': float(vib_kld.item()) if vib_kld is not None else 0.0,
            'vib_beta': beta_vib,
            'diversity': diversity_loss.item(),
            'calib_reg': float(calib_reg.item()) if torch.is_tensor(calib_reg) else 0.0,
            'mask_sparsity': float(mask_sparsity_penalty.item()) if torch.is_tensor(mask_sparsity_penalty) else 0.0,
            'eff_bits_per_frame': float(eff_bits_per_frame.item()),
            'rate_bits_per_frame': float(eff_bits_per_frame.item()),  # ä¿®å¤ï¼šä¾›æ—¥å¿—ä½¿ç”¨
            'nom_bits_per_frame': float(nom_bits_per_frame.item()),
            'kbps_eff': float(kbps_eff.item()) if isinstance(kbps_eff, torch.Tensor) else float(kbps_eff),
            'rate_loss': float(rate_loss.item()),
            'current_kbps': float(kbps_eff) if 'kbps_eff' in locals() else target_kbps,
            'lambda_rate': float(self._lambda_rate),
            'loss_weights': weights,
            # æ–°å¢ï¼šRVQç³»ç»ŸçŠ¶æ€
            'compression_ready': model_outputs.get('compression_ready', False),
            'using_real_rate': model_outputs.get('compression_ready', False),
            # æ–°å¢ï¼šå¤šä»»åŠ¡æ¢¯åº¦å¹³è¡¡çŠ¶æ€
            'gradient_balancing_active': self.is_gradient_aware,
            'weight_adaptation_rate': getattr(self.weight_scheduler, 'adaptation_rate', 0.0),
            'gradient_balance_alpha': self.config.get('gradient_balance_alpha', 0.16),
            'initial_loss_ratios_set': hasattr(self, '_initial_loss_ratios')
        }

        # æ›´æ–°æŸå¤±å†å²
        self.update_loss_history(loss_details)

        return total_loss, loss_details

    def _compute_gradient_balanced_weights(
        self,
        model: torch.nn.Module,
        individual_losses: Dict[str, torch.Tensor],
        current_weights: Dict[str, float],
        current_step: int
    ) -> Dict[str, float]:
        """
        åŸºäºStop Gradientçš„é«˜æ•ˆå¤šä»»åŠ¡æƒé‡å¹³è¡¡

        ä¼˜åŠ¿ï¼š
        1. ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œé¿å…å¹²æ‰°ä¸»è®­ç»ƒè¿‡ç¨‹
        2. è®¡ç®—å¼€é”€å°ï¼ŒåŸºäºæŸå¤±å€¼å˜åŒ–è¶‹åŠ¿
        3. æ•°å€¼ç¨³å®šï¼Œä¸ä¾èµ–æ¢¯åº¦èŒƒæ•°
        """
        # é…ç½®å‚æ•°
        adaptation_rate = self.config.get('gradient_adaptation_rate', 0.01)
        smoothing_factor = 0.9  # EMAå¹³æ»‘å› å­

        # åªå¯¹æœ‰æ•ˆæŸå¤±è¿›è¡Œå¹³è¡¡ - åŒ…æ‹¬å°æ•°å€¼çš„VIBå’Œrate loss
        valid_losses = {}
        for k, v in individual_losses.items():
            if v is not None:
                # å¯¹VIBå’Œrateå…è®¸æ›´å°çš„é˜ˆå€¼ï¼ˆå› ä¸ºå®ƒä»¬çš„loss scaleè¾ƒå°ï¼‰
                if k in ['vib', 'rate']:
                    if v.item() > 1e-12:  # æ›´å°çš„é˜ˆå€¼
                        valid_losses[k] = v
                else:
                    if v > 1e-8:  # å…¶ä»–lossçš„æ­£å¸¸é˜ˆå€¼
                        valid_losses[k] = v

        if len(valid_losses) < 2:
            return current_weights

        # ä½¿ç”¨ .detach() é˜»æ­¢æ¢¯åº¦ä¼ æ’­ï¼Œåªè§‚å¯ŸæŸå¤±å€¼
        current_loss_values = {k: v.detach().item() for k, v in valid_losses.items()}

        # åˆå§‹åŒ–æŸå¤±å†å²è®°å½•
        if not hasattr(self, '_loss_history'):
            self._loss_history = {}
            self._loss_ema = {}

        # æ›´æ–°æŸå¤±EMA (æŒ‡æ•°ç§»åŠ¨å¹³å‡)
        for task_name, loss_val in current_loss_values.items():
            if task_name not in self._loss_ema:
                self._loss_ema[task_name] = loss_val
                self._loss_history[task_name] = []
            else:
                # EMAæ›´æ–°
                self._loss_ema[task_name] = (smoothing_factor * self._loss_ema[task_name] +
                                            (1 - smoothing_factor) * loss_val)

            # ä¿ç•™æœ€è¿‘çš„æŸå¤±å†å²ï¼ˆç”¨äºè¶‹åŠ¿åˆ†æï¼‰
            self._loss_history[task_name].append(loss_val)
            if len(self._loss_history[task_name]) > 10:  # åªä¿ç•™æœ€è¿‘10æ­¥
                self._loss_history[task_name].pop(0)

        # å¦‚æœå†å²æ•°æ®ä¸å¤Ÿï¼Œè¿”å›å½“å‰æƒé‡
        if current_step < 20:  # å‰20æ­¥ä¸è°ƒæ•´
            return current_weights

        # è®¡ç®—æŸå¤±å¹³è¡¡å› å­
        updated_weights = current_weights.copy()

        # ç¡®ä¿rateæƒé‡è¢«åŒ…å«ï¼ˆå¦‚æœå®ƒåœ¨valid_lossesä¸­ä½†ä¸åœ¨current_weightsä¸­ï¼‰
        for task_name in valid_losses.keys():
            if task_name not in updated_weights:
                if task_name == 'rate':
                    updated_weights[task_name] = 0.5     # Rateçš„åˆå§‹æƒé‡
                else:
                    updated_weights[task_name] = 0.1     # å…¶ä»–lossçš„é»˜è®¤æƒé‡

        # è®¡ç®—å¹³å‡æŸå¤±EMAç”¨äºå½’ä¸€åŒ–
        avg_loss_ema = sum(self._loss_ema.values()) / len(self._loss_ema)

        for task_name in valid_losses.keys():
            if task_name not in updated_weights:
                continue  # è·³è¿‡æœªåˆå§‹åŒ–çš„æƒé‡

            # å½“å‰ä»»åŠ¡çš„ç›¸å¯¹æŸå¤±å¤§å°
            task_loss_ema = self._loss_ema[task_name]
            relative_loss = task_loss_ema / (avg_loss_ema + 1e-8)

            # æŸå¤±è¶‹åŠ¿ï¼šæœ€è¿‘3æ­¥çš„å¹³å‡ vs å‰é¢3æ­¥çš„å¹³å‡
            if len(self._loss_history[task_name]) >= 6:
                recent_avg = sum(self._loss_history[task_name][-3:]) / 3
                earlier_avg = sum(self._loss_history[task_name][-6:-3]) / 3
                trend = (recent_avg - earlier_avg) / (earlier_avg + 1e-8)
            else:
                trend = 0.0

            # æƒé‡è°ƒæ•´é€»è¾‘ï¼š
            # 1. å¦‚æœæŸå¤±ç›¸å¯¹è¾ƒå¤§ä¸”è¿˜åœ¨ä¸Šå‡ï¼Œå¢åŠ æƒé‡
            # 2. å¦‚æœæŸå¤±ç›¸å¯¹è¾ƒå°ä¸”åœ¨ä¸‹é™ï¼Œå‡å°‘æƒé‡
            current_w = updated_weights[task_name]

            # åŸºäºç›¸å¯¹æŸå¤±å¤§å°çš„è°ƒæ•´
            if relative_loss > 1.5:  # æŸå¤±æ˜æ˜¾å¤§äºå¹³å‡å€¼
                weight_factor = 1.0 + adaptation_rate
            elif relative_loss < 0.7:  # æŸå¤±æ˜æ˜¾å°äºå¹³å‡å€¼
                weight_factor = 1.0 - adaptation_rate * 0.5
            else:
                weight_factor = 1.0

            # åŸºäºè¶‹åŠ¿çš„å¾®è°ƒ
            if abs(trend) > 0.1:  # è¶‹åŠ¿æ˜æ˜¾
                if trend > 0:  # æŸå¤±ä¸Šå‡ï¼Œå¢åŠ æƒé‡
                    weight_factor *= (1.0 + adaptation_rate * 0.5)
                else:  # æŸå¤±ä¸‹é™ï¼Œç•¥å‡æƒé‡
                    weight_factor *= (1.0 - adaptation_rate * 0.3)

            # å¹³æ»‘æ›´æ–°æƒé‡
            new_weight = current_w * weight_factor

            # æƒé‡çº¦æŸ
            new_weight = max(0.01, min(5.0, new_weight))
            updated_weights[task_name] = new_weight

        # æƒé‡å½’ä¸€åŒ–ï¼ˆä¿æŒæ€»å’Œç¨³å®šï¼‰ - ä¿®å¤ï¼šåªå¯¹åŸæœ‰æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # åˆ†ç¦»åŸæœ‰æƒé‡å’Œæ–°å¢æƒé‡
        original_keys = {'feat', 'wave', 'semantic', 'quality', 'commitment'}
        new_keys = {'rate'}

        original_weights = {k: v for k, v in updated_weights.items() if k in original_keys}
        new_weights = {k: v for k, v in updated_weights.items() if k in new_keys}

        # åªå¯¹åŸæœ‰æƒé‡è¿›è¡Œå½’ä¸€åŒ–
        original_total = sum(original_weights.values())
        target_original = sum(current_weights[k] for k in original_keys if k in current_weights)

        # è°ƒè¯•ï¼šæ£€æŸ¥å½’ä¸€åŒ–å‰åçš„æƒé‡
        if current_step % 100 == 0:
            print(f"Before normalization: rate={new_weights.get('rate', 0):.6f}")
            print(f"Original weights total: {original_total:.4f} -> target: {target_original:.4f}")

        if original_total > 0 and target_original > 0:
            norm_factor = target_original / original_total
            for k in original_keys:
                if k in updated_weights:
                    updated_weights[k] *= norm_factor

        # æ–°æƒé‡ä¸å½’ä¸€åŒ–ï¼Œä¿æŒåŠ¨æ€è°ƒæ•´çš„æ•ˆæœ
        if current_step % 100 == 0:
            print(f"After normalization: rate={updated_weights.get('rate', 0):.6f} (not normalized)")

        # æ—¥å¿—è®°å½•ï¼ˆæ¯100æ­¥ï¼‰
        if current_step % 100 == 0:
            print("\n=== Multi-task Loss-based Balancing (Stop Gradient) ===")
            for task_name in valid_losses.keys():
                loss_ema = self._loss_ema.get(task_name, 0)
                relative = loss_ema / (avg_loss_ema + 1e-8)
                trend = 0.0
                if len(self._loss_history.get(task_name, [])) >= 6:
                    recent = sum(self._loss_history[task_name][-3:]) / 3
                    earlier = sum(self._loss_history[task_name][-6:-3]) / 3
                    trend = (recent - earlier) / (earlier + 1e-8)

                print(f"  {task_name}: loss_ema={loss_ema:.3f} relative={relative:.2f} trend={trend:+.3f}")

        return updated_weights

    def _compute_stage_aware_weights(
        self,
        model: torch.nn.Module,
        individual_losses: Dict[str, torch.Tensor],
        current_weights: Dict[str, float],
        current_step: int
    ) -> Dict[str, float]:
        """
        é˜¶æ®µæ„ŸçŸ¥çš„æƒé‡è°ƒæ•´ï¼šç»“åˆè®­ç»ƒé˜¶æ®µç­–ç•¥å’Œæ¢¯åº¦å¹³è¡¡

        ç­–ç•¥ï¼š
        1. è·å–å½“å‰é˜¶æ®µçš„ç›®æ ‡æƒé‡ï¼ˆæ¥è‡ªæ¨¡å‹çš„get_dynamic_loss_weightsï¼‰
        2. ä½¿ç”¨æ¢¯åº¦å¹³è¡¡è¿›è¡Œå¾®è°ƒï¼Œä½†é™åˆ¶åœ¨é˜¶æ®µæƒé‡èŒƒå›´å†…
        3. ç¡®ä¿ä¸è¿èƒŒé˜¶æ®µæ€§è®­ç»ƒç­–ç•¥
        """
        # 1. è·å–é˜¶æ®µç›®æ ‡æƒé‡
        if hasattr(model, 'get_dynamic_loss_weights'):
            stage_weights = model.get_dynamic_loss_weights(current_step)
        else:
            stage_weights = current_weights

        # 2. åŸºäºé˜¶æ®µæƒé‡çš„æ¢¯åº¦å¹³è¡¡å¾®è°ƒ
        balanced_weights = self._compute_gradient_balanced_weights(
            model, individual_losses, stage_weights, current_step
        )

        # 3. çº¦æŸè°ƒæ•´å¹…åº¦ï¼Œé¿å…åç¦»é˜¶æ®µç­–ç•¥å¤ªè¿œ
        final_weights = {}

        # åˆå¹¶æ‰€æœ‰éœ€è¦å¤„ç†çš„æƒé‡é”®ï¼ˆcurrent_weights + stage_weightsï¼‰
        all_keys = set(current_weights.keys()) | set(stage_weights.keys())

        for key in all_keys:
            current_value = current_weights.get(key, 0.0)
            stage_target = stage_weights.get(key, current_value)
            balanced_value = balanced_weights.get(key, stage_target)

            # é™åˆ¶åç¦»é˜¶æ®µç›®æ ‡çš„å¹…åº¦ï¼Œä¸ºè¯­ä¹‰æŸå¤±æä¾›ç‰¹æ®Šä¿æŠ¤
            if key == 'semantic':
                # è¯­ä¹‰æŸå¤±çš„æƒé‡ä¸å…è®¸è¢«é™ä½å¤ªå¤šï¼Œé˜²æ­¢è¯­ä¹‰å­¦ä¹ å´©æºƒ
                max_deviation = 0.3  # è¯­ä¹‰æƒé‡æœ€å¤§åç¦»30%
                min_weight = stage_target * 0.8  # è¯­ä¹‰æƒé‡ä¸èƒ½ä½äºé˜¶æ®µç›®æ ‡çš„80%
                max_weight = stage_target * (1 + max_deviation)
            elif key == 'wave':
                # waveæƒé‡ä¸å…è®¸è¿‡åº¦å¢åŠ ï¼Œé˜²æ­¢æŠ¢å¤ºå…¶ä»–ä»»åŠ¡çš„å­¦ä¹ 
                max_deviation = 0.4  # waveæƒé‡æœ€å¤§åç¦»40%
                min_weight = stage_target * (1 - max_deviation)
                max_weight = stage_target * 1.3  # waveæƒé‡ä¸èƒ½è¶…è¿‡é˜¶æ®µç›®æ ‡çš„130%
            else:
                # å…¶ä»–æƒé‡çš„æ ‡å‡†çº¦æŸ
                max_deviation = 0.5  # æœ€å¤§åç¦»50%
                min_weight = stage_target * (1 - max_deviation)
                max_weight = stage_target * (1 + max_deviation)

            final_weights[key] = max(min_weight, min(max_weight, balanced_value))

        # 4. è°ƒè¯•è¾“å‡ºï¼ˆæ¯100æ­¥ï¼‰
        if current_step % 100 == 0:
            print(f"\n=== Stage-Aware Weight Adjustment (Step {current_step}) ===")
            for key in ['feat', 'wave', 'semantic', 'commitment', 'rate', 'quality']:
                if key in stage_weights and key in final_weights:
                    stage_w = stage_weights[key]
                    final_w = final_weights[key]
                    print(f"  {key}: stage={stage_w:.3f} â†’ final={final_w:.3f}")

        return final_weights

    def compute_stability_loss(
        self,
        model_outputs: Dict[str, torch.Tensor],
        current_step: int
    ) -> torch.Tensor:
        """è®¡ç®—è®­ç»ƒç¨³å®šæ€§æŸå¤±"""
        device = model_outputs['quantized_latent'].device

        # 1. RVQ perplexityç¨³å®šæ€§
        if 'rvq_details' in model_outputs:
            rvq_details = model_outputs['rvq_details']
            if rvq_details.get('stage_perplexities') is not None:
                perplexities_data = rvq_details.get('stage_perplexities')

                # ä¿®å¤ï¼šå®‰å…¨å¤„ç†perplexitiesç±»å‹
                if isinstance(perplexities_data, list):
                    if len(perplexities_data) > 0:
                        perplexities = torch.tensor(perplexities_data, device=device)
                    else:
                        perplexities = []
                elif torch.is_tensor(perplexities_data):
                    perplexities = perplexities_data
                else:
                    perplexities = []

                if len(perplexities) > 0:
                    # æœŸæœ›æ¯ä¸ªé˜¶æ®µéƒ½æœ‰åˆç†çš„perplexity(é¿å…codebook collapse)
                    target_perplexities = torch.tensor([64.0, 32.0, 16.0], device=device)[:len(perplexities)]
                    perplexity_loss = sum([
                        F.mse_loss(p.unsqueeze(0), target.unsqueeze(0))
                        for p, target in zip(perplexities, target_perplexities)
                    ]) / len(perplexities)
                else:
                    perplexity_loss = torch.tensor(0.0, device=device)
            else:
                perplexity_loss = torch.tensor(0.0, device=device)
        else:
            perplexity_loss = torch.tensor(0.0, device=device)

        # 2. ç ç‡æ–¹å·®çº¦æŸ(é˜²æ­¢ç ç‡éœ‡è¡)
        rate_stats = model_outputs.get('rate_stats', {})
        if 'std_kbps' in rate_stats and rate_stats['std_kbps'] > 0:
            rate_variance_penalty = torch.tensor(rate_stats['std_kbps'], device=device).clamp(min=0, max=1.0)
        else:
            rate_variance_penalty = torch.tensor(0.0, device=device)

        return perplexity_loss + 0.1 * rate_variance_penalty

    def update_loss_history(self, loss_details: Dict[str, float]):
        """æ›´æ–°æŸå¤±å†å²ç”¨äºç›‘æ§"""
        self.loss_history['rate'].append(loss_details['current_kbps'])
        self.loss_history['quality'].append(loss_details['wave'])
        self.loss_history['stability'].append(loss_details['total'])

        # ä¿æŒå†å²é•¿åº¦
        max_history = 100
        for key in self.loss_history:
            if len(self.loss_history[key]) > max_history:
                self.loss_history[key] = self.loss_history[key][-max_history:]

    def get_training_diagnostics(self) -> Dict[str, float]:
        """è·å–è®­ç»ƒè¯Šæ–­ä¿¡æ¯"""
        if not self.loss_history['rate']:
            return {}

        import numpy as np

        recent_rates = self.loss_history['rate'][-20:]  # æœ€è¿‘20æ­¥
        recent_quality = self.loss_history['quality'][-20:]
        recent_stability = self.loss_history['stability'][-20:]

        return {
            'avg_rate_kbps': np.mean(recent_rates),
            'rate_std': np.std(recent_rates),
            'rate_in_range': np.mean([1.1 <= r <= 1.3 for r in recent_rates]),
            'avg_quality': np.mean(recent_quality),
            'stability_trend': np.mean(np.diff(recent_stability)) if len(recent_stability) > 1 else 0.0
        }

# === ä¾¿åˆ©å‡½æ•° ===

def create_stage5_loss_computer(config: Dict) -> Stage5ComprehensiveLoss:
    """åˆ›å»ºStage5æŸå¤±è®¡ç®—å™¨"""
    return Stage5ComprehensiveLoss(config)


if __name__ == "__main__":
    # æµ‹è¯•æŸå¤±å‡½æ•°
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, feature_dim, seq_len = 2, 24, 100

    model_outputs = {
        'quantized_features': torch.randn(batch_size, feature_dim, seq_len, device=device),
        'reconstructed_features': torch.randn(batch_size, 36, seq_len, device=device),
        'synthesized_audio': torch.randn(batch_size, 1, seq_len * 160, device=device),
        'semantic_features': torch.randn(batch_size, 36, seq_len, device=device),
        'quality_prediction': torch.randn(batch_size, 3, device=device),
        'rate_bits_per_frame': torch.tensor(24.0, device=device),
        'rate_loss': torch.tensor(0.1, device=device),
        'commitment_loss': torch.tensor(0.05, device=device),
        'rate_stats': {'mean_kbps': 1.2, 'std_kbps': 0.05, 'in_range_ratio': 0.95}
    }

    targets = {
        'original_features': torch.randn(batch_size, 36, seq_len, device=device),
        'target_audio': torch.randn(batch_size, 1, seq_len * 160, device=device),
        'period': torch.randint(20, 200, (batch_size, seq_len), device=device)
    }

    # æµ‹è¯•æŸå¤±è®¡ç®—
    config = {
        'total_steps': 8000,
        'rate_warmup_steps': 500,
        'frame_rate': 50,
        'temporal_smoothness': 0.1,
        'max_jump_threshold': 2.0,
        'min_quality_threshold': 2.5,
        'enable_stability_loss': True
    }

    loss_computer = create_stage5_loss_computer(config)

    for step in [100, 1500, 4000]:  # æµ‹è¯•ä¸åŒé˜¶æ®µ
        total_loss, details = loss_computer.compute_comprehensive_loss(
            model_outputs, targets, step
        )

        print(f"\nStep {step}:")
        print(f"  Total loss: {total_loss.item():.4f}")
        print(f"  Current kbps: {details['current_kbps']:.3f}")
        print(f"  Rate warmup factor: {details['rate_warmup_factor']:.3f}")
        print(f"  Loss weights: feat={details['loss_weights']['feat']:.2f}, "
              f"wave={details['loss_weights']['wave']:.2f}, "
              f"rate={details['loss_weights']['rate']:.2f}")

    # æµ‹è¯•è®­ç»ƒè¯Šæ–­
    print(f"\nTraining diagnostics:")
    diagnostics = loss_computer.get_training_diagnostics()
    for k, v in diagnostics.items():
        print(f"  {k}: {v:.4f}")
