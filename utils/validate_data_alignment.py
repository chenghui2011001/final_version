#!/usr/bin/env python3
"""
æ•°æ®å¯¹é½éªŒè¯è„šæœ¬
æ£€éªŒç‰¹å¾æ–‡ä»¶(.f32)ä¸éŸ³é¢‘æ–‡ä»¶(.pcm)çš„ä¸¥æ ¼10mså¸§ç‡å¯¹é½
"""

import numpy as np
import torch
import argparse
from pathlib import Path
from feature_spec import get_default_feature_spec


def validate_alignment(features_path: str, audio_path: str, frame_size: int = 160):
    """éªŒè¯ç‰¹å¾æ–‡ä»¶ä¸éŸ³é¢‘æ–‡ä»¶çš„å¯¹é½"""
    print("ğŸ” éªŒè¯æ•°æ®å¯¹é½...")

    features_path = Path(features_path)
    audio_path = Path(audio_path)

    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    assert features_path.exists(), f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {features_path}"
    assert audio_path.exists(), f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}"

    # åŠ è½½ç‰¹å¾
    spec = get_default_feature_spec()
    features_data = np.fromfile(features_path, dtype=np.float32)
    feature_frames = len(features_data) // spec.total_dim
    features = features_data.reshape(-1, spec.total_dim)

    print(f"ğŸ“Š ç‰¹å¾æ–‡ä»¶: {features_path.name}")
    print(f"  ç‰¹å¾ç»´åº¦: {spec.total_dim}")
    print(f"  ç‰¹å¾å¸§æ•°: {feature_frames:,}")
    print(f"  æ–‡ä»¶å¤§å°: {features_path.stat().st_size / (1024**2):.1f} MB")

    # åŠ è½½éŸ³é¢‘
    audio_data = np.fromfile(audio_path, dtype=np.int16).astype(np.float32) / 32768.0
    audio_samples = len(audio_data)
    audio_frames = audio_samples // frame_size

    print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path.name}")
    print(f"  é‡‡æ ·ç‚¹æ•°: {audio_samples:,}")
    print(f"  éŸ³é¢‘å¸§æ•°: {audio_frames:,} (åŸºäº{frame_size}æ ·æœ¬/å¸§)")
    print(f"  æ—¶é•¿: {audio_samples/16000:.1f}ç§’")
    print(f"  æ–‡ä»¶å¤§å°: {audio_path.stat().st_size / (1024**2):.1f} MB")

    # å¯¹é½æ£€æŸ¥
    print(f"\nâš–ï¸ å¯¹é½æ£€æŸ¥:")
    frame_diff = abs(feature_frames - audio_frames)
    print(f"  å¸§æ•°å·®å¼‚: {frame_diff}")

    if frame_diff == 0:
        print("  âœ… å®Œç¾å¯¹é½ï¼")
        alignment_status = "perfect"
    elif frame_diff <= 1:
        print("  âœ… åŸºæœ¬å¯¹é½ (å·®å¼‚åœ¨1å¸§å†…ï¼Œå¯æ¥å—)")
        alignment_status = "good"
    elif frame_diff <= 5:
        print("  âš ï¸ è½»å¾®ä¸å¯¹é½ (å·®å¼‚åœ¨5å¸§å†…ï¼Œå¯æ¥å—)")
        alignment_status = "acceptable"
    elif frame_diff <= 100:
        print("  âš ï¸ ä¸­ç­‰ä¸å¯¹é½ï¼Œä½†å¯èƒ½å¯ç”¨äºè®­ç»ƒ")
        alignment_status = "marginal"
    else:
        print("  âŒ ä¸¥é‡ä¸å¯¹é½ï¼")
        alignment_status = "bad"

    # æ—¶é—´å¯¹é½æ£€æŸ¥
    feature_duration = feature_frames * 10  # ms
    audio_duration = audio_frames * 10      # ms
    time_diff = abs(feature_duration - audio_duration)

    print(f"  ç‰¹å¾æ—¶é•¿: {feature_duration:.1f}ms ({feature_frames}å¸§)")
    print(f"  éŸ³é¢‘æ—¶é•¿: {audio_duration:.1f}ms ({audio_frames}å¸§)")
    print(f"  æ—¶é•¿å·®å¼‚: {time_diff:.1f}ms")

    # ç‰¹å¾è´¨é‡æ£€æŸ¥
    print(f"\nğŸ§ª ç‰¹å¾è´¨é‡æ£€æŸ¥:")

    # ä½¿ç”¨FeatureSpecæ£€æŸ¥å„éƒ¨åˆ†ç‰¹å¾
    # è½¬æ¢ä¸ºtensorä»¥ç¡®ä¿æ­£ç¡®æå–
    features_tensor = torch.from_numpy(features[:1000])
    f0 = spec.extract_feature(features_tensor, 'f0').numpy()
    voicing = spec.extract_feature(features_tensor, 'voicing').numpy()
    ceps = spec.extract_feature(features_tensor, 'ceps').numpy()

    print(f"  F0èŒƒå›´: [{f0.min():.3f}, {f0.max():.3f}], å‡å€¼: {f0.mean():.3f}")

    # æ­£ç¡®æ˜¾ç¤ºvoicingå€¼
    unique_voicing, counts = np.unique(voicing.flatten(), return_counts=True)
    print(f"  Voicingåˆ†å¸ƒ: å…±{len(unique_voicing)}ç§å€¼")
    for val, count in zip(unique_voicing, counts):
        print(f"    {val}: {count}æ¬¡ ({count/len(voicing.flatten())*100:.1f}%)")

    print(f"  CEPSèŒƒå›´: [{ceps.min():.3f}, {ceps.max():.3f}], æ ‡å‡†å·®: {ceps.std():.3f}")

    # é™éŸ³æ£€æµ‹
    audio_rms = np.sqrt(np.mean(audio_data**2))
    print(f"  éŸ³é¢‘RMS: {audio_rms:.6f} ({20*np.log10(audio_rms+1e-12):.1f} dB)")

    if audio_rms < 1e-5:
        print("  âš ï¸ éŸ³é¢‘èƒ½é‡è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨é™éŸ³æ®µ")

    return {
        'alignment_status': alignment_status,
        'frame_diff': frame_diff,
        'time_diff_ms': time_diff,
        'feature_frames': feature_frames,
        'audio_frames': audio_frames,
        'audio_rms': audio_rms
    }


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯ç‰¹å¾ä¸éŸ³é¢‘çš„å¯¹é½')
    parser.add_argument('--features', type=str, required=True, help='ç‰¹å¾æ–‡ä»¶è·¯å¾„(.f32)')
    parser.add_argument('--audio', type=str, required=True, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„(.pcm)')
    parser.add_argument('--frame-size', type=int, default=160, help='å¸§å¤§å°(æ ·æœ¬æ•°)')

    args = parser.parse_args()

    print("ğŸ§ª æ•°æ®å¯¹é½éªŒè¯")
    print("=" * 50)

    try:
        result = validate_alignment(args.features, args.audio, args.frame_size)

        print("=" * 50)
        if result['alignment_status'] in ['perfect', 'good']:
            print("ğŸ‰ éªŒè¯é€šè¿‡ï¼æ•°æ®å¯¹é½è‰¯å¥½")
        elif result['alignment_status'] == 'acceptable':
            print("âš ï¸ éªŒè¯é€šè¿‡ï¼Œä½†å­˜åœ¨è½»å¾®å¯¹é½é—®é¢˜")
        else:
            print("âŒ éªŒè¯å¤±è´¥ï¼Œå­˜åœ¨ä¸¥é‡å¯¹é½é—®é¢˜")

        return result['alignment_status'] in ['perfect', 'good', 'acceptable']

    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    main()