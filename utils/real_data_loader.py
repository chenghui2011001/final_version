#!/usr/bin/env python3
"""
Real Data Loader for AETHER Training
çœŸå®æ•°æ®åŠ è½½å™¨ - å‚è€ƒFarGanSOTAè®¾è®¡ï¼Œç»„ä»¶åŒ–æ•°æ®åŠ è½½
ä½¿ç”¨FeatureSpecé…ç½®å¯¹è±¡ï¼Œä¸å†ä¾èµ–ç¡¬ç¼–ç ç‰¹å¾åˆ‡ç‰‡
"""

import os
import json
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, Optional, Dict, List
from pathlib import Path
from .feature_spec import get_default_feature_spec, FeatureSpec
try:
    from ..models.feature_adapter import get_fargan_feature_spec
except ImportError:  # pragma: no cover
    from models.feature_adapter import get_fargan_feature_spec


class AETHERRealDataset(Dataset):
    """
    AETHERçœŸå®æ•°æ®é›†åŠ è½½å™¨
    æ”¯æŒå¤§è§„æ¨¡æ•°æ®åŠ è½½å’Œèƒ½é‡é€‰æ‹©
    ä½¿ç”¨FeatureSpecé…ç½®ï¼Œä¸ä¾èµ–ç¡¬ç¼–ç ç‰¹å¾ç´¢å¼•
    """

    def __init__(self,
                 data_dir: str,
                 sequence_length: int = 800,  # 8ç§’éŸ³é¢‘
                 frame_size: int = 160,
                 max_samples: Optional[int] = None,
                 stride: int = 400,
                 energy_selection: bool = True,
                 feature_spec: Optional[FeatureSpec] = None,
                 features_file: Optional[str] = None,
                 audio_file: Optional[str] = None,
                 validation_split: float = 0.0,
                 split_mode: str = "train"):
        """
        Args:
            data_dir: æ•°æ®ç›®å½•
            sequence_length: åºåˆ—é•¿åº¦(å¸§)
            frame_size: æ¯å¸§éŸ³é¢‘æ ·æœ¬æ•°
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            stride: åºåˆ—é—´éš”
            energy_selection: æ˜¯å¦åŸºäºèƒ½é‡é€‰æ‹©
            feature_spec: ç‰¹å¾è§„èŒƒé…ç½®ï¼Œé»˜è®¤ä½¿ç”¨48ç»´æ ‡å‡†é…ç½®
        """

        self.sequence_length = sequence_length
        self.frame_size = frame_size
        self.stride = stride
        self.energy_selection = energy_selection
        self.feature_spec = feature_spec or get_default_feature_spec()
        self.validation_split = validation_split
        self.split_mode = split_mode

        # æ–‡ä»¶è·¯å¾„
        self.data_dir = Path(data_dir)

        # ä½¿ç”¨ç›´æ¥æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„ï¼Œå¦åˆ™å›é€€åˆ°é»˜è®¤è·¯å¾„ç­–ç•¥
        if features_file is not None:
            features_path = Path(features_file)
        else:
            # æ ¹æ®ç‰¹å¾è§„èŒƒé€‰æ‹©å¯¹åº”çš„ç‰¹å¾æ–‡ä»¶
            if self.feature_spec.total_dim == 36:
                features_path = self.data_dir / "lmr_export" / "features_36_fargan_baseline.f32"
            else:
                features_path = self.data_dir / "lmr_export" / "features_48_complete.f32"

        if audio_file is not None:
            audio_path = Path(audio_file)
        else:
            audio_path = self.data_dir / "out_speech.pcm"

        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        for path, name in [(features_path, "features"), (audio_path, "audio")]:
            if not path.exists():
                raise FileNotFoundError(f"{name} file not found: {path}")

        print(f"Loading real data from {data_dir}...")

        # åŠ è½½ç‰¹å¾ - ä½¿ç”¨å†…å­˜æ˜ å°„ï¼Œé¿å…å°†å…¨é‡æ•°æ®è½½å…¥å†…å­˜
        feature_dim = self.feature_spec.total_dim
        self._features_mmap = np.memmap(str(features_path), dtype=np.float32, mode='r')
        total_features = self._features_mmap.size // feature_dim
        self.features = self._features_mmap.reshape(-1, feature_dim)
        print(f"  Loaded features: {self.features.shape} (ä½¿ç”¨{feature_dim}ç»´ç‰¹å¾è§„èŒƒ)")

        # åŠ è½½éŸ³é¢‘ï¼ˆint16ï¼‰- ä½¿ç”¨å†…å­˜æ˜ å°„ï¼ŒæŒ‰éœ€è½¬æ¢ä¸ºfloat
        self._audio_mmap = np.memmap(str(audio_path), dtype=np.int16, mode='r')
        self.audio = self._audio_mmap  # å»¶è¿Ÿè½¬æ¢ä¸ºfloat32ï¼ˆåœ¨__getitem__æ—¶ï¼‰
        print(f"  Loaded audio: {self.audio.shape} ({len(self.audio)/16000/3600:.1f}h)")

        # ä¸¥æ ¼éªŒè¯10mså¸§ç‡å¯¹é½ (16kHzé‡‡æ ·ç‡ä¸‹ï¼Œ160æ ·æœ¬=10ms)
        audio_frames = len(self.audio) // self.frame_size
        feature_frames = len(self.features)

        print(f"  éŸ³é¢‘å¸§æ•°: {audio_frames:,} (åŸºäº{self.frame_size}æ ·æœ¬/å¸§)")
        print(f"  ç‰¹å¾å¸§æ•°: {feature_frames:,}")

        # ç¡®ä¿å¸§ç‡å¯¹é½
        if abs(audio_frames - feature_frames) > 1:
            print(f"  âš ï¸ å¸§æ•°ä¸åŒ¹é…è¾ƒå¤§: éŸ³é¢‘{audio_frames} vs ç‰¹å¾{feature_frames}")

        min_frames = min(audio_frames, feature_frames)
        print(f"  å¯¹é½åˆ°: {min_frames:,} å¸§ ({min_frames*10:.1f}ms)")

        # ä¸¥æ ¼è£å‰ªç¡®ä¿å¯¹é½
        self.features = self.features[:min_frames]
        audio_samples = min_frames * self.frame_size
        self.audio = self.audio[:audio_samples]

        # éªŒè¯æœ€ç»ˆå¯¹é½
        final_audio_frames = len(self.audio) // self.frame_size
        final_feature_frames = len(self.features)
        assert final_audio_frames == final_feature_frames, \
            f"å¯¹é½åä»ä¸åŒ¹é…: éŸ³é¢‘{final_audio_frames} vs ç‰¹å¾{final_feature_frames}"

        # è®¡ç®—æœ‰æ•ˆåºåˆ—ä½ç½®
        valid_start = 2  # è·³è¿‡å‰2å¸§
        valid_end = min_frames - sequence_length - 2

        if valid_end <= valid_start:
            raise ValueError(f"æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºé•¿åº¦ä¸º{sequence_length}çš„åºåˆ—")

        # ç”Ÿæˆåºåˆ—èµ·å§‹ä½ç½®
        if energy_selection and sequence_length >= 400:  # é•¿åºåˆ—ä½¿ç”¨èƒ½é‡é€‰æ‹©
            all_positions = self._select_high_energy_positions(
                valid_start, valid_end, stride, max_samples
            )
        else:
            # çŸ­åºåˆ—ä½¿ç”¨å‡åŒ€æ­¥é•¿
            all_positions = list(range(valid_start, valid_end, stride))
            if max_samples is not None and len(all_positions) > max_samples:
                # å‡åŒ€é‡‡æ ·è€Œä¸æ˜¯æˆªæ–­
                indices = np.linspace(0, len(all_positions)-1, max_samples, dtype=int)
                all_positions = [all_positions[i] for i in indices]

        # åº”ç”¨è®­ç»ƒ/éªŒè¯åˆ†å‰²ï¼ˆéé‡å ä½ç½®ï¼‰
        if validation_split > 0.0:
            total_positions = len(all_positions)
            val_size = int(total_positions * validation_split)

            # ä½¿ç”¨éé‡å ç­–ç•¥ï¼šéªŒè¯é›†ä½¿ç”¨æ•°æ®çš„æœ€åéƒ¨åˆ†
            if split_mode == "val":
                self.valid_positions = all_positions[-val_size:] if val_size > 0 else []
                split_info = f"validation ({val_size}/{total_positions})"
            else:
                self.valid_positions = all_positions[:-val_size] if val_size > 0 else all_positions
                split_info = f"training ({len(self.valid_positions)}/{total_positions})"

            print(f"  Applied {validation_split:.1%} split -> {split_info}")
        else:
            self.valid_positions = all_positions
            split_info = "no split"

        print(f"  Valid sequences: {len(self.valid_positions):,} (stride={stride}, energy_based={energy_selection and sequence_length>=400}, {split_info})")

        # é¢„è®¡ç®—CSIç¼“å­˜ä»¥ä¼˜åŒ–è¿è¡Œæ—¶æ€§èƒ½ï¼ˆä»…å¯¹ä¸­å°æ•°æ®é›†å¯ç”¨ï¼‰
        if len(self.valid_positions) > 0 and len(self.valid_positions) <= 50000:
            self._precompute_csi_cache()
        else:
            self.csi_cache = {}
            if len(self.valid_positions) > 50000:
                print(f"  âš¡ Skipping CSI precomputation for large dataset ({len(self.valid_positions):,} samples)")
                print(f"  ğŸ“‹ Using on-demand CSI generation for better startup time")

    def _select_high_energy_positions(self, valid_start: int, valid_end: int, stride: int, max_samples: Optional[int]):
        """åŸºäºéŸ³é¢‘èƒ½é‡å¯†åº¦å’Œå¤šæ ·æ€§é€‰æ‹©åºåˆ—ä½ç½®"""
        candidate_positions = list(range(valid_start, valid_end, stride))

        if len(candidate_positions) == 0:
            return candidate_positions

        # å¤§æ•°æ®é›†ä¼˜åŒ–ï¼šå½“å€™é€‰æ•°éå¸¸å¤§æ—¶ï¼Œè·³è¿‡é€æ®µèƒ½é‡è®¡ç®—ï¼Œç›´æ¥è¿”å›å‡åŒ€æ­¥é•¿ä½ç½®
        if len(candidate_positions) > 200_000:
            print(f"    âš¡ Large dataset detected (N={len(candidate_positions):,}), skipping energy scan")
            return candidate_positions

        # è®¡ç®—æ¯ä¸ªå€™é€‰ä½ç½®çš„èƒ½é‡å’Œæ–¹å·®ï¼ˆæŒ‰éœ€å°†éŸ³é¢‘æ®µè½¬ä¸ºfloat32å¹¶å½’ä¸€åŒ–ï¼‰
        energies = []
        variances = []
        for pos in candidate_positions:
            audio_start = pos * self.frame_size
            audio_end = audio_start + self.sequence_length * self.frame_size
            seg_i16 = self.audio[audio_start:audio_end]
            seg = seg_i16.astype(np.float32) / 32768.0 if len(seg_i16) > 0 else seg_i16
            energy = np.sqrt(np.mean(seg ** 2)) if len(seg) > 0 else 0.0
            variance = np.var(seg) if len(seg) > 0 else 0.0
            energies.append(energy)
            variances.append(variance)

        energies = np.asarray(energies, dtype=np.float32)
        variances = np.asarray(variances, dtype=np.float32)

        # æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥ï¼šç»“åˆèƒ½é‡å’Œå¤šæ ·æ€§
        if max_samples is not None and max_samples < len(candidate_positions):
            # 70%é«˜èƒ½é‡æ ·æœ¬ + 30%éšæœºæ ·æœ¬ä¿è¯å¤šæ ·æ€§
            high_energy_count = int(max_samples * 0.7)
            diverse_count = max_samples - high_energy_count

            # é€‰æ‹©é«˜èƒ½é‡æ ·æœ¬
            energy_sorted_indices = np.argsort(energies)[::-1]
            high_energy_indices = energy_sorted_indices[:high_energy_count]

            # ä»å‰©ä½™ä½ç½®ä¸­éšæœºé€‰æ‹©ï¼Œåå‘é«˜æ–¹å·®çš„ä½ç½®
            remaining_indices = energy_sorted_indices[high_energy_count:]
            if len(remaining_indices) > 0:
                # ä½¿ç”¨åŠ æƒéšæœºé‡‡æ ·ï¼Œæ–¹å·®è¶Šå¤§æƒé‡è¶Šå¤§
                remaining_variances = variances[remaining_indices]
                if np.sum(remaining_variances) > 0:
                    weights = remaining_variances / np.sum(remaining_variances)
                    diverse_indices = np.random.choice(
                        remaining_indices,
                        size=min(diverse_count, len(remaining_indices)),
                        replace=False,
                        p=weights
                    )
                else:
                    # å¦‚æœæ–¹å·®éƒ½ä¸º0ï¼Œåˆ™å‡åŒ€éšæœºé‡‡æ ·
                    diverse_indices = np.random.choice(
                        remaining_indices,
                        size=min(diverse_count, len(remaining_indices)),
                        replace=False
                    )
            else:
                diverse_indices = []

            selected_indices = np.concatenate([high_energy_indices, diverse_indices])
        else:
            # å¦‚æœæ²¡æœ‰æ ·æœ¬é™åˆ¶ï¼ŒæŒ‰èƒ½é‡æ’åºè¿”å›æ‰€æœ‰ä½ç½®
            energy_sorted_indices = np.argsort(energies)[::-1]
            selected_indices = energy_sorted_indices

        # è¿”å›æŒ‰å¸§ä½ç½®æ’åºçš„é€‰ä¸­ä½ç½®
        selected_positions = [candidate_positions[i] for i in selected_indices]
        selected_positions.sort()

        print(f"    Selected {len(selected_positions):,}/{len(candidate_positions):,} samples")
        if len(energies) > 0:
            print(f"    Energy range: [{np.min(energies):.4f}, {np.max(energies):.4f}]")
            print(f"    Variance range: [{np.min(variances):.6f}, {np.max(variances):.6f}]")
            if max_samples is not None and max_samples < len(candidate_positions):
                print(f"    Strategy: {int(max_samples * 0.7)} high-energy + {max_samples - int(max_samples * 0.7)} diverse")

        return selected_positions

    def __len__(self):
        return len(self.valid_positions)

    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:
        # ä½RMSç‰‡æ®µè¿‡æ»¤ï¼šè‹¥ç‰‡æ®µèƒ½é‡è¿‡ä½ï¼Œå°è¯•å‘åå¯»æ‰¾å¯ç”¨ç‰‡æ®µï¼ˆæœ€å¤šå°è¯•5æ¬¡ï¼‰
        attempt = 0
        pos_idx = int(idx)
        features = None
        audio = None
        while attempt < 5:
            start_frame = self.valid_positions[pos_idx]
            end_frame = start_frame + self.sequence_length

            # è·å–ç‰¹å¾åºåˆ—
            features = torch.tensor(self.features[start_frame:end_frame], dtype=torch.float32)

            # è·å–å¯¹åº”éŸ³é¢‘ï¼ˆæŒ‰éœ€å°†int16è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ–ï¼‰
            audio_start = start_frame * self.frame_size
            audio_end = audio_start + self.sequence_length * self.frame_size
            seg_i16 = self.audio[audio_start:audio_end]
            seg = seg_i16.astype(np.float32) / 32768.0 if len(seg_i16) > 0 else seg_i16
            audio = torch.from_numpy(seg.copy()).to(dtype=torch.float32)

            # è®¡ç®—RMSï¼Œæ”¾å®½é˜ˆå€¼ä»¥è·å¾—æ›´å¤šè®­ç»ƒæ ·æœ¬
            rms = float(torch.sqrt(torch.mean(audio.pow(2)) + 1e-12).item())
            if rms >= 5e-5:  # é™ä½é˜ˆå€¼ä»1e-4åˆ°5e-5
                break
            attempt += 1
            pos_idx = (pos_idx + 1) % len(self.valid_positions)

        # ä½¿ç”¨é¢„è®¡ç®—çš„CSIç¼“å­˜ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        if idx in self.csi_cache:
            csi_cached = self.csi_cache[idx]
            # åˆ›å»ºfading_onehot
            fading_onehot = torch.zeros(8)
            fading_onehot[csi_cached['fading_type']] = 1.0

            csi_dict = {
                'snr_db': torch.tensor(csi_cached['snr_db'], dtype=torch.float32),
                'ber': torch.tensor(csi_cached['ber'], dtype=torch.float32),
                'fading_onehot': fading_onehot,
            }
        else:
            # å›é€€åˆ°å®æ—¶ç”Ÿæˆï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
            csi_dict = self._generate_csi(audio.numpy(), features, pos_idx)

        return {
            'x': features,
            'y': features.clone(),  # è‡ªç¼–ç ä»»åŠ¡
            'audio': audio,
            'csi': csi_dict,
            'seq_idx': idx
        }

    def _generate_csi(self, audio: np.ndarray, features: torch.Tensor, idx: int) -> Dict[str, torch.Tensor]:
        """ç”Ÿæˆç»Ÿä¸€çš„10ç»´ä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆsnr_db(1) + ber(1) + fading_onehot(8)ï¼‰ã€‚"""
        # åŸºäºéŸ³é¢‘çœŸå®èƒ½é‡è®¡ç®—SNR
        energy = np.mean(audio ** 2)
        snr_base = 10 * np.log10(energy + 1e-10) + 35
        snr_db = np.clip(snr_base + np.random.normal(0, 3), 0, 30)

        # åŸºäºSNRè®¡ç®—BER
        ber = np.clip(10 ** (-snr_db / 15), 1e-5, 0.05)

        # è¡°è½ç±»å‹å¾ªç¯ - ä¿æŒ8ç±»ä»¥åŒ¹é…åŸå§‹é…ç½®
        fading_onehot = np.zeros(8)
        fading_type = (idx // 2000) % 8
        fading_onehot[fading_type] = 1.0

        return {
            'snr_db': torch.tensor(snr_db, dtype=torch.float32),  # [1] ç»´
            'ber': torch.tensor(ber, dtype=torch.float32),        # [1] ç»´
            'fading_onehot': torch.from_numpy(fading_onehot).float(),  # [8] ç»´
        }

    def _precompute_csi_cache(self):
        """é¢„è®¡ç®—CSIç¼“å­˜ä»¥ä¼˜åŒ–æ€§èƒ½"""
        print(f"  Precomputing CSI cache for {len(self.valid_positions):,} samples...")

        # é¢„è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„CSIä¿¡æ¯
        self.csi_cache = {}

        # æ‰¹é‡é¢„è®¡ç®—ä»¥æé«˜æ•ˆç‡
        batch_size = 1000
        for i in range(0, len(self.valid_positions), batch_size):
            end_idx = min(i + batch_size, len(self.valid_positions))

            for j in range(i, end_idx):
                pos_idx = j
                start_frame = self.valid_positions[pos_idx]

                # è·å–éŸ³é¢‘ç‰‡æ®µç”¨äºCSIè®¡ç®—
                audio_start = start_frame * self.frame_size
                audio_end = audio_start + self.sequence_length * self.frame_size
                audio_segment = self.audio[audio_start:audio_end]

                # åŸºäºéŸ³é¢‘èƒ½é‡è®¡ç®—SNRï¼ˆæ— éšæœºå™ªå£°ï¼Œä¿è¯å¯é‡ç°ï¼‰
                energy = np.mean(audio_segment ** 2)
                snr_base = 10 * np.log10(energy + 1e-10) + 35
                snr_db = np.clip(snr_base, 0, 30)  # ç§»é™¤éšæœºå™ªå£°æé«˜æ€§èƒ½

                # åŸºäºSNRè®¡ç®—BER
                ber = np.clip(10 ** (-snr_db / 15), 1e-5, 0.05)

                # è¡°è½ç±»å‹å¾ªç¯
                fading_type = (pos_idx // 2000) % 8

                # ç›´æ¥å­˜å‚¨numpyæ•°ç»„ï¼Œé¿å…é‡å¤tensoråˆ›å»º
                self.csi_cache[pos_idx] = {
                    'snr_db': float(snr_db),
                    'ber': float(ber),
                    'fading_type': int(fading_type)
                }

            if (i + batch_size) % 10000 == 0:
                print(f"    Processed {i + batch_size:,}/{len(self.valid_positions):,} samples")

        print(f"  CSI cache precomputed: {len(self.csi_cache):,} entries")

    def get_info(self) -> dict:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'total_frames': len(self.features),
            'total_samples': len(self.valid_positions),
            'sequence_length': self.sequence_length,
            'feature_dim': self.feature_spec.total_dim,
            'audio_duration_hours': len(self.audio) / 16000 / 3600,
            'frame_size': self.frame_size,
            'sequence_duration_seconds': self.sequence_length * self.frame_size / 16000,
            'feature_spec_info': self.feature_spec.get_feature_info(),
            'validation_split': getattr(self, 'validation_split', 0.0),
            'split_mode': getattr(self, 'split_mode', 'train')
        }


def create_aether_data_loader(
    data_dir: str = "/home/bluestar/FARGAN/opus/data_cn",
    sequence_length: int = 200,  # ä¿®å¤é»˜è®¤å€¼
    batch_size: int = 32,        # ä¿®å¤é»˜è®¤å€¼
    max_samples: Optional[int] = None,
    num_workers: int = 4,
    energy_selection: bool = True,
    test_mode: bool = False,
    feature_spec_type: str = "aether",
    features_file: Optional[str] = None,
    audio_file: Optional[str] = None,
    validation_split: float = 0.0,
    split_mode: str = "train",
    stride_frames: Optional[int] = None,  # æ–°å¢å¯é…ç½®æ­¥å¹…å‚æ•°
    # Optional DataLoader performance knobs
    pin_memory: Optional[bool] = None,
    persistent_workers: Optional[bool] = None,
    prefetch_factor: Optional[int] = None,
) -> Tuple[DataLoader, AETHERRealDataset]:
    """
    åˆ›å»ºAETHERæ•°æ®åŠ è½½å™¨

    Args:
        data_dir: æ•°æ®ç›®å½•
        sequence_length: åºåˆ—é•¿åº¦(å¸§)
        batch_size: æ‰¹å¤§å°
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        energy_selection: æ˜¯å¦ä½¿ç”¨èƒ½é‡é€‰æ‹©
        test_mode: æµ‹è¯•æ¨¡å¼
        feature_spec_type: ç‰¹å¾è§„èŒƒç±»å‹
        features_file: ç‰¹å¾æ–‡ä»¶è·¯å¾„
        audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        validation_split: éªŒè¯é›†æ¯”ä¾‹ (0.0-1.0)
        split_mode: åˆ†å‰²æ¨¡å¼ ("train" æˆ– "val")
        stride_frames: æ­¥å¹…å¤§å°(å¸§)ï¼ŒNoneåˆ™ä½¿ç”¨è‡ªåŠ¨ç­–ç•¥

    Returns:
        dataloader, dataset
    """

    print(f"Creating AETHER data loader (test_mode={test_mode})...")

    # æ­¥å¹…ç­–ç•¥ï¼šå¯é…ç½®æˆ–æ™ºèƒ½è‡ªé€‚åº”
    if stride_frames is not None:
        stride = stride_frames
        print(f"Using user-specified stride: {stride} frames")
    else:
        # é»˜è®¤ç­–ç•¥ï¼šæ ¹æ®åºåˆ—é•¿åº¦æ™ºèƒ½é€‰æ‹©æ­¥å¹…
        if sequence_length >= 800:  # 8ç§’ä»¥ä¸Šï¼šä½¿ç”¨æ›´å¤§æ­¥å¹…å‡å°‘é‡å 
            stride = sequence_length // 8  # 87.5%é‡å  (æ”¹è¿›ï¼šä»//16åˆ°//8)
        elif sequence_length >= 400:  # 4-8ç§’ï¼šä¸­ç­‰æ­¥å¹…
            stride = sequence_length // 6  # 83.3%é‡å 
        else:  # çŸ­åºåˆ—ï¼šè¾ƒå°æ­¥å¹…ä½†ä»æ¯”åŸæ¥å¤§
            stride = max(sequence_length // 4, 60)  # 75%é‡å æˆ–è‡³å°‘60å¸§
        print(f"Using adaptive stride: {stride} frames for sequence_length={sequence_length}")

    # æ ¹æ®ç‰¹å¾è§„èŒƒç±»å‹é€‰æ‹©å¯¹åº”çš„é…ç½®
    if feature_spec_type == "fargan":
        feature_spec = get_fargan_feature_spec()
        print(f"Using FARGAN feature spec: 36 dimensions")
    else:
        feature_spec = get_default_feature_spec()
        print(f"Using AETHER feature spec: 48 dimensions")

    # åˆ›å»ºæ•°æ®é›†
    dataset = AETHERRealDataset(
        data_dir=data_dir,
        sequence_length=sequence_length,
        frame_size=160,
        max_samples=max_samples,
        stride=stride,
        energy_selection=energy_selection,
        feature_spec=feature_spec,
        features_file=features_file,
        audio_file=audio_file,
        validation_split=validation_split,
        split_mode=split_mode
    )

    # æ‰“å°æ•°æ®é›†ä¿¡æ¯
    info = dataset.get_info()
    print(f"Dataset info:")
    print(f"  Total frames: {info['total_frames']:,}")
    print(f"  Training samples: {info['total_samples']:,}")
    print(f"  Audio duration: {info['audio_duration_hours']:.1f}h")
    print(f"  Sequence length: {info['sequence_length']} frames ({info['sequence_duration_seconds']:.1f}s)")

    # ğŸ”¥ CPUä¼˜åŒ–ï¼šåŸºäºåˆ†æä¼˜åŒ–DataLoaderé…ç½®
    # æ¨èé…ç½®ï¼šworker=4, prefetch=2, pin_memory=True, persistent_workers=True
    dl_pin_memory = True if pin_memory is None else bool(pin_memory)
    dl_persistent_workers = (num_workers > 0) if persistent_workers is None else bool(persistent_workers)

    # ä¼˜åŒ–prefetch_factorï¼šä»4é™è‡³2ä»¥å‡å°‘CPU-GPUé˜Ÿåˆ—ç«äº‰
    if prefetch_factor is None:
        dl_prefetch_factor = 2 if num_workers > 0 else None
    else:
        dl_prefetch_factor = int(prefetch_factor)

    # å¦‚æœworkersè¿‡å¤šï¼Œè‡ªåŠ¨è°ƒæ•´ä»¥ä¼˜åŒ–æ€§èƒ½
    optimized_workers = num_workers
    if num_workers > 6:
        optimized_workers = 4
        print(f"  âš¡ Auto-optimized workers: {num_workers} â†’ {optimized_workers} (reduce CPU-GPU contention)")

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=optimized_workers,
        pin_memory=dl_pin_memory,
        drop_last=True,
        persistent_workers=dl_persistent_workers,
        prefetch_factor=dl_prefetch_factor
    )

    print(f"Data loader created:")
    print(f"  Batch size: {batch_size}")
    print(f"  Total batches: {len(dataloader):,}")
    print(f"  Workers: {optimized_workers} (pin_memory={dl_pin_memory}, persistent_workers={dl_persistent_workers}, prefetch={dl_prefetch_factor})")

    return dataloader, dataset


# ---- Combined multi-expert dataset (æ··åˆæ‰¹ï¼šæŒ‰é…æ¯”ä»å››ç±»æ•°æ®é›†ä¸­é‡‡æ ·) ----

EXPERT_KEYS = ['harmonic', 'transient', 'burst_inpaint', 'low_snr']


class CombinedExpertDataset(Dataset):
    """å°†å››ä¸ªä¸“å®¶æ•°æ®é›†åˆå¹¶ä¸ºä¸€ä¸ªæ•°æ®é›†ï¼ŒæŒ‰é…æ¯”åœ¨ __getitem__ ä¸­éšæœºé€‰æ‹©å­é›†å¹¶è¿”å›æ ·æœ¬ã€‚

    - ä½¿ç”¨å†…éƒ¨çš„ AETHERRealDataset å®ä¾‹ï¼ˆæ¯ç±»ä¸€ä¸ªï¼‰
    - æ¯ä¸ªæ ·æœ¬é™„å¸¦ 'expert_class' æ ‡ç­¾ï¼š0=harmonic, 1=transient, 2=burst_inpaint, 3=low_snr
    - __len__ è¿”å›å››ä¸ªå­é›†é•¿åº¦ä¹‹å’Œçš„è¿‘ä¼¼å€¼ï¼ˆç”¨äºè¿›åº¦ä¼°ç®—ï¼‰ï¼›é‡‡æ ·éšæœºï¼Œä¸ä½¿ç”¨ idx ç´¢å¼•çœŸå®ä½ç½®
    """
    def __init__(
        self,
        datasets: Dict[str, AETHERRealDataset],
        mix_ratio: List[float],
    ) -> None:
        super().__init__()
        self.datasets = datasets
        self.mix_ratio = np.array(mix_ratio, dtype=np.float64)
        self.mix_ratio = self.mix_ratio / max(1e-8, self.mix_ratio.sum())

        # æ„å»ºç±»åˆ«åˆ°ç´¢å¼•çš„æ˜ å°„
        self.key_to_id = {k: i for i, k in enumerate(EXPERT_KEYS)}
        self.id_to_key = {i: k for k, i in self.key_to_id.items()}

        # è®°å½•æ¯ä¸ªå­é›†é•¿åº¦ç”¨äºé‡‡æ ·
        self.lengths = {k: len(ds) for k, ds in datasets.items()}
        self.total_len = int(sum(self.lengths.values()))

        print("CombinedExpertDataset:")
        for k in EXPERT_KEYS:
            if k in self.lengths:
                print(f"  - {k}: {self.lengths[k]:,} samples")
        print(f"  Mix ratio: {self.mix_ratio.tolist()}")

        # ä¸ºç±»åˆ«æŠ½æ ·æ„å»ºç´¯ç§¯åˆ†å¸ƒ
        self.cumprob = np.cumsum(self.mix_ratio)

    def __len__(self) -> int:
        # è¿‘ä¼¼æ€»é•¿åº¦ï¼ˆä¸ä¸¥æ ¼ä½¿ç”¨ï¼‰ï¼Œä»…ç”¨äºDataLoaderè¿›åº¦ä¸epochå¤§å°æ§åˆ¶
        return self.total_len

    def _sample_class_id(self) -> int:
        r = np.random.rand()
        for i, cp in enumerate(self.cumprob):
            if r <= cp:
                return i
        return len(self.cumprob) - 1

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # å¿½ç•¥ idxï¼ŒæŒ‰é…æ¯”éšæœºé€‰æ‹©ç±»åˆ«
        cid = self._sample_class_id()
        key = self.id_to_key[cid]
        ds = self.datasets[key]

        # åœ¨è¯¥å­é›†ä¸­éšæœºå–ä¸€ä¸ªæ ·æœ¬
        ridx = np.random.randint(0, len(ds))
        item = ds[ridx]

        # é™„åŠ ç±»åˆ«æ ‡ç­¾
        item['expert_class'] = torch.tensor(cid, dtype=torch.long)
        return item


def create_combined_data_loader(
    data_root: str,
    sequence_length: int,
    batch_size: int,
    frame_size: int = 160,
    stride_frames: Optional[int] = None,
    energy_selection: bool = True,
    feature_dims: int = 36,
    max_samples: Optional[int] = None,
    num_workers: int = 4,
    pin_memory: Optional[bool] = None,
    persistent_workers: Optional[bool] = None,
    prefetch_factor: Optional[int] = None,
) -> Tuple[DataLoader, CombinedExpertDataset]:
    """åˆ›å»ºæ··åˆä¸“å®¶æ•°æ®åŠ è½½å™¨ã€‚

    é‡‡ç”¨å›ºå®šå‘½åè§„åˆ™åœ¨ data_root ä¸‹å¯»æ‰¾å››ç±»æ•°æ®ï¼›è‹¥å­˜åœ¨ small-200k ç‰ˆæœ¬ä¼˜å…ˆä½¿ç”¨ï¼Œå¦åˆ™å›é€€åˆ° *_enhanced å‘½åã€‚
    """
    print("Creating CombinedExpertDataset (multi-expert mixed batches)...")
    # æ ¹æ®feature_dimsé€‰æ‹©è§„èŒƒ
    assert feature_dims in (36, 48), "Only 36 or 48 dims supported"
    if feature_dims == 36:
        feature_spec = get_fargan_feature_spec()
    else:
        feature_spec = get_default_feature_spec()
    # æ–‡ä»¶åæ¨¡æ¿ï¼ˆä¼˜å…ˆsmall-200kï¼‰
    root = Path(data_root)
    small_ok = (root / 'harmonic_200k_36.f32').exists()

    def paths_for(name: str):
        if small_ok:
            return (
                str(root / f"{name}_200k_36.f32"),
                str(root / f"{name}_200k.pcm"),
            )
        else:
            return (
                str(root / f"{name}_enhanced_36.f32"),
                str(root / f"{name}_enhanced.pcm"),
            )

    datasets: Dict[str, AETHERRealDataset] = {}
    for k in EXPERT_KEYS:
        fpath, apath = paths_for(k)
        try:
            ds = AETHERRealDataset(
                data_dir=data_root,
                sequence_length=sequence_length,
                frame_size=frame_size,
                max_samples=max_samples,
                stride=stride_frames if stride_frames is not None else max(100, sequence_length // 8),
                energy_selection=energy_selection,
                feature_spec=feature_spec,
                features_file=fpath,
                audio_file=apath,
            )
            datasets[k] = ds
        except FileNotFoundError as e:
            print(f"  [WARN] Missing subset for {k}: {e}")

    if not datasets:
        raise RuntimeError("No subsets found for CombinedExpertDataset")

    # é»˜è®¤ç­‰æ¯”æ··åˆï¼ˆå‡è¡¡ï¼‰
    mix_ratio = [1.0 if k in datasets else 0.0 for k in EXPERT_KEYS]
    s = sum(mix_ratio)
    mix_ratio = [x / s for x in mix_ratio]

    combined = CombinedExpertDataset(datasets=datasets, mix_ratio=mix_ratio)

    # DataLoaderå‚æ•°ä¸å•é›†ä¸€è‡´
    dl_pin_memory = True if pin_memory is None else bool(pin_memory)
    dl_persistent_workers = (num_workers > 0) if persistent_workers is None else bool(persistent_workers)
    dl_prefetch_factor = 2 if (prefetch_factor is None and num_workers > 0) else prefetch_factor

    loader = DataLoader(
        combined,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=dl_pin_memory,
        drop_last=True,
        persistent_workers=dl_persistent_workers,
        prefetch_factor=dl_prefetch_factor,
    )

    print("Data loader (combined) created:")
    print(f"  Batch size: {batch_size}")
    print(f"  Total batches: {len(loader):,}")
    print(f"  Workers: {num_workers} (pin_memory={dl_pin_memory}, persistent_workers={dl_persistent_workers}, prefetch={dl_prefetch_factor})")
    return loader, combined


def create_train_val_loaders(
    validation_split: float = 0.15,
    **kwargs
) -> Tuple[DataLoader, DataLoader, AETHERRealDataset, AETHERRealDataset]:
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆéé‡å åˆ†å‰²ï¼‰

    Args:
        validation_split: éªŒè¯é›†æ¯”ä¾‹ (0.0-1.0)
        **kwargs: ä¼ é€’ç»™create_aether_data_loaderçš„å…¶ä»–å‚æ•°

    Returns:
        train_loader, val_loader, train_dataset, val_dataset
    """
    if validation_split <= 0.0:
        raise ValueError("validation_split must be > 0.0 for creating train/val split")

    # ç§»é™¤splitç›¸å…³å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    kwargs.pop('validation_split', None)
    kwargs.pop('split_mode', None)

    print(f"Creating train/val split with {validation_split:.1%} validation...")

    # åˆ›å»ºè®­ç»ƒé›†
    train_loader, train_dataset = create_aether_data_loader(
        validation_split=validation_split,
        split_mode="train",
        **kwargs
    )

    # åˆ›å»ºéªŒè¯é›†
    # éªŒè¯é›†ä½¿ç”¨è¾ƒå°çš„batch_sizeä»¥èŠ‚çœå†…å­˜
    val_batch_size = min(kwargs.get('batch_size', 4), 32)
    val_kwargs = kwargs.copy()
    val_kwargs['batch_size'] = val_batch_size
    val_kwargs['num_workers'] = min(val_kwargs.get('num_workers', 4), 2)  # éªŒè¯æ—¶å‡å°‘workers

    val_loader, val_dataset = create_aether_data_loader(
        validation_split=validation_split,
        split_mode="val",
        **val_kwargs
    )

    print(f"Train/Val split completed:")
    print(f"  Training: {len(train_dataset):,} samples, {len(train_loader):,} batches")
    print(f"  Validation: {len(val_dataset):,} samples, {len(val_loader):,} batches")

    return train_loader, val_loader, train_dataset, val_dataset


def test_aether_data_loader():
    """æµ‹è¯•AETHERæ•°æ®åŠ è½½å™¨"""
    print("ğŸ§ª æµ‹è¯•AETHERæ•°æ®åŠ è½½å™¨")
    print("=" * 50)

    try:
        dataloader, dataset = create_aether_data_loader(
            sequence_length=800,  # 8ç§’
            batch_size=2,
            max_samples=100,  # æµ‹è¯•ç”¨å°æ•°æ®
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            test_mode=True
        )

        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        for batch_idx, batch in enumerate(dataloader):
            print(f"Batch {batch_idx}:")
            print(f"  Features: {batch['x'].shape}")
            print(f"  Audio: {batch['audio'].shape}")
            print(f"  CSI keys: {list(batch['csi'].keys())}")

            print(f"  Feature range: [{batch['x'].min():.3f}, {batch['x'].max():.3f}]")
            print(f"  Audio range: [{batch['audio'].min():.3f}, {batch['audio'].max():.3f}]")
            print(f"  Audio duration: {batch['audio'].shape[-1] / 16000:.1f}s")
            break

        print("âœ“ AETHERæ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âœ— AETHERæ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_aether_data_loader()
