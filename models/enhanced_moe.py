# -*- coding: utf-8 -*-
"""
Simplified MicroMoE: basic sample-level routing aligned with jiagou.md.

Core Features:
- Sample-level top-k routing (not token-level)
- Simple load balancing with uniform prior
- Expert dropout for regularization
- Compatible with CSI conditioning
"""

from __future__ import annotations
from typing import Dict, Tuple, Optional
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from .utils import global_pool, extract_acoustic_priors
except ImportError:  # pragma: no cover
    from utils import global_pool, extract_acoustic_priors


# --- å¼ºåŒ–åçš„ RobustLSTMï¼šå‡ºç°éæ•°ä¹Ÿä¼šå›é€€åˆ°FP32 ---
class RobustLSTM(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.lstm = nn.LSTM(*args, **kwargs)
        self.force_fp32 = False

    def forward(self, x, *args, **kwargs):
        x = x.contiguous()
        orig = x.dtype

        def _run(inp):
            out, hid = self.lstm(inp, *args, **kwargs)
            return out, hid

        try:
            if self.force_fp32 and orig in (torch.bfloat16, torch.float16):
                out, hid = _run(x.float())
            else:
                out, hid = _run(x)
            # å…³é”®ï¼šäº§å‡ºåæ•°å€¼è‡ªæ£€
            if torch.isnan(out).any() or torch.isinf(out).any():
                raise RuntimeError("non-finite in LSTM output")
        except RuntimeError:
            if orig in (torch.bfloat16, torch.float16):
                self.force_fp32 = True
                out, hid = _run(x.float())
            else:
                raise

            # å†åšä¸€æ¬¡è‡ªæ£€ï¼ˆæç«¯ä¿é™©ï¼‰
            if torch.isnan(out).any() or torch.isinf(out).any():
                out = torch.nan_to_num(out, nan=0.0, posinf=1e4, neginf=-1e4)

        # å›åˆ°åŸdtype
        if isinstance(hid, tuple):
            hid = tuple(h.to(orig) for h in hid)
        else:
            hid = hid.to(orig)
        return out.to(orig), hid


# --- æ–°å¢ RobustGRUï¼šåŒæ ·çš„ç­–ç•¥ ---
class RobustGRU(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.gru = nn.GRU(*args, **kwargs)
        self.force_fp32 = False

    def forward(self, x, *args, **kwargs):
        x = x.contiguous()
        orig = x.dtype

        def _run(inp):
            out, hid = self.gru(inp, *args, **kwargs)
            return out, hid

        try:
            if self.force_fp32 and orig in (torch.bfloat16, torch.float16):
                out, hid = _run(x.float())
            else:
                out, hid = _run(x)
            if torch.isnan(out).any() or torch.isinf(out).any():
                raise RuntimeError("non-finite in GRU output")
        except RuntimeError:
            if orig in (torch.bfloat16, torch.float16):
                self.force_fp32 = True
                out, hid = _run(x.float())
            else:
                raise
            if torch.isnan(out).any() or torch.isinf(out).any():
                out = torch.nan_to_num(out, nan=0.0, posinf=1e4, neginf=-1e4)

        if isinstance(hid, tuple):
            hid = tuple(h.to(orig) for h in hid)
        else:
            hid = hid.to(orig)
        return out.to(orig), hid


class AcousticFeatureExtractor(nn.Module):
    """ä¸‰å±‚ç‰¹å¾æå–å™¨ - åˆ©ç”¨åŸå§‹å£°å­¦ã€Ribbonè¯­ä¹‰ã€Threadå¾®éŸ³æ®µç‰¹å¾"""
    def __init__(self, d_raw: int = 36, d_model: int = 128, feature_dim: int = 64, n_experts: int = 4):
        super().__init__()
        self.d_raw = d_raw
        self.d_model = d_model
        self.feature_dim = feature_dim
        self.n_experts = n_experts  # åŠ¨æ€ä¸“å®¶æ•°é‡

        # åŸå§‹å£°å­¦ç‰¹å¾åˆ†æå™¨ - ç›´æ¥ä»36ç»´ç‰¹å¾æå–å£°å­¦ä¿¡æ¯
        # E1: Harmonic - F0è½¨è¿¹åˆ†æ (å‡è®¾ç‰¹å¾ç»´åº¦åŒ…å«F0ç›¸å…³ä¿¡æ¯)
        self.raw_f0_extractor = nn.Conv1d(d_raw, 16, kernel_size=7, padding=3)  # åŸå§‹F0ç‰¹å¾
        self.raw_pitch_tracker = RobustLSTM(d_raw, 8, batch_first=True, bidirectional=True)  # F0è½¨è¿¹è·Ÿè¸ª

        # E2: Transient - é«˜é¢‘ç¬æ€åˆ†æ
        self.raw_transient_detector = nn.Conv1d(d_raw, 16, kernel_size=3, padding=1)  # çŸ­æ—¶ç¬æ€
        self.raw_energy_analyzer = nn.Conv1d(d_raw, 16, kernel_size=5, padding=2)  # èƒ½é‡åˆ†å¸ƒ

        # E3: Burst-Inpaint - è¿ç»­æ€§åˆ†æ
        self.raw_continuity_check = nn.Conv1d(d_raw, 16, kernel_size=9, padding=4, dilation=2)  # è¿ç»­æ€§
        self.raw_gap_detector = RobustGRU(d_raw, 8, batch_first=True, bidirectional=True) # ç¼ºå¤±æ£€æµ‹

        # E4: Low-SNR - ä¿¡å·è´¨é‡åˆ†æ
        self.raw_snr_estimator = nn.Sequential(
            nn.Linear(d_raw, d_raw//2),
            nn.ReLU(),
            nn.Linear(d_raw//2, 16)
        )
        self.raw_noise_profiler = nn.Conv1d(d_raw, 16, kernel_size=11, padding=5)  # å™ªå£°è½®å»“

        # ç¼–ç ç‰¹å¾åˆ†æå™¨ - ä»128ç»´è¯­ä¹‰ç‰¹å¾è¡¥å……é«˜å±‚ä¿¡æ¯
        # E1: Harmonic - è°æ³¢è¯­ä¹‰ç†è§£
        self.encoded_harmonic_semantic = nn.Conv1d(d_model, 8, kernel_size=7, padding=3)

        # E2: Transient - ç¬æ€è¯­ä¹‰ç†è§£
        self.encoded_transient_semantic = nn.Conv1d(d_model, 8, kernel_size=3, padding=1)

        # E3: Burst-Inpaint - ä¸Šä¸‹æ–‡è¯­ä¹‰ç†è§£
        self.encoded_context_semantic = nn.Conv1d(d_model, 8, kernel_size=11, padding=5)

        # E4: Low-SNR - è¯­ä¹‰ç¨³å®šæ€§ç†è§£
        self.encoded_stability_semantic = nn.Conv1d(d_model, 8, kernel_size=9, padding=4)

        # åŒå±‚ç‰¹å¾èåˆï¼šåŸå§‹å£°å­¦(16) + ç¼–ç è¯­ä¹‰(8) = 24 per expert - åŠ¨æ€æ•°é‡
        self.expert_fusion = nn.ModuleList([
            nn.Linear(24, 16) for _ in range(n_experts)  # åŠ¨æ€ä¸“å®¶èåˆå™¨
        ])

        # æœ€ç»ˆç‰¹å¾èåˆ - åŠ¨æ€è¾“å…¥ç»´åº¦
        expert_feature_dim = n_experts * 16  # ä¸“å®¶æ•°é‡ * 16ç‰¹å¾
        self.feature_fusion = nn.Linear(expert_feature_dim, feature_dim)

    def forward(
        self,
        x_raw: torch.Tensor,
        ribbon_stream: torch.Tensor,
        thread_stream: torch.Tensor,
        fused_stream: torch.Tensor = None
    ) -> torch.Tensor:
        """
        ä¸‰å±‚ç‰¹å¾æå–ï¼šåŸå§‹å£°å­¦ + Ribbonè¯­ä¹‰å¸¦ + Threadå¾®éŸ³æ®µå¸¦

        Args:
            x_raw: [B, T, 36] åŸå§‹è¾“å…¥ç‰¹å¾
            ribbon_stream: [B, T, 128] Ribboné•¿è¯­ä¹‰å¸¦ (3xä¸‹é‡‡æ ·å¤„ç†)
            thread_stream: [B, T, 128] Threadå¾®éŸ³æ®µå¸¦ (åŸåˆ†è¾¨ç‡å¤„ç†)
            fused_stream: [B, T, 128] èåˆç‰¹å¾ (å¯é€‰)
        Returns:
            features: [B, feature_dim] ä¸‰å±‚èåˆçš„ä¸“å®¶åå¥½ç‰¹å¾
        """
        b, t, _ = x_raw.shape

        # è½¬ç½®ä¸ºconv1dæ ¼å¼
        x_raw_conv = x_raw.transpose(1, 2)  # [B, 36, T]
        ribbon_conv = ribbon_stream.transpose(1, 2)  # [B, 128, T]
        thread_conv = thread_stream.transpose(1, 2)  # [B, 128, T]

        expert_features = []

        # E1: Harmonic Expert - è°æ³¢åˆ†æ
        # å±‚1: åŸå§‹å£°å­¦F0è½¨è¿¹åˆ†æ
        raw_f0 = self.raw_f0_extractor(x_raw_conv)  # [B, 16, T]
        # ä½¿ç”¨æ›´å®‰å…¨çš„ç»Ÿè®¡è®¡ç®—é¿å…fp16ä¸‹çš„NaN
        with torch.no_grad():
            raw_f0_safe = torch.clamp(raw_f0.float(), min=-100.0, max=100.0)
            f0_var = raw_f0_safe.var(dim=-1, unbiased=False, keepdim=False)
            f0_stability = torch.clamp(f0_var + 1e-6, min=1e-6, max=100.0).to(raw_f0.dtype)  # [B, 16] F0ç¨³å®šæ€§

        # RobustLSTMä¼šè‡ªåŠ¨å¤„ç†tensorè¿ç»­æ€§å’Œdtypeå…¼å®¹æ€§
        pitch_out, _ = self.raw_pitch_tracker(x_raw)  # [B, T, 16]

        # æ£€æŸ¥LSTMè¾“å‡ºæ˜¯å¦æœ‰NaN
        if torch.isnan(pitch_out).any():
            print(f"âš ï¸ pitch_out has NaN! Input range: [{x_raw.min():.3f}, {x_raw.max():.3f}]")
            pitch_out = torch.where(torch.isnan(pitch_out), torch.zeros_like(pitch_out), pitch_out)

        with torch.no_grad():
            pitch_safe = torch.clamp(pitch_out.float(), min=-100.0, max=100.0)
            pitch_std = pitch_safe.std(dim=1, unbiased=False, keepdim=False)
            pitch_continuity = torch.clamp(pitch_std + 1e-6, min=1e-6, max=100.0).to(pitch_out.dtype)  # [B, 16] éŸ³è°ƒè¿ç»­æ€§

        # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - è°æ³¢çš„è¯­è¨€å­¦æ„ä¹‰(éŸ³èŠ‚/è¯æ±‡çº§åˆ«çš„F0æ¨¡å¼)
        ribbon_harmonic = self.encoded_harmonic_semantic(ribbon_conv)  # [B, 8, T]
        # Add numerical stability for mixed precision
        ribbon_harmonic_safe = torch.clamp(ribbon_harmonic.float(), min=-100.0, max=100.0)
        ribbon_harmonic_pooled = torch.nan_to_num(ribbon_harmonic_safe.mean(dim=-1),
                                                 nan=0.0, posinf=1.0, neginf=-1.0).to(ribbon_harmonic.dtype)

        # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - ç»†ç²’åº¦F0è°ƒåˆ¶å’Œè°æ³¢å¾®ç»“æ„
        # ä½¿ç”¨Threadæµåˆ†æçŸ­æ—¶F0å˜åŒ–å’Œè°æ³¢ç»†èŠ‚
        thread_safe = torch.clamp(thread_stream.float(), min=-100.0, max=100.0)
        thread_f0_micro = torch.nan_to_num(thread_safe.mean(dim=1)[:, :8],
                                          nan=0.0, posinf=1.0, neginf=-1.0).to(thread_stream.dtype)

        # ä¸‰å±‚èåˆ: åŸå§‹F0(16) + Ribbonè¯­ä¹‰F0(8) + Threadå¾®F0(8) = 32 â†’ 16
        # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ˜¯å¦æœ‰NaN
        components = [f0_stability, pitch_continuity, ribbon_harmonic_pooled, thread_f0_micro]
        for i, comp in enumerate(components):
            if torch.isnan(comp).any():
                print(f"âš ï¸ Component {i} has NaN: {comp.shape}, range=[{comp.min():.3f}, {comp.max():.3f}]")
                components[i] = torch.where(torch.isnan(comp), torch.zeros_like(comp), comp)

        harmonic_triple = torch.cat(components, dim=-1)  # [B, 32]
        # å®‰å…¨æˆªæ–­ + å»éæ•°
        triple_safe = torch.nan_to_num(harmonic_triple[:, :24], nan=0.0, posinf=1e4, neginf=-1e4)
        if torch.isnan(triple_safe).any():
            print(f"âš ï¸ triple_safe has NaN before fusion!")
            triple_safe = torch.where(torch.isnan(triple_safe), torch.zeros_like(triple_safe), triple_safe)

        harmonic_fused = self.expert_fusion[0](triple_safe)  # [B, 16]

        # æ£€æŸ¥èåˆç»“æœ
        if torch.isnan(harmonic_fused).any():
            print(f"âš ï¸ harmonic_fused has NaN after fusion!")
            harmonic_fused = torch.where(torch.isnan(harmonic_fused), torch.zeros_like(harmonic_fused), harmonic_fused)

        expert_features.append(harmonic_fused)

        # E2: Transient Expert - ç¬æ€åˆ†æ
        # å±‚1: åŸå§‹å£°å­¦ç¬æ€æ£€æµ‹
        raw_transient = self.raw_transient_detector(x_raw_conv)  # [B, 16, T]
        with torch.no_grad():
            transient_safe = torch.clamp(raw_transient.float(), min=-100.0, max=100.0)
            transient_std = transient_safe.std(dim=-1, unbiased=False, keepdim=False)
            transient_intensity = torch.clamp(transient_std + 1e-6, min=1e-6, max=100.0).to(raw_transient.dtype)  # [B, 16] ç¬æ€å¼ºåº¦

        raw_energy = self.raw_energy_analyzer(x_raw_conv)  # [B, 16, T]
        with torch.no_grad():
            energy_safe = torch.clamp(raw_energy.float(), min=-100.0, max=100.0)
            energy_var = energy_safe.var(dim=-1, unbiased=False, keepdim=False)
            energy_variance = torch.clamp(energy_var + 1e-6, min=1e-6, max=100.0).to(raw_energy.dtype)  # [B, 16] èƒ½é‡å˜åŒ–

        # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - éŸ³ç´ çº§ç¬æ€æ¨¡å¼(çˆ†ç ´éŸ³vsæ‘©æ“¦éŸ³çš„è¯­è¨€å­¦åˆ†ç±»)
        ribbon_transient = self.encoded_transient_semantic(ribbon_conv)  # [B, 8, T]
        with torch.no_grad():
            ribbon_trans_safe = torch.clamp(ribbon_transient.float(), min=-100.0, max=100.0)
            ribbon_trans_std = ribbon_trans_safe.std(dim=-1, unbiased=False, keepdim=False)
            # Add nan_to_num for mixed precision stability
            ribbon_trans_std = torch.nan_to_num(ribbon_trans_std, nan=0.0, posinf=1.0, neginf=0.0)
            ribbon_transient_pooled = torch.clamp(ribbon_trans_std + 1e-6, min=1e-6, max=100.0).to(ribbon_transient.dtype)  # [B, 8] è¯­è¨€å­¦ç¬æ€æ¨¡å¼

        # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - æœ€é€‚åˆç¬æ€ï¼ä¿æŒåŸåˆ†è¾¨ç‡æ•è·çŸ­æ—¶å†²å‡»
        with torch.no_grad():
            thread_safe = torch.clamp(thread_stream.float(), min=-100.0, max=100.0)
            thread_std = thread_safe.std(dim=1, unbiased=False, keepdim=False)[:, :8]
            # Add nan_to_num for mixed precision stability
            thread_std = torch.nan_to_num(thread_std, nan=0.0, posinf=1.0, neginf=0.0)
            thread_transient_micro = torch.clamp(thread_std + 1e-6, min=1e-6, max=100.0).to(thread_stream.dtype)  # [B, 8] Threadå¤©ç„¶é€‚åˆç¬æ€

        # ä¸‰å±‚èåˆ: åŸå§‹ç¬æ€(16) + Ribbonè¯­ä¹‰ç¬æ€(8) + Threadå¾®ç¬æ€(8) = 32 â†’ 16
        transient_components = [transient_intensity, energy_variance, ribbon_transient_pooled, thread_transient_micro]
        for i, comp in enumerate(transient_components):
            if torch.isnan(comp).any():
                print(f"âš ï¸ Transient component {i} has NaN")
                transient_components[i] = torch.where(torch.isnan(comp), torch.zeros_like(comp), comp)

        transient_triple = torch.cat(transient_components, dim=-1)
        transient_safe = transient_triple[:, :24]
        transient_safe = torch.nan_to_num(transient_safe, nan=0.0, posinf=1e4, neginf=-1e4)
        if torch.isnan(transient_safe).any():
            transient_safe = torch.where(torch.isnan(transient_safe), torch.zeros_like(transient_safe), transient_safe)

        transient_fused = self.expert_fusion[1](transient_safe)  # [B, 16]
        if torch.isnan(transient_fused).any():
            print(f"âš ï¸ transient_fused has NaN after fusion!")
            transient_fused = torch.where(torch.isnan(transient_fused), torch.zeros_like(transient_fused), transient_fused)

        expert_features.append(transient_fused)

        # E3: Burst-Inpaint Expert - è¿ç»­æ€§å’Œä¿®å¤åˆ†æ
        # å±‚1: åŸå§‹å£°å­¦è¿ç»­æ€§æ£€æŸ¥
        raw_continuity = self.raw_continuity_check(x_raw_conv)  # [B, 16, T]
        if t > 1:
            continuity_breaks = torch.diff(raw_continuity, dim=-1).abs().mean(dim=-1)  # [B, 16]
        else:
            continuity_breaks = torch.zeros(b, 16, device=x_raw.device)

        gap_out, _ = self.raw_gap_detector(x_raw)  # [B, T, 16]
        gap_pattern = torch.clamp(gap_out.float().std(dim=1, unbiased=False) + 1e-6, min=1e-6, max=100.0).to(gap_out.dtype)  # [B, 16] ç¼ºå¤±æ¨¡å¼

        # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - é•¿ç¨‹ä¸Šä¸‹æ–‡ä¾èµ–(å¥å­çº§ä¿®å¤è¯­ä¹‰)
        ribbon_context = self.encoded_context_semantic(ribbon_conv)  # [B, 8, T]
        ribbon_context_pooled = ribbon_context.mean(dim=-1)  # [B, 8] é•¿ç¨‹è¯­ä¹‰ä¸Šä¸‹æ–‡

        # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - å±€éƒ¨é‚»è¿‘ä¿®å¤(éŸ³ç´ é—´è¿‡æ¸¡)
        thread_local_context = torch.clamp(thread_stream.float().var(dim=1, unbiased=False)[:, :8] + 1e-6, min=1e-6, max=100.0).to(thread_stream.dtype)  # [B, 8] å±€éƒ¨å˜å¼‚ç”¨äºæ£€æµ‹ç¼ºå¤±

        # ä¸‰å±‚èåˆ: åŸå§‹è¿ç»­æ€§(16) + Ribboné•¿ç¨‹(8) + Threadå±€éƒ¨(8) = 32 â†’ 16
        inpaint_triple = torch.cat([continuity_breaks, gap_pattern, ribbon_context_pooled, thread_local_context], dim=-1)
        inpaint_safe   = torch.nan_to_num(inpaint_triple[:, :24], nan=0.0, posinf=1e4, neginf=-1e4)
        inpaint_fused  = self.expert_fusion[2](inpaint_safe)  # [B, 16]
        expert_features.append(inpaint_fused)

        # E4: Low-SNR Expert - ä¿¡å·è´¨é‡å’Œç¨³å®šæ€§åˆ†æ (ä»…å½“n_experts=4æ—¶)
        if self.n_experts >= 4:
            # å±‚1: åŸå§‹å£°å­¦SNRä¼°è®¡
            x_raw_mean = x_raw.mean(dim=1)  # [B, 36]
            raw_snr = self.raw_snr_estimator(x_raw_mean)  # [B, 16]

            raw_noise = self.raw_noise_profiler(x_raw_conv)  # [B, 16, T]
            noise_profile = torch.clamp(raw_noise.float().std(dim=-1, unbiased=False) + 1e-6, min=1e-6, max=100.0).to(raw_noise.dtype)  # [B, 16] å™ªå£°ç‰¹å¾

            # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - è¯­ä¹‰ç¨³å®šæ€§(è¯æ±‡/å¥å­çº§ä¸€è‡´æ€§)
            ribbon_stability = self.encoded_stability_semantic(ribbon_conv)  # [B, 8, T]
            ribbon_stability_pooled = torch.clamp(ribbon_stability.float().var(dim=-1, unbiased=False) + 1e-6, min=1e-6, max=100.0).to(ribbon_stability.dtype)  # [B, 8] é•¿æœŸè¯­ä¹‰ç¨³å®šæ€§

            # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - ä¿¡å·è´¨é‡å¾®åˆ†æ(éŸ³ç´ çº§SNR)
            thread_signal_quality = thread_stream.mean(dim=1)[:, 8:16]  # [B, 8] Threadå¾®ä¿¡å·è´¨é‡

            # ä¸‰å±‚èåˆ: åŸå§‹SNR(16) + Ribbonè¯­ä¹‰ç¨³å®š(8) + Threadè´¨é‡(8) = 32 â†’ 16
            lowsnr_triple = torch.cat([raw_snr, noise_profile, ribbon_stability_pooled, thread_signal_quality], dim=-1)
            lowsnr_fused = self.expert_fusion[3](lowsnr_triple[:, :24])  # [B, 16]
            expert_features.append(lowsnr_fused)

        # ç»„åˆæ‰€æœ‰ä¸“å®¶ç‰¹å¾ (åŠ¨æ€é•¿åº¦)
        all_expert_features = torch.cat(expert_features, dim=-1)  # [B, n_experts*16]
        all_expert_features = torch.nan_to_num(all_expert_features, nan=0.0, posinf=1e4, neginf=-1e4)
        # æœ€ç»ˆç‰¹å¾èåˆ
        acoustic_features = self.feature_fusion(all_expert_features)  # [B, feature_dim]

        return acoustic_features


class UnifiedAudioExpert(nn.Module):
    """ä¸“ä¸šåŒ–å¼•å¯¼çš„ç»Ÿä¸€éŸ³é¢‘ä¸“å®¶ - ä¿æŒFFNæ¶æ„ä½†æ·»åŠ ä¸“ä¸šåŒ–åå¥½

    Optional F0-conditioning:
    - Keeps the public forward signature unchanged.
    - Parent can inject a per-sample conditioning vector via `set_f0_condition`.
    - When enabled, applies a lightweight FiLM-style modulation before the expert FFN.
    """
    def __init__(self, d_model: int = 128, d_ff: int = None, expert_id: int = 0,
                 use_f0_condition: bool = False, f0_cond_dim: int = 6):
        super().__init__()
        self.expert_id = expert_id
        self.d_model = d_model
        self.use_f0_condition = use_f0_condition
        self.f0_cond_dim = f0_cond_dim

        # è‡ªé€‚åº”FFNç»´åº¦
        if d_ff is None:
            d_ff = d_model * 4
        self.d_ff = d_ff

        # ä¸“å®¶ç‰¹å®šçš„FFNæ¶æ„ - ç»“æ„æ€§å·®å¼‚åŒ–
        if self.expert_id == 0:
            # Harmonicä¸“å®¶ï¼šæ·±å±‚ç½‘ç»œï¼Œå…³æ³¨é•¿æœŸä¾èµ–
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.LayerNorm(d_ff),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(d_ff, d_ff // 2),
                nn.GELU(),
                nn.Linear(d_ff // 2, d_model),
                nn.Dropout(0.05)
            )
        elif self.expert_id == 1:
            # Transientä¸“å®¶ï¼šå®½å±‚ç½‘ç»œï¼Œå…³æ³¨ç¬æ€ç‰¹å¾
            wide_ff = int(d_ff * 1.5)
            self.ffn = nn.Sequential(
                nn.Linear(d_model, wide_ff),
                nn.ReLU(),  # ä½¿ç”¨ReLUå¢å¼ºéçº¿æ€§
                nn.Dropout(0.1),
                nn.Linear(wide_ff, d_model),
                nn.Dropout(0.05)
            )
        elif self.expert_id == 2:
            # BurstInpaintä¸“å®¶ï¼šæ®‹å·®ç½‘ç»œï¼Œå…³æ³¨å±€éƒ¨ä¿®å¤
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(d_ff, d_model),
                nn.Dropout(0.05)
            )
            # é¢å¤–çš„æ®‹å·®è·¯å¾„
            self.inpaint_residual = nn.Sequential(
                nn.Linear(d_model, d_model // 4),
                nn.GELU(),
                nn.Linear(d_model // 4, d_model)
            )
        else:
            # LowSNRä¸“å®¶ï¼šä¿å®ˆç½‘ç»œï¼Œå…³æ³¨å™ªå£°æŠ‘åˆ¶
            conservative_ff = int(d_ff * 0.75)
            self.ffn = nn.Sequential(
                nn.Linear(d_model, conservative_ff),
                nn.Tanh(),  # ä½¿ç”¨Tanhé™åˆ¶è¾“å‡ºèŒƒå›´
                nn.Dropout(0.05),  # æ›´å°çš„dropout
                nn.Linear(conservative_ff, d_model),
                nn.Dropout(0.02)
            )

        self.layer_norm = nn.LayerNorm(d_model)

        # ä¸“å®¶ç‰¹å®šçš„å·®å¼‚åŒ–æœºåˆ¶
        self.expert_bias = nn.Parameter(torch.zeros(d_model))

        # ä¸“ä¸šåŒ–å¼•å¯¼ï¼šæ¯ä¸ªä¸“å®¶æœ‰ç‰¹å®šçš„ç‰¹å¾å…³æ³¨æƒé‡
        self.specialization_weights = nn.Parameter(torch.ones(d_model) * 0.1)  # å°åˆå§‹æƒé‡

        # å¯é€‰ï¼šF0 æ¡ä»¶åŒ–ï¼ˆFiLM é£æ ¼è°ƒåˆ¶ï¼‰
        # ç”±çˆ¶æ¨¡å—é€šè¿‡ set_f0_condition(cond:[B,f0_cond_dim] or [B,T,f0_cond_dim]) è®¾ç½®ã€‚
        # ä»…å½“ use_f0_condition=True æ—¶å¯ç”¨ï¼›é»˜è®¤å…³é—­ä»¥ä¿æŒè¡Œä¸ºä¸å˜ã€‚
        if self.use_f0_condition:
            self.f0_gate = nn.Sequential(
                nn.Linear(self.f0_cond_dim, 2 * d_model)
            )
        else:
            self.f0_gate = None

        # è¿è¡Œæ—¶ç¼“å­˜çš„æ¡ä»¶ï¼ˆä¸è¿›å…¥state_dictï¼‰
        self._f0_cond = None

        # æ ¹æ®expert_idè®¾ç½®ä¸“ä¸šåŒ–åå¥½
        self._init_specialization_bias()

    def _init_specialization_bias(self):
        """åŸºäºéŸ³é¢‘åœºæ™¯çš„ä¸“å®¶ä¸“ä¸šåŒ–åˆå§‹åŒ–"""
        with torch.no_grad():
            # åŸºäºéŸ³é¢‘å¤„ç†éœ€æ±‚çš„å·®å¼‚åŒ–åˆå§‹åŒ–

            # ä¿®å¤: ä½¿ç”¨å¹³è¡¡çš„å·®å¼‚åŒ–åˆå§‹åŒ–ç­–ç•¥
            base_spec_std = 0.05  # é€‚ä¸­çš„åŸºç¡€ç‰¹åŒ–æƒé‡æ ‡å‡†å·®ï¼Œé¿å…è¿‡åº¦å·®å¼‚
            base_bias_std = 0.01  # é€‚ä¸­çš„åŸºç¡€åç½®æ ‡å‡†å·®ï¼Œå¹³è¡¡ä¸ªæ€§å’Œç¨³å®šæ€§

            if self.expert_id == 0:
                # Expert 0: Harmonicä¸“å®¶ - ä¸“æ³¨éŸ³è°ƒç¨³å®šæ€§å’Œå‘¨æœŸç»“æ„
                # è½»å¾®åå‘ä½é¢‘ã€ç¨³å®šç‰¹å¾
                self.specialization_weights.normal_(-0.02, base_spec_std * 0.8)  # è½»å¾®è´Ÿåå‘ï¼Œå…³æ³¨ç¨³å®šæ€§
                self.expert_bias.normal_(-0.005, base_bias_std)  # è½»å¾®è´Ÿåç½®ï¼Œä¿å®ˆå¤„ç†

            elif self.expert_id == 1:
                # Expert 1: Transientä¸“å®¶ - ä¸“æ³¨åŠ¨æ€å˜åŒ–å’Œç¬æ€æ£€æµ‹
                # è½»å¾®åå‘é«˜é¢‘ã€åŠ¨æ€ç‰¹å¾
                self.specialization_weights.normal_(0.03, base_spec_std * 1.2)  # è½»å¾®æ­£åå‘ï¼Œæ•æ„Ÿæ£€æµ‹
                self.expert_bias.normal_(0.008, base_bias_std * 1.5)  # è½»å¾®æ­£åç½®ï¼Œæ¿€æ´»åŠ¨æ€

            elif self.expert_id == 2:
                # Expert 2: BurstInpaintä¸“å®¶ - ä¸“æ³¨ä¸Šä¸‹æ–‡è¿ç»­æ€§å’Œä¿®å¤
                # ä¸­æ€§åå‘ï¼Œå…³æ³¨è¿ç»­æ€§
                self.specialization_weights.normal_(0.01, base_spec_std)  # è½»å¾®æ­£åå‘ï¼Œä¸Šä¸‹æ–‡å…³æ³¨
                self.expert_bias.normal_(0.003, base_bias_std * 1.2)  # è½»å¾®æ­£åç½®

            elif self.expert_id == 3:
                # Expert 3: LowSNRä¸“å®¶ - ä¸“æ³¨åŸå§‹ç‰¹å¾å«å™ªæ€§åˆ†æå’Œè´¨é‡è‡ªé€‚åº”
                # è½»å¾®åå‘å™ªå£°æŠ‘åˆ¶
                self.specialization_weights.normal_(-0.01, base_spec_std * 0.6)  # è½»å¾®è´Ÿåå‘ï¼Œé¿å…æ”¾å¤§å™ªå£°
                self.expert_bias.normal_(-0.003, base_bias_std * 0.8)  # è½»å¾®è´Ÿåç½®ï¼Œé™å™ªå¤„ç†

            # ä¿®å¤: ä½¿ç”¨æ›´ä¿å®ˆçš„ä¸“å®¶åŠŸèƒ½åç½®æ¨¡å¼
            specialty_patterns = {
                0: [0.005, -0.003, 0.002, -0.004],  # Harmonic: ç¨³å®šæ¨¡å¼
                1: [0.008, 0.005, -0.007, 0.006],   # Transient: åŠ¨æ€æ¨¡å¼
                2: [0.003, 0.004, 0.005, -0.004],   # BurstInpaint: ä¸Šä¸‹æ–‡æ¨¡å¼
                3: [-0.002, 0.001, -0.001, 0.002]   # LowSNR: ç‰¹å¾è´¨é‡è¯„ä¼°æ¨¡å¼
            }
            if self.expert_id in specialty_patterns:
                pattern = specialty_patterns[self.expert_id]
                for i, val in enumerate(pattern):
                    if i * 32 < len(self.expert_bias):
                        self.expert_bias[i*32:(i+1)*32] += val

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [B, T, D]
        residual = x
        x = self.layer_norm(x)

        # F0 æ¡ä»¶åŒ–ï¼ˆè‹¥å¯ç”¨ä¸”æä¾›äº† condï¼‰
        if self.use_f0_condition and self.f0_gate is not None and (self._f0_cond is not None):
            cond = self._f0_cond
            # æ”¯æŒ [B, C] æˆ– [B, T, C] ä¸¤ç§å½¢çŠ¶
            if cond.dim() == 3:
                # æ—¶é—´å¹³å‡åˆ°æ ·æœ¬çº§æç¤ºï¼ˆæ•°å€¼æ›´ç¨³ï¼‰
                cond_vec = cond.mean(dim=1)
            else:
                cond_vec = cond  # [B, C]

            # æ•°å€¼å®‰å…¨ä¸ç±»å‹å¯¹é½
            cond_vec = torch.nan_to_num(cond_vec, nan=0.0, posinf=1e4, neginf=-1e4).to(x.dtype)
            ab = self.f0_gate(cond_vec)  # [B, 2D]
            a, b = ab.chunk(2, dim=-1)
            # è½»é‡è°ƒåˆ¶ï¼Œé™åˆ¶å¹…åº¦
            a = 0.25 * torch.tanh(a)
            b = 0.10 * torch.tanh(b)
            # åº”ç”¨åˆ°æ¯ä¸ªæ—¶é—´æ­¥
            x = x * (1.0 + a.unsqueeze(1)) + b.unsqueeze(1)

        # ä¿®å¤: å¢å¼ºä¸“å®¶å·®å¼‚åŒ–æœºåˆ¶
        # 1. ç‰¹å¾é€‰æ‹©æ€§å…³æ³¨ (æ›´å¼ºçš„å·®å¼‚åŒ–)
        attention_mask = torch.sigmoid(self.specialization_weights)  # [D] -> [0, 1]
        x_specialized = x * attention_mask.unsqueeze(0).unsqueeze(0)  # [B, T, D]

        # 2. ä¸“å®¶ç‰¹å®šçš„ç‰¹å¾å˜æ¢
        expert_transform = torch.tanh(self.expert_bias.unsqueeze(0).unsqueeze(0))  # [1, 1, D]
        x_transformed = x + 0.1 * expert_transform  # å°å¹…ç‰¹å¾è°ƒåˆ¶

        # 3. ç»“åˆåŸå§‹ç‰¹å¾ã€ä¸“ä¸šåŒ–ç‰¹å¾å’Œå˜æ¢ç‰¹å¾
        # è®©æ¯ä¸ªä¸“å®¶æœ‰ä¸åŒçš„æ··åˆæ¯”ä¾‹
        mix_weight = 0.3 + 0.4 * torch.sigmoid(self.specialization_weights.mean())  # [0.3, 0.7]
        x = mix_weight * x_specialized + (1 - mix_weight) * x_transformed

        # FFNå¤„ç† (ä¸“ä¸šåŒ–çš„ä¸»è¦è®¡ç®—)
        x = self.ffn(x)

        # BurstInpaintä¸“å®¶çš„é¢å¤–æ®‹å·®è·¯å¾„
        if self.expert_id == 2 and hasattr(self, 'inpaint_residual'):
            inpaint_contrib = self.inpaint_residual(residual)
            x = x + 0.1 * inpaint_contrib  # å°æƒé‡æ·»åŠ ä¿®å¤è´¡çŒ®

        # æ·»åŠ ä¸“å®¶ç‰¹å®šçš„è¾“å‡ºåç½® (å¢å¼ºå·®å¼‚åŒ–)
        output_bias = self.expert_bias * 0.1  # æ§åˆ¶è¾“å‡ºåç½®å¼ºåº¦
        x = x + output_bias.unsqueeze(0).unsqueeze(0)

        # æ®‹å·®è¿æ¥
        return x + residual

    # ---- æ¡ä»¶æ³¨å…¥ APIï¼ˆä¿æŒ forward æ¥å£ä¸å˜ï¼‰----
    def set_f0_condition(self, cond: torch.Tensor):
        """Set per-sample F0-related conditioning vector.

        Accepts [B, C] (sample-level) or [B, T, C] (token-level). Stored transiently.
        """
        self._f0_cond = cond

    def clear_condition(self):
        self._f0_cond = None

    def get_specialization_info(self):
        """è·å–ä¸“å®¶ä¸“ä¸šåŒ–ä¿¡æ¯ - åŸºäºéŸ³é¢‘åœºæ™¯åŠŸèƒ½"""
        specializations = {
            0: "Harmonic Expert (tonal stability, periodic structure)",
            1: "Transient Expert (dynamic changes, burst detection)",
            2: "BurstInpaint Expert (context continuity, repair)",
            3: "LowSNR Expert (feature noise analysis, quality assessment)"
        }
        audio_scenarios = {
            0: "Voiced speech, musical tones, stable pitch",
            1: "Consonants, percussive sounds, rapid changes",
            2: "Packet loss, gaps, missing segments",
            3: "Noisy raw features, poor feature quality"
        }
        return {
            'expert_id': self.expert_id,
            'specialization': specializations.get(self.expert_id, "Unknown"),
            'audio_scenario': audio_scenarios.get(self.expert_id, "Unknown"),
            'bias_norm': self.expert_bias.norm().item(),
            'spec_weight_norm': self.specialization_weights.norm().item()
        }


# æ³¨æ„ï¼šTransientExpert ç°åœ¨ç”± UnifiedAudioExpert ä»£æ›¿


# æ³¨æ„ï¼šBurstInpaintExpert ç°åœ¨ç”± UnifiedAudioExpert ä»£æ›¿


# æ³¨æ„ï¼šLowSNRExpert ç°åœ¨ç”± UnifiedAudioExpert ä»£æ›¿


class SpecializedMicroMoE(nn.Module):
    """ä¸“ä¸šåŒ–MicroMoE - åŸºäºéŸ³é¢‘å†…å®¹çš„token-levelè·¯ç”±

    Features:
    - 4ä¸ªä¸“ä¸šåŒ–expert: Harmonic/Transient/BurstInpaint/LowSNR
    - Acoustic-aware routeræ›¿ä»£global_pool
    - Token-level routingæ”¯æŒæ—¶åºå†…çš„åŠ¨æ€experté€‰æ‹©
    - CSI integration for Stage3 ablation compatibility
    """
    def __init__(
        self,
        D: int = 128,
        d_csi: int = 10,
        n_experts: int = 4,
        topk: int = 2,
        expert_dropout: float = 0.0,
        balance_weight: float = 0.5,
        router_use_csi: bool = True,
        use_token_level: bool = True,
    ):
        super().__init__()
        self.d_model = D
        self.d_csi = d_csi
        self.n_experts = n_experts
        self.topk = topk
        self.expert_dropout = expert_dropout
        self.balance_weight = balance_weight
        self.router_use_csi = router_use_csi
        self.use_token_level = use_token_level
        self.router_jitter = 0.0  # è®­ç»ƒæ€çš„å°æŠ–åŠ¨å¼ºåº¦ï¼Œå¤–éƒ¨å¯æ³¨å…¥
        # åŒå±‚acoustic feature extractor: åŸå§‹å£°å­¦ç‰¹å¾ + ç¼–ç ç‰¹å¾ - ä¼ é€’ä¸“å®¶æ•°é‡
        self.acoustic_extractor = AcousticFeatureExtractor(d_raw=36, d_model=D, feature_dim=64, n_experts=n_experts)

        # Enhanced router: çº¯éŸ³é¢‘ç‰¹å¾è·¯ç”± - acoustic_features(64) + global_stats(16) -> experts(4)
        # ä¸å†ä½¿ç”¨CSIï¼Œæ”¹ä¸ºåŸºäºéŸ³é¢‘å†…å®¹æœ¬èº«çš„ç‰¹å¾è¿›è¡Œä¸“å®¶è·¯ç”±
        acoustic_dim = 64
        global_stats_dim = 16  # é¢‘è°±è´¨å¿ƒå˜åŒ– + è°±æ»šé™
        router_input_dim = acoustic_dim + global_stats_dim  # ç§»é™¤CSIä¾èµ–

        if use_token_level:
            # Token-level router: å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥çš„è·¯ç”±
            self.token_router = nn.Sequential(
                nn.Linear(D, 64),  # æ¯ä¸ªtokençš„ç‰¹å¾
                nn.ReLU(),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_experts)
            )

        # Sample-level router: åŸºäºacoustic features
        hidden_dim = max(32, router_input_dim // 2)
        self.sample_router = nn.Sequential(
            nn.Linear(router_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, n_experts)
        )

        # Routeråˆå§‹åŒ–ï¼šç¡®ä¿å‡åŒ€çš„ä¸“å®¶é€‰æ‹©å’Œæ•°å€¼ç¨³å®šæ€§
        for router in [self.sample_router, self.token_router if use_token_level else None]:
            if router is not None:
                with torch.no_grad():
                    # ä¿®å¤1: ä½¿ç”¨æ›´æ¸©å’Œçš„åç½®åˆå§‹åŒ–ï¼Œé¿å…æç«¯å€¼
                    router[-1].bias.fill_(-0.1)  # æ¸©å’Œçš„è´Ÿå€¼ï¼Œé¿å…è¿‡åº¦é›†ä¸­
                    # ä¿®å¤2: å¢å¤§è·¯ç”±å±‚æƒé‡æ–¹å·®ï¼Œç¡®ä¿è¶³å¤Ÿçš„å­¦ä¹ èƒ½åŠ›
                    nn.init.normal_(router[-1].weight, mean=0.0, std=0.05)

                    # ä¿®å¤3: ä¸ºéšè—å±‚ä¹Ÿæ·»åŠ åˆç†çš„åˆå§‹åŒ–
                    for layer in router[:-1]:
                        if isinstance(layer, nn.Linear):
                            nn.init.xavier_uniform_(layer.weight, gain=0.5)
                            if layer.bias is not None:
                                nn.init.zeros_(layer.bias)

        # ä½¿ç”¨ç»Ÿä¸€çš„ç®€åŒ–ä¸“å®¶æ¶æ„ - è®©è·¯ç”±å™¨å†³å®šä¸“ä¸šåŒ–
        # ä¸å†é¢„è®¾ä¸“å®¶åŠŸèƒ½ï¼Œè€Œæ˜¯é€šè¿‡è·¯ç”±å™¨å­¦ä¹ è‡ªç„¶åˆ†å·¥ï¼š
        # Expert 0 -> å¯èƒ½ç‰¹åŒ–ä¸ºè°æ³¢å¤„ç†
        # Expert 1 -> å¯èƒ½ç‰¹åŒ–ä¸ºç¬æ€å¤„ç†
        # Expert 2 -> å¯èƒ½ç‰¹åŒ–ä¸ºä¿®å¤å¤„ç†
        self.experts = nn.ModuleList([
            UnifiedAudioExpert(
                d_model=D,
                expert_id=i,
                use_f0_condition=(i == 0),  # ä»…å¯¹è°æ³¢ä¸“å®¶å¯ç”¨F0æ¡ä»¶åŒ–
                f0_cond_dim=6
            ) for i in range(n_experts)
        ])

        # Expert utilization tracking
        self.register_buffer('expert_counts', torch.zeros(n_experts))
        self.register_buffer('total_samples', torch.tensor(0.0))

        # ä¸“å®¶ä¸“ä¸šåŒ–å¼•å¯¼æœºåˆ¶
        self.register_buffer('expert_signal_types', torch.zeros(n_experts, 3))  # [harmonic, transient, noise]

        # è®­ç»ƒæ­¥æ•°è·Ÿè¸ªï¼ˆç”¨äºæ¸©åº¦é€€ç«ï¼‰
        self._current_training_step = 0
        self.register_buffer('expert_update_counts', torch.zeros(n_experts))

    def set_training_step(self, step: int):
        """è®¾ç½®å½“å‰è®­ç»ƒæ­¥æ•°ï¼Œç”¨äºæ¸©åº¦é€€ç«ç­‰æœºåˆ¶"""
        self._current_training_step = step

        # ç”±äºä½¿ç”¨ç»Ÿä¸€æ¶æ„ï¼Œç§»é™¤ä¸“å®¶ç‰¹å®šçš„å­¦ä¹ ç‡å€æ•°
        # è®©æ‰€æœ‰ä¸“å®¶åœ¨ç›¸åŒæ¡ä»¶ä¸‹å…¬å¹³ç«äº‰å’Œå­¦ä¹ 
        self.register_buffer('expert_lr_multipliers', torch.ones(n_experts))

        # ğŸ”§ å‚æ•°åˆå§‹åŒ–ç¨³å®šåŒ– - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        self._init_parameters()

    def _init_parameters(self):
        """ç»Ÿä¸€çš„ç®€æ´åˆå§‹åŒ–ç­–ç•¥ - æ‰€æœ‰ä¸“å®¶ä½¿ç”¨ç›¸åŒçš„ç¨³å®šåˆå§‹åŒ–"""
        for expert in self.experts:
            # ç»Ÿä¸€çš„æ ‡å‡†åˆå§‹åŒ–ï¼Œè®©è®­ç»ƒè¿‡ç¨‹è‡ªç„¶åˆ†åŒ–
            for module in expert.modules():
                if isinstance(module, nn.Linear):
                    # ä½¿ç”¨æ ‡å‡†Xavieråˆå§‹åŒ–
                    nn.init.xavier_uniform_(module.weight, gain=1.0)
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                elif isinstance(module, nn.LayerNorm):
                    # LayerNormæ ‡å‡†åˆå§‹åŒ–
                    nn.init.ones_(module.weight)
                    nn.init.zeros_(module.bias)

    # æ—§çš„å¤æ‚åˆå§‹åŒ–æ–¹æ³•å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨ç»Ÿä¸€ç®€æ´çš„åˆå§‹åŒ–

        # å¯¹acoustic feature extractoråº”ç”¨æ ‡å‡†åˆå§‹åŒ–
        for module in self.acoustic_extractor.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, (nn.Conv1d, nn.Conv2d)):
                nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, (nn.LSTM, nn.GRU)):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param, gain=1.0)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
                        if 'bias_hh' in name and 'GRU' in str(type(module)):
                            hidden_size = param.size(0) // 3
                            param.data[hidden_size:2*hidden_size].fill_(1.0)

    def update_expert_usage(self, assignments: torch.Tensor):
        """Update expert usage statistics with numerical safety."""
        with torch.no_grad():  # ç¡®ä¿ç»Ÿè®¡æ›´æ–°ä¸å‚ä¸æ¢¯åº¦å›¾
            # assignments: [B, E]
            counts = assignments.sum(dim=0)  # [E]
            total = assignments.sum()

            # EMA update with clamping
            momentum = 0.99
            self.expert_counts = momentum * self.expert_counts + (1 - momentum) * counts.detach()
            self.total_samples = momentum * self.total_samples + (1 - momentum) * total.detach()

    def load_balance_loss(self, gate_logits: torch.Tensor) -> torch.Tensor:
        """Compute load balance loss to encourage uniform expert usage with enhanced numerical stability."""
        # å¼ºåˆ¶fp32è®¡ç®—é¿å…bf16ä¸‹çš„æ•°å€¼é—®é¢˜
        gate_logits = gate_logits.float().clamp_(-15, 15)  # å‡å°èŒƒå›´ï¼Œå¢å¼ºç¨³å®šæ€§
        probs = F.softmax(gate_logits, dim=-1)  # [B, E]
        mean_probs = probs.mean(dim=0)  # [E]

        # ä¿®å¤: æ·»åŠ æ¸©åº¦ç¼©æ”¾ï¼Œé˜²æ­¢æç«¯åˆ†å¸ƒï¼ˆä¸è·¯ç”±å™¨æ¸©åº¦ä¸€è‡´ï¼‰
        temperature = 3.0  # ä¸è·¯ç”±å™¨æ¸©åº¦ä¸€è‡´
        smooth_probs = F.softmax(gate_logits / temperature, dim=-1).mean(dim=0)

        # ä½¿ç”¨æ›´ç¨³å®šçš„KLæ•£åº¦æŸå¤±ï¼Œä½†æ·»åŠ æ¸©åº¦æ­£åˆ™
        uniform = torch.full_like(mean_probs, 1.0 / self.n_experts)

        # ç»“åˆMSEå’ŒKLæ•£åº¦ï¼Œå¢å¼ºç¨³å®šæ€§
        mse_loss = F.mse_loss(smooth_probs, uniform)
        kl_loss = F.kl_div(
            torch.log(mean_probs + 1e-8),
            uniform,
            reduction='sum'
        ) / self.n_experts

        # åŠ æƒç»„åˆï¼ŒMSEæ›´ç¨³å®šï¼ŒKLæ›´æœ‰æ•ˆ
        combined_loss = 0.7 * mse_loss + 0.3 * kl_loss

        # ç¡®ä¿æŸå¤±ä¸ºæ­£ä¸”æœ‰æ„ä¹‰çš„æ¢¯åº¦ï¼Œä½†é™åˆ¶ä¸Šç•Œé˜²æ­¢çˆ†ç‚¸
        return combined_loss.clamp(min=1e-8, max=1.0)

    def forward(self, h: torch.Tensor, csi_vec: torch.Tensor = None, x_raw: torch.Tensor = None, dual_streams: dict = None) -> torch.Tensor:
        """
        ä¸“ä¸šåŒ–MoEå‰å‘ä¼ æ’­ - åŸºäºä¸‰å±‚ç‰¹å¾çš„æ™ºèƒ½è·¯ç”±

        Args:
            h: [B, T, D] èåˆåç‰¹å¾ (DualStreamè¾“å‡º)
            csi_vec: [B, d_csi] optional CSI vector for Stage3 compatibility
            x_raw: [B, T, 36] åŸå§‹è¾“å…¥ç‰¹å¾
            dual_streams: dict containing {'ribbon_stream', 'thread_stream', 'fused_stream'}

        Returns:
            output: [B, T, D] expert-processed features
        """
        b, t, d = h.shape

        # ğŸš¨ ç´§æ€¥è°ƒè¯•æ¨¡å¼ï¼šå®Œå…¨ç»•è¿‡MoEé€»è¾‘ï¼Œä½†ä»è®°å½•ç»Ÿè®¡
        if hasattr(self, '_emergency_bypass') and self._emergency_bypass:
            # åœ¨ç»•è¿‡æ¨¡å¼ä¸‹ï¼Œæ¨¡æ‹Ÿå‡åŒ€çš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
            if self.training:
                with torch.no_grad():
                    # åˆ›å»ºå‡åŒ€åˆ†å¸ƒçš„å‡ä¸“å®¶åˆ†é…ç”¨äºç»Ÿè®¡
                    uniform_assignments = torch.ones(b, self.n_experts, device=h.device) / self.n_experts
                    self.update_expert_usage(uniform_assignments)
            return h  # ç›´æ¥è¿”å›è¾“å…¥ï¼Œå®Œå…¨è·³è¿‡MoE

        # æ£€æŸ¥æ˜¯å¦æä¾›å®Œæ•´ç‰¹å¾
        if x_raw is None or dual_streams is None:
            # å…¼å®¹æ¨¡å¼ï¼šç›´æ¥è¿”å›è¾“å…¥ç‰¹å¾ï¼Œé¿å…acoustic_extractorçš„æ½œåœ¨é—®é¢˜
            return h
        else:
            # å°è¯•ä¸‰å±‚ç‰¹å¾æå–ï¼Œä½†æ·»åŠ å¼‚å¸¸æ•è·
            try:
                # 1. ä¸‰å±‚ç‰¹å¾æå–ï¼šåŸå§‹å£°å­¦ + Ribbonè¯­ä¹‰ + Threadå¾®éŸ³æ®µ
                ribbon_stream = dual_streams.get('ribbon_stream', h)  # [B,T,128]
                thread_stream = dual_streams.get('thread_stream', h)  # [B,T,128]
                acoustic_features = self.acoustic_extractor(x_raw, ribbon_stream, thread_stream)  # [B, 64]
                acoustic_features = torch.nan_to_num(acoustic_features, nan=0.0, posinf=1e4, neginf=-1e4)
            except Exception as e:
                print(f"Warning: acoustic_extractor failed: {e}, falling back to bypass mode")
                return h

        # 2. Prepare enhanced router input with additional context
        # åŸºç¡€å£°å­¦ç‰¹å¾
        base_router_input = acoustic_features  # [B, 64]

        # æ·»åŠ å…¨å±€ç»Ÿè®¡ç‰¹å¾å¢å¼ºè·¯ç”±å†³ç­–
        with torch.no_grad():
            # è®¡ç®—è¾“å…¥ç‰¹å¾çš„å…¨å±€ç»Ÿè®¡
            h_mean = h.mean(dim=1)  # [B, D] - åºåˆ—å‡å€¼
            h_std = h.std(dim=1)    # [B, D] - åºåˆ—æ–¹å·®
            h_max, _ = h.max(dim=1) # [B, D] - åºåˆ—æœ€å¤§å€¼

            # æå–å…³é”®ç»Ÿè®¡ç‰¹å¾ï¼ˆé™ç»´åˆ°16ç»´ï¼‰
            global_stats = torch.cat([
                h_mean[:, :8],   # å‰8ç»´å‡å€¼
                h_std[:, :4],    # å‰4ç»´æ ‡å‡†å·®
                h_max[:, :4]     # å‰4ç»´æœ€å¤§å€¼
            ], dim=-1)  # [B, 16]

        # ç»„åˆè·¯ç”±è¾“å…¥ï¼šçº¯éŸ³é¢‘ç‰¹å¾è·¯ç”±ï¼Œä¸ä½¿ç”¨CSI
        # ä¸“å®¶è·¯ç”±åŸºäºéŸ³é¢‘å†…å®¹ï¼šHarmonic/Transient/BurstInpaint/LowSNRéƒ½ä»éŸ³é¢‘ç‰¹å¾ä¸­åˆ†æ
        router_input = torch.cat([base_router_input, global_stats], dim=-1)  # [B, 64+16] çº¯éŸ³é¢‘ç‰¹å¾

        # 3. Sample-level routing based on acoustic preferences
        sample_gate_logits = self.sample_router(router_input)  # [B, E]
        # å¼ºåˆ¶æ•°å€¼ç¨³å®šæ€§ï¼šfp32è·¯ç”±è®¡ç®— + å®‰å…¨å½’ä¸€åŒ–
        sample_gate_logits = sample_gate_logits.float().clamp_(-15, 15)  # fp32 + æ›´ä¿å®ˆçš„æˆªæ–­

        # ä¿®å¤ï¼šæ¸è¿›å¼æ¸©åº¦é€€ç« - ä»æ¢ç´¢åˆ°ä¸“ä¸šåŒ–
        # è·å–è®­ç»ƒæ­¥æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        training_step = getattr(self, '_current_training_step', 0)

        # æ¸©åº¦é€€ç«ç­–ç•¥ï¼šå¼€å§‹é«˜æ¸©æ¢ç´¢ï¼Œé€æ­¥é™æ¸©ä¸“ä¸šåŒ–
        initial_temp = 2.5    # åˆå§‹æ¸©åº¦ï¼šé€‚åº¦æ¢ç´¢
        final_temp = 1.0      # æœ€ç»ˆæ¸©åº¦ï¼šæ˜ç¡®ä¸“ä¸šåŒ–
        annealing_steps = 5000  # é€€ç«æ­¥æ•°

        if training_step < annealing_steps:
            progress = training_step / annealing_steps
            routing_temperature = initial_temp - (initial_temp - final_temp) * progress
        else:
            routing_temperature = final_temp

        sample_gate_logits = sample_gate_logits / routing_temperature

        # >>> ROUTER EXPLORATION JITTER <<<
        if self.training and getattr(self, 'router_jitter', 0.0) > 0.0:
            j = float(self.router_jitter)
            sample_gate_logits = sample_gate_logits + j * torch.randn_like(sample_gate_logits)
        # <<< END JITTER <<<

        sample_probs = F.softmax(sample_gate_logits, dim=-1)  # fp32 softmax

        # ä¿å­˜è·¯ç”±ä¿¡æ¯ç”¨äºè¯Šæ–­ï¼ˆä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼‰
        if self.training:
            self._last_router_logits = sample_gate_logits.detach()

        if self.use_token_level:
            # 4. Token-level routing for fine-grained control
            token_gate_logits = self.token_router(h)  # [B, T, E]
            # å¼ºåˆ¶æ•°å€¼ç¨³å®šæ€§ï¼šfp32è·¯ç”±è®¡ç®— + å®‰å…¨å½’ä¸€åŒ–
            token_gate_logits = token_gate_logits.float().clamp_(-15, 15)  # fp32 + æ›´ä¿å®ˆçš„æˆªæ–­

            # ä¿®å¤ï¼šæ·»åŠ æ¸©åº¦ç¼©æ”¾é˜²æ­¢æç«¯è·¯ç”±å†³ç­–
            token_gate_logits = token_gate_logits / routing_temperature

            # >>> ROUTER EXPLORATION JITTER (token-level) <<<
            if self.training and getattr(self, 'router_jitter', 0.0) > 0.0:
                j = float(self.router_jitter)
                token_gate_logits = token_gate_logits + j * torch.randn_like(token_gate_logits)
            # <<< END JITTER <<<

            token_probs = F.softmax(token_gate_logits, dim=-1)  # fp32 softmax

            # èåˆsample-levelå’Œtoken-level routing
            # Sampleåå¥½ä½œä¸ºå…ˆéªŒï¼Œtokenç»†åŒ–å±€éƒ¨å†³ç­–
            sample_probs_expanded = sample_probs.unsqueeze(1).expand(b, t, self.n_experts)  # [B, T, E]
            combined_probs = 0.6 * sample_probs_expanded + 0.4 * token_probs  # [B, T, E]

            # Token-level top-k selection
            weights, indices = torch.topk(combined_probs, k=self.topk, dim=-1)  # [B, T, k]
            weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-6)  # å®‰å…¨å½’ä¸€åŒ–
            weights = weights.to(h.dtype)  # å›åˆ°åŸ dtypeï¼ˆbf16/fp16ï¼‰

            # Create token-level assignment matrix
            assignments = torch.zeros_like(combined_probs)  # [B, T, E]
            assignments.scatter_(-1, indices, weights)  # [B, T, E]

            # Expert dropout
            if self.training and self.expert_dropout > 0:
                dropout_mask = (torch.rand(b, t, self.n_experts, device=h.device) > self.expert_dropout).float()
                assignments = assignments * dropout_mask
                assignments = assignments / (assignments.sum(dim=-1, keepdim=True) + 1e-6)

            # Apply experts with token-level weighting
            # Optional F0 conditioning vector from raw features
            f0_cond = None
            if x_raw is not None:
                try:
                    f0_cond = extract_acoustic_priors(x_raw)  # [B,6]
                except Exception:
                    f0_cond = None

            expert_outputs = []
            for i, expert in enumerate(self.experts):
                if (f0_cond is not None) and getattr(expert, 'use_f0_condition', False) and hasattr(expert, 'set_f0_condition'):
                    expert.set_f0_condition(f0_cond)
                expert_out = expert(h)  # [B, T, D]
                if hasattr(expert, 'clear_condition'):
                    expert.clear_condition()
                expert_outputs.append(expert_out)

            expert_stack = torch.stack(expert_outputs, dim=-1)  # [B, T, D, E]

            # Token-level weighted combination
            output = torch.einsum('btde,bte->btd', expert_stack, assignments)  # [B, T, D]

            # Usage tracking (sample-level for consistency)
            if self.training:
                sample_assignments = assignments.mean(dim=1)  # [B, E] - average over time
                self.update_expert_usage(sample_assignments.detach())

        else:
            # Sample-level routing (fallback)
            weights, indices = torch.topk(sample_probs, k=self.topk, dim=-1)  # [B, k]
            weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-6)
            weights = weights.to(h.dtype)  # å›åˆ°åŸ dtype

            assignments = torch.zeros_like(sample_probs)  # [B, E]
            assignments.scatter_(1, indices, weights)

            if self.training and self.expert_dropout > 0:
                dropout_mask = (torch.rand(b, self.n_experts, device=h.device) > self.expert_dropout).float()
                assignments = assignments * dropout_mask
                assignments = assignments / (assignments.sum(dim=-1, keepdim=True) + 1e-6)

            # Apply experts
            # Optional F0 conditioning vector from raw features
            f0_cond = None
            if x_raw is not None:
                try:
                    f0_cond = extract_acoustic_priors(x_raw)  # [B,6]
                except Exception:
                    f0_cond = None

            expert_outputs = []
            for i, expert in enumerate(self.experts):
                if (f0_cond is not None) and getattr(expert, 'use_f0_condition', False) and hasattr(expert, 'set_f0_condition'):
                    expert.set_f0_condition(f0_cond)
                expert_out = expert(h)  # [B, T, D]
                if hasattr(expert, 'clear_condition'):
                    expert.clear_condition()
                expert_outputs.append(expert_out)

            expert_stack = torch.stack(expert_outputs, dim=-1)  # [B, T, D, E]
            output = torch.einsum('btde,be->btd', expert_stack, assignments)  # [B, T, D]

            if self.training:
                self.update_expert_usage(assignments.detach())

        return output

    @torch._dynamo.disable  # ä¸è¿›å…¥ torch.compile å›¾ï¼Œå®‰å…¨ä½¿ç”¨ .item()
    def get_expert_utilization(self) -> torch.Tensor:
        """Expert utilization for logging/metrics only (kept outside compiled graph)."""
        if self.total_samples.item() > 0:
            util = self.expert_counts / (self.total_samples + 1e-8)
        else:
            util = torch.ones_like(self.expert_counts) / self.n_experts
        return util.detach()

    def get_expert_param_groups(self, base_lr: float = 1e-4):
        """è·å–ä¸“å®¶å‚æ•°ç»„ï¼Œç»Ÿä¸€å­¦ä¹ ç‡é…ç½®"""
        param_groups = []

        # æ‰€æœ‰ä¸“å®¶ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡é…ç½®
        for i, expert in enumerate(self.experts):
            param_groups.append({
                'params': list(expert.parameters()),
                'lr': base_lr,
                'name': f'unified_expert_{i}',
                'weight_decay': 1e-6  # ç»Ÿä¸€çš„æƒé‡è¡°å‡
            })

        # å…¶ä»–æ¨¡å—ä½¿ç”¨åŸºç¡€å­¦ä¹ ç‡
        other_params = []
        for name, module in self.named_children():
            if name != 'experts':
                other_params.extend(list(module.parameters()))

        if other_params:
            param_groups.append({
                'params': other_params,
                'lr': base_lr,
                'name': 'other_modules',
                'weight_decay': 1e-6
            })

        return param_groups

    def get_routing_analysis(self) -> dict:
        """åˆ†æè·¯ç”±å™¨çš„å­¦ä¹ æˆæœï¼Œæ­ç¤ºä¸“å®¶ç‰¹åŒ–æ–¹å‘"""
        analysis = {
            'expert_utilization': self.get_expert_utilization().tolist(),
            'routing_entropy': None,
            'specialization_info': {}
        }

        # è®¡ç®—è·¯ç”±ç†µï¼ˆå¤šæ ·æ€§æŒ‡æ ‡ï¼‰
        util = self.get_expert_utilization()
        if util.sum() > 0:
            util_norm = util / (util.sum() + 1e-8)
            routing_entropy = -(util_norm * torch.log(util_norm + 1e-8)).sum().item()
            analysis['routing_entropy'] = routing_entropy

        # åˆ†ææ¯ä¸ªä¸“å®¶çš„ç‰¹åŒ–æƒ…å†µ
        for i in range(len(self.experts)):
            expert_util = util[i].item()
            if expert_util > 0.1:  # åªåˆ†æä½¿ç”¨ç‡è¶…è¿‡10%çš„ä¸“å®¶
                specialization = "unknown"
                if i == 0 and expert_util > 0.4:
                    specialization = "likely_harmonic"  # è°æ³¢ç‰¹åŒ–
                elif i == 1 and expert_util > 0.4:
                    specialization = "likely_transient"  # ç¬æ€ç‰¹åŒ–
                elif i == 2 and expert_util > 0.4:
                    specialization = "likely_inpaint"  # ä¿®å¤ç‰¹åŒ–

                analysis['specialization_info'][f'expert_{i}'] = {
                    'utilization': expert_util,
                    'likely_specialization': specialization,
                    'parameters': sum(p.numel() for p in self.experts[i].parameters())
                }

        return analysis

    def print_routing_summary(self):
        """æ‰“å°è·¯ç”±å™¨å­¦ä¹ æˆæœçš„ç®€æ´æ‘˜è¦"""
        analysis = self.get_routing_analysis()

        print("[MoE Router Learning Summary]")
        print(f"   Expert Utilization: {[f'{u:.3f}' for u in analysis['expert_utilization']]}")
        print(f"   Routing Entropy: {analysis.get('routing_entropy', 0.0):.3f} (higher = more diverse)")

        for expert_id, info in analysis['specialization_info'].items():
            specialization = info['likely_specialization']
            util = info['utilization']
            print(f"   {expert_id}: {util:.1%} usage -> {specialization}")


    def get_aux_losses(
        self,
        h: torch.Tensor,
        csi_vec: torch.Tensor = None,
        x_raw: torch.Tensor = None,
        dual_streams: Dict[str, torch.Tensor] | None = None,
    ) -> Dict[str, torch.Tensor]:
        aux_losses: Dict[str, torch.Tensor] = {}
        b = h.size(0)

        # 1) å‡†å¤‡è·¯ç”±è¾“å…¥ï¼ˆä¸ forward åŒæ­¥ï¼‰
        if x_raw is not None and dual_streams is not None:
            ribbon_stream = dual_streams.get('ribbon_stream', h)
            thread_stream = dual_streams.get('thread_stream', h)
            acoustic_features = self.acoustic_extractor(x_raw, ribbon_stream, thread_stream)  # [B,64]
        else:
            acoustic_features = torch.zeros(b, 64, device=h.device, dtype=h.dtype)

        # çº¯éŸ³é¢‘ç‰¹å¾è·¯ç”±ï¼šä¸forwardæ–¹æ³•ä¿æŒä¸€è‡´ï¼Œæ·»åŠ global_stats
        # è®¡ç®—global_stats - ä¸forwardæ–¹æ³•å®Œå…¨ä¸€è‡´
        h_mean = h.mean(dim=1)  # [B, D]
        h_std = h.std(dim=1, unbiased=False)  # [B, D]
        h_max = h.max(dim=1)[0]  # [B, D]

        # ç¡®ä¿å–è¶³å¤Ÿçš„ç»´åº¦
        n_dims = min(4, h_mean.size(-1))  # é¿å…è¶…å‡ºhçš„å®é™…ç»´åº¦
        global_stats = torch.cat([
            h_mean[:, :n_dims],   # å‰n_dimsç»´å‡å€¼
            h_std[:, :n_dims],    # å‰n_dimsç»´æ ‡å‡†å·®
            h_max[:, :n_dims]     # å‰n_dimsç»´æœ€å¤§å€¼
        ], dim=-1)  # [B, n_dims*3]

        # å¦‚æœglobal_statsä¸è¶³16ç»´ï¼Œè¡¥é›¶åˆ°16ç»´
        if global_stats.size(-1) < 16:
            padding = torch.zeros(global_stats.size(0), 16 - global_stats.size(-1), device=global_stats.device, dtype=global_stats.dtype)
            global_stats = torch.cat([global_stats, padding], dim=-1)

        router_input = torch.cat([acoustic_features, global_stats], dim=-1)  # [B, acoustic_dim+16] ä¸forwardä¸€è‡´

        # 2) Sample-level å¹³è¡¡æŸå¤±
        sample_gate_logits = self.sample_router(router_input).float().clamp_(-20, 20)  # æ•°å€¼ç¨³å®š
        aux_losses['moe_balance_loss'] = self.load_balance_loss(sample_gate_logits)     # ç»´æŒä¸º tensor

        # 3) Token-level å¹³è¡¡æŸå¤±ï¼ˆè‹¥å¯ç”¨ token routerï¼‰
        if self.use_token_level:
            token_gate_logits = self.token_router(h).float().clamp_(-20, 20)
            e = token_gate_logits.shape[-1]
            token_gate_flat = token_gate_logits.view(-1, e)                              # å…³é”®ï¼šé¿å… UnboundLocalError
            aux_losses['moe_token_balance_loss'] = self.load_balance_loss(token_gate_flat)

        # 4) ä¸“å®¶å·®å¼‚åŒ–æŸå¤± - ä¸´æ—¶ç¦ç”¨ä»¥å‡å°‘ä¸å¿…è¦çº¦æŸ
        # diversification_loss = self.compute_expert_diversification_loss()
        # aux_losses['expert_diversification_loss'] = diversification_loss
        # æ³¨é‡Šï¼šå½“å‰å·®å¼‚åŒ–æŸå¤±è¿‡é«˜(moe_d=0.0112)ï¼Œå¯èƒ½é˜»ç¢ä¸“å®¶ç³»ç»Ÿæ€§èƒ½æå‡

        # 5) å¯é€‰çš„åå¥½/ç¨€ç–åº¦æŒ‡æ ‡ï¼ˆä»…åšç›‘æ§ï¼Œä¸ç›´æ¥å…¥ lossï¼‰
        if x_raw is not None and dual_streams is not None:
            with torch.no_grad():
                aux_losses['moe_harmonic_pref'] = acoustic_features.mean()
        return aux_losses

    def compute_expert_diversification_loss(self) -> torch.Tensor:
        """è®¡ç®—ä¸“å®¶å·®å¼‚åŒ–æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶å­¦ä¼šä¸åŒçš„ç‰¹å¾è¡¨ç¤º"""
        # æ”¶é›†æ‰€æœ‰ä¸“å®¶çš„å…³é”®å‚æ•°
        expert_biases = []
        expert_spec_weights = []

        for expert in self.experts:
            expert_biases.append(expert.expert_bias.flatten())  # [D]
            expert_spec_weights.append(expert.specialization_weights.flatten())  # [D]

        # å°†ä¸“å®¶å‚æ•°å †å  [n_experts, D]
        biases_stack = torch.stack(expert_biases, dim=0)  # [E, D]
        spec_weights_stack = torch.stack(expert_spec_weights, dim=0)  # [E, D]

        # è®¡ç®—ä¸“å®¶é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        bias_similarity = torch.mm(biases_stack, biases_stack.t())  # [E, E]
        spec_similarity = torch.mm(spec_weights_stack, spec_weights_stack.t())  # [E, E]

        # å»é™¤å¯¹è§’çº¿ï¼ˆä¸“å®¶ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        mask = ~torch.eye(self.n_experts, device=biases_stack.device, dtype=torch.bool)
        bias_off_diag = bias_similarity[mask]
        spec_off_diag = spec_similarity[mask]

        # é¼“åŠ±ä¸“å®¶é—´å·®å¼‚åŒ–ï¼šç›¸ä¼¼åº¦è¶Šå°è¶Šå¥½
        bias_div_loss = torch.relu(bias_off_diag).mean()  # æƒ©ç½šæ­£ç›¸ä¼¼åº¦
        spec_div_loss = torch.relu(spec_off_diag).mean()  # æƒ©ç½šæ­£ç›¸ä¼¼åº¦

        # ç»„åˆå·®å¼‚åŒ–æŸå¤±
        total_div_loss = 0.5 * bias_div_loss + 0.5 * spec_div_loss

        return total_div_loss.clamp(min=0.0, max=1.0)  # é™åˆ¶èŒƒå›´ï¼Œé˜²æ­¢çˆ†ç‚¸



class CompatibleMicroMoE(nn.Module):
    """å‘åå…¼å®¹çš„MicroMoEæ¥å£ - åŒ…è£…SpecializedMicroMoEæˆ–EnhancedMicroMoEWithBypass"""
    def __init__(self, d_model: int, n_experts: int = 4, top_k: int = 2, **kwargs):
        super().__init__()

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç›´æµé€šè·¯
        enable_direct_pathway = kwargs.pop('enable_direct_pathway', False)

        if enable_direct_pathway:
            # ä½¿ç”¨å¢å¼ºç‰ˆæœ¬EnhancedMicroMoEWithBypass
            enhanced_kwargs = {
                'D': d_model,
                'n_experts': n_experts,
                'topk': top_k,
            }
            # ä¼ é€’å¢å¼ºç‰ˆæœ¬çš„kwargs
            for key, value in kwargs.items():
                if key in ['d_csi', 'expert_dropout', 'balance_weight', 'router_use_csi', 'use_token_level',
                          'initial_bypass_weight', 'adaptive_threshold', 'pathway_warmup_steps']:
                    enhanced_kwargs[key] = value

            # æ·»åŠ é»˜è®¤çš„ç›´æµé€šè·¯å‚æ•°
            enhanced_kwargs['enable_direct_pathway'] = True
            self.specialized_moe = EnhancedMicroMoEWithBypass(**enhanced_kwargs)
            self._is_enhanced = True
        else:
            # ä½¿ç”¨æ ‡å‡†ç‰ˆæœ¬SpecializedMicroMoE
            specialized_kwargs = {
                'D': d_model,
                'n_experts': n_experts,
                'topk': top_k,
            }
            # ä¼ é€’å…¶ä»–kwargs
            for key, value in kwargs.items():
                if key in ['d_csi', 'expert_dropout', 'balance_weight', 'router_use_csi', 'use_token_level']:
                    specialized_kwargs[key] = value

            self.specialized_moe = SpecializedMicroMoE(**specialized_kwargs)
            self._is_enhanced = False

    def forward(self, h: torch.Tensor, router_input: torch.Tensor, x_raw: torch.Tensor = None, dual_streams: dict = None, training_step: int = 0) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """å…¼å®¹æ¥å£ - è¿”å›outputå’Œaux losses"""
        # router_inputç°åœ¨æ˜¯acoustic_features(64)æˆ–acoustic_features(64)+csi(10)
        # ç”±äºacoustic_extractorå·²ç»åœ¨ä¸Šå±‚è°ƒç”¨è¿‡ï¼Œè¿™é‡Œåªéœ€è¦è§£æCSI

        expected_acoustic_dim = 64  # AcousticFeatureExtractorè¾“å‡ºç»´åº¦
        if router_input.shape[-1] > expected_acoustic_dim:
            # åŒ…å«CSIçš„æƒ…å†µ: [acoustic_features(64) + csi(d_csi)]
            csi_dim = router_input.shape[-1] - expected_acoustic_dim
            if csi_dim == self.specialized_moe.d_csi:
                csi_vec = router_input[:, -csi_dim:]  # [B, d_csi]
            else:
                # CSIç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æˆªæ–­æˆ–é›¶å¡«å……
                if csi_dim > self.specialized_moe.d_csi:
                    csi_vec = router_input[:, -self.specialized_moe.d_csi:]  # æˆªæ–­
                else:
                    # é›¶å¡«å……
                    padding = torch.zeros(router_input.shape[0], self.specialized_moe.d_csi - csi_dim,
                                        device=router_input.device, dtype=router_input.dtype)
                    csi_vec = torch.cat([router_input[:, expected_acoustic_dim:], padding], dim=-1)
        elif router_input.shape[-1] == expected_acoustic_dim:
            # çº¯acoustic featuresï¼Œæ— CSI
            csi_vec = None
        else:
            # ç»´åº¦ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯legacyæ ¼å¼
            csi_vec = None

        if self._is_enhanced:
            # è°ƒç”¨å¢å¼ºç‰ˆæœ¬ï¼Œä¼ é€’training_step
            output = self.specialized_moe(h, csi_vec, x_raw=x_raw, dual_streams=dual_streams, training_step=training_step)
        else:
            # è°ƒç”¨æ ‡å‡†ç‰ˆæœ¬
            output = self.specialized_moe(h, csi_vec, x_raw=x_raw, dual_streams=dual_streams)

        aux_losses = self.specialized_moe.get_aux_losses(h, csi_vec, x_raw=x_raw, dual_streams=dual_streams)
        if 'balance_loss' in aux_losses:               # å…¼å®¹æ—§é”®
            aux_losses['moe_balance_loss'] = aux_losses.pop('balance_loss')
        if 'token_balance_loss' in aux_losses:         # å…¼å®¹æ—§é”®
            aux_losses['moe_token_balance_loss'] = aux_losses.pop('token_balance_loss')
        return output, aux_losses

    def get_expert_utilization(self):
        """å‘åå…¼å®¹æ–¹æ³•"""
        return self.specialized_moe.get_expert_utilization()

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¢å¼ºç‰ˆæœ¬ï¼‰"""
        if self._is_enhanced and hasattr(self.specialized_moe, 'get_performance_stats'):
            return self.specialized_moe.get_performance_stats()
        return {}

    def get_separated_outputs(self):
        """è·å–åˆ†ç¦»çš„è¾“å‡ºç”¨äºæŸå¤±è®¡ç®—ï¼ˆä»…å¢å¼ºç‰ˆæœ¬ï¼‰"""
        if self._is_enhanced and hasattr(self.specialized_moe, 'get_separated_outputs'):
            return self.specialized_moe.get_separated_outputs()
        return None, None

    def update_performance_ema(self, direct_loss: float, expert_loss: float):
        """æ›´æ–°æ€§èƒ½EMAï¼ˆä»…å¢å¼ºç‰ˆæœ¬ï¼‰"""
        if self._is_enhanced and hasattr(self.specialized_moe, 'update_performance_ema'):
            self.specialized_moe.update_performance_ema(direct_loss, expert_loss)


# ä¿æŒæ—§çš„MicroMoEç±»ä»¥å…¼å®¹å¯èƒ½çš„ç›´æ¥å¼•ç”¨
class MicroMoE(nn.Module):
    """Legacy MicroMoE - redirects to SpecializedMicroMoE for consistency"""
    def __init__(self, D: int = 128, d_csi: int = 10, n_experts: int = 4, topk: int = 2, **kwargs):
        super().__init__()
        # ç›´æ¥ä½¿ç”¨SpecializedMicroMoEï¼Œä½†ç¦ç”¨token-level routingä»¥ä¿æŒå…¼å®¹æ€§
        kwargs['use_token_level'] = kwargs.get('use_token_level', False)
        self.specialized_moe = SpecializedMicroMoE(
            D=D, d_csi=d_csi, n_experts=n_experts, topk=topk, **kwargs
        )

    def forward(self, h: torch.Tensor, router_input: torch.Tensor) -> torch.Tensor:
        """Legacy interface for basic MoE functionality"""
        # è§£ærouter_inputä»¥æå–CSI
        if router_input.shape[-1] > self.specialized_moe.d_model:
            csi_dim = router_input.shape[-1] - self.specialized_moe.d_model
            if csi_dim == self.specialized_moe.d_csi:
                csi_vec = router_input[:, -csi_dim:]
            else:
                csi_vec = None
        else:
            csi_vec = None

        return self.specialized_moe(h, csi_vec)

    def get_expert_utilization(self):
        return self.specialized_moe.get_expert_utilization()

    def get_aux_losses(self, h: torch.Tensor, csi_vec: torch.Tensor = None):
        return self.specialized_moe.get_aux_losses(h, csi_vec)


class EnhancedMicroMoEWithBypass(nn.Module):
    """å¸¦ç›´æµé€šè·¯çš„å¢å¼ºMicroMoE - ç”¨äºæ€§èƒ½å¯¹æ¯”éªŒè¯

    æ ¸å¿ƒç‰¹æ€§:
    - å¯é…ç½®çš„ç›´æµé€šè·¯ï¼Œæ”¯æŒç»•è¿‡ä¸“å®¶ç³»ç»Ÿ
    - åŠ¨æ€æƒé‡è°ƒåº¦ï¼ŒåŸºäºæ€§èƒ½è‡ªé€‚åº”è°ƒæ•´ä¸“å®¶vsç›´æµæƒé‡
    - åˆ†ç¦»æŸå¤±è®¡ç®—ï¼Œç‹¬ç«‹ç›‘æ§ä¸“å®¶å’Œç›´æµé€šè·¯æ€§èƒ½
    - æ¸è¿›å¼å¯ç”¨ï¼Œé¿å…è®­ç»ƒåˆæœŸä¸“å®¶è·¯ç”±æ¬¡ä¼˜è§£
    """

    def __init__(
        self,
        D: int = 128,
        d_csi: int = 10,
        n_experts: int = 3,
        topk: int = 2,
        expert_dropout: float = 0.0,
        balance_weight: float = 0.5,
        router_use_csi: bool = True,
        use_token_level: bool = True,
        # ç›´æµé€šè·¯ç›¸å…³å‚æ•°
        enable_direct_pathway: bool = True,
        initial_bypass_weight: float = 0.8,
        adaptive_threshold: float = 0.05,
        pathway_warmup_steps: int = 1500,
    ):
        super().__init__()

        # åŸºç¡€MoEé…ç½®
        self.d_model = D
        self.enable_direct_pathway = enable_direct_pathway
        self.adaptive_threshold = adaptive_threshold
        self.pathway_warmup_steps = pathway_warmup_steps

        # åŸå§‹ä¸“å®¶ç³»ç»Ÿ
        self.moe_system = SpecializedMicroMoE(
            D=D, d_csi=d_csi, n_experts=n_experts, topk=topk,
            expert_dropout=expert_dropout, balance_weight=balance_weight,
            router_use_csi=router_use_csi, use_token_level=use_token_level
        )

        # ä¸ºå…¼å®¹æ€§æ·»åŠ acoustic_extractoråˆ«å
        self.acoustic_extractor = self.moe_system.acoustic_extractor

        # ç›´æµé€šè·¯æ¨¡å—
        if enable_direct_pathway:
            # Stage1ç­‰æ•ˆçš„çœŸæ­£ç›´é€šè·¯å¾„ï¼šç®€å•è€Œé«˜æ•ˆ
            self.simple_direct_pathway = nn.Sequential(
                nn.LayerNorm(D),
                nn.Linear(D, D),
                nn.GELU(),
                nn.Linear(D, D)
            )

            # ä¼ ç»Ÿç›´é€šè·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
            self.direct_pathway = nn.Sequential(
                nn.LayerNorm(D),
                nn.Linear(D, D),
                nn.GELU(),
                nn.Linear(D, D)
            )

            # å¯å­¦ä¹ çš„æƒé‡å‚æ•°
            self.bypass_weight = nn.Parameter(torch.tensor(initial_bypass_weight))

            # æ€§èƒ½ç›‘æ§ç¼“å†²åŒº
            self.register_buffer('expert_loss_ema', torch.tensor(float('inf')))
            self.register_buffer('direct_loss_ema', torch.tensor(float('inf')))
            self.register_buffer('training_step_count', torch.tensor(0))

            # è¾“å‡ºç¼“å­˜ç”¨äºåˆ†ç¦»æŸå¤±è®¡ç®—
            self._last_direct_output = None
            self._last_expert_output = None

    def compute_pathway_weights(self, training_step: int):
        """åŸºäºè®­ç»ƒæ­¥æ•°å’Œæ€§èƒ½åŠ¨æ€è°ƒæ•´ç›´æµä¸ä¸“å®¶æƒé‡ï¼Œæ”¯æŒæ¶æ„çº§ç»•è¿‡

        Args:
            training_step: å½“å‰è®­ç»ƒæ­¥æ•°

        Returns:
            tuple: (bypass_weight, expert_weight)
        """
        if not self.enable_direct_pathway:
            return 0.0, 1.0

        # Stage3ä¸“é—¨ç­–ç•¥ï¼šå¿«é€Ÿæ¿€æ´»expertç³»ç»Ÿ
        if training_step < 500:
            # å‰500æ­¥æœ€å°å¯åŠ¨æœŸï¼šä½¿ç”¨è¾ƒä½bypassæƒé‡
            return 0.4, 0.6  # ä¼˜å…ˆexpertæ¨¡å¼

        # é˜¶æ®µ2: å¿«é€Ÿè¿‡æ¸¡åˆ°expertä¸»å¯¼
        elif training_step < self.pathway_warmup_steps:
            # å¿«é€Ÿè¡°å‡åˆ°ä¸“å®¶ä¸»å¯¼æ¨¡å¼
            progress = (training_step - 500) / max(1, self.pathway_warmup_steps - 500)
            bypass_weight = 0.4 - 0.1 * progress  # ä»0.4é™åˆ°0.3
            expert_weight = 1.0 - bypass_weight
            return bypass_weight, expert_weight

        # Stage3è®­ç»ƒç­–ç•¥ï¼šå¤§å¹…å»¶é•¿expertè®­ç»ƒæœŸ
        if training_step < 15000:  # ä»5000å»¶é•¿åˆ°15000æ­¥
            # å‰15000æ­¥å¼ºåˆ¶ä½¿ç”¨expertï¼Œå‡ ä¹ç¦ç”¨æ€§èƒ½æ¯”è¾ƒ
            self.bypass_weight.data = torch.clamp(self.bypass_weight.data * 0.998, 0.1, 0.3)
        elif training_step < 25000:  # 25000æ­¥å†…ä¿å®ˆè°ƒæ•´
            # ç¼“æ…¢è¿‡æ¸¡æœŸï¼šæ”¾å®½æ€§èƒ½æ¯”è¾ƒé˜ˆå€¼
            if (torch.isfinite(self.expert_loss_ema) and torch.isfinite(self.direct_loss_ema) and
                self.expert_loss_ema < float('inf') and self.direct_loss_ema < float('inf')):

                # è®¡ç®—æ€§èƒ½æ¯”ç‡ (ä¸“å®¶ / ç›´æµ)
                performance_ratio = self.expert_loss_ema / (self.direct_loss_ema + 1e-8)

                # æ”¾å®½é˜ˆå€¼ï¼šåªæœ‰åœ¨ä¸“å®¶çœŸæ­£æ˜¾è‘—æ›´å¥½æ—¶æ‰é™ä½bypass
                if performance_ratio < 0.8:  # ä¸“å®¶å¿…é¡»æ¯”directå¥½20%ä»¥ä¸Š
                    self.bypass_weight.data *= 0.995  # éå¸¸ä¿å®ˆçš„è¡°å‡
                # åªæœ‰åœ¨æ€§èƒ½æ¯”ç‡è¶…è¿‡2.0æ—¶æ‰å¢åŠ bypassï¼ˆæ¯”ä¹‹å‰çš„1.1å®½æ¾å¾ˆå¤šï¼‰
                elif performance_ratio > 2.0:
                    self.bypass_weight.data *= 1.001  # æä¿å®ˆçš„å¢åŠ 
        else:
            # 25000æ­¥åæ‰å¼€å§‹æ­£å¸¸çš„æ€§èƒ½æ¯”è¾ƒ
            if (torch.isfinite(self.expert_loss_ema) and torch.isfinite(self.direct_loss_ema) and
                self.expert_loss_ema < float('inf') and self.direct_loss_ema < float('inf')):

                performance_ratio = self.expert_loss_ema / (self.direct_loss_ema + 1e-8)

                if performance_ratio < (1.0 - self.adaptive_threshold):
                    self.bypass_weight.data *= 0.99
                elif performance_ratio > (1.0 + self.adaptive_threshold):
                    self.bypass_weight.data *= 1.002

        # æƒé‡é™åˆ¶ï¼šå¼ºåˆ¶åº”ç”¨clamp
        self.bypass_weight.data = torch.clamp(self.bypass_weight.data, 0.1, 0.85)
        bypass_weight = torch.clamp(self.bypass_weight, 0.1, 0.85)
        expert_weight = 1.0 - bypass_weight

        return float(bypass_weight), float(expert_weight)

    def forward(
        self,
        h: torch.Tensor,
        csi_vec: torch.Tensor = None,
        x_raw: torch.Tensor = None,
        dual_streams: dict = None,
        training_step: int = 0
    ) -> torch.Tensor:
        """å¢å¼ºå‰å‘ä¼ æ’­ï¼Œæ”¯æŒæ¶æ„çº§ç›´é€šè·¯å¾„ä¸ä¸“å®¶ç³»ç»Ÿæ··åˆ

        Args:
            h: [B, T, D] è¾“å…¥ç‰¹å¾
            csi_vec: [B, d_csi] ä¿¡é“çŠ¶æ€ä¿¡æ¯
            x_raw: [B, T, 36] åŸå§‹å£°å­¦ç‰¹å¾
            dual_streams: åŒæµç‰¹å¾å­—å…¸
            training_step: å½“å‰è®­ç»ƒæ­¥æ•°

        Returns:
            torch.Tensor: [B, T, D] å¤„ç†åçš„ç‰¹å¾
        """
        if not self.enable_direct_pathway:
            # æ ‡å‡†MoEæ¨¡å¼
            return self.moe_system(h, csi_vec, x_raw, dual_streams)

        # æ›´æ–°æ­¥æ•°è®¡æ•°
        if self.training:
            self.training_step_count += 1

        # 1. è®¡ç®—åŠ¨æ€æƒé‡
        bypass_weight, expert_weight = self.compute_pathway_weights(training_step)

        # 2. æ¶æ„çº§ç»•è¿‡ï¼šå½“ç›´é€šæƒé‡ >= 0.9æ—¶ï¼Œå®Œå…¨è·³è¿‡MoEç³»ç»Ÿ
        if bypass_weight >= 0.9:
            # Stage1ç­‰æ•ˆæ¨¡å¼ï¼šåªä½¿ç”¨ç®€å•ç›´é€šè·¯å¾„ï¼Œé¿å…MoEå¤æ‚åº¦
            output = self.simple_direct_pathway(h)
            if self.training:
                # åœ¨æ¶æ„çº§ç»•è¿‡æ¨¡å¼ä¸‹ï¼Œæ¨¡æ‹Ÿä¸“å®¶è¾“å‡ºç”¨äºç»Ÿè®¡
                self._last_direct_output = output.detach().clone()
                self._last_expert_output = output.detach().clone()  # æ¨¡æ‹Ÿç›¸åŒè¾“å‡º
                self._last_bypass_weight = 1.0
                self._last_expert_weight = 0.0
            return output

        # 3. æ··åˆæ¨¡å¼ï¼šè®¡ç®—ä¸¤ä¸ªè·¯å¾„ï¼ˆå½“bypass_weightåœ¨0.1-0.9ä¹‹é—´ï¼‰
        if bypass_weight > 0.1:
            # ç›´é€šè·¯å¾„ï¼šä½¿ç”¨ç®€å•ç‰ˆæœ¬ç¡®ä¿ä¸Stage1ç­‰æ•ˆ
            direct_output = self.simple_direct_pathway(h)  # [B, T, D]

            # ä¸“å®¶ç³»ç»Ÿè·¯å¾„ï¼šå®Œæ•´MoEè®¡ç®—
            expert_output = self.moe_system(h, csi_vec, x_raw, dual_streams)  # [B, T, D]

            # åŠ æƒèåˆ
            mixed_output = bypass_weight * direct_output + expert_weight * expert_output

            # ç¼“å­˜è¾“å‡ºç”¨äºåˆ†ç¦»æŸå¤±è®¡ç®—
            if self.training:
                self._last_direct_output = direct_output.detach().clone()
                self._last_expert_output = expert_output.detach().clone()
                self._last_bypass_weight = bypass_weight
                self._last_expert_weight = expert_weight

            return mixed_output

        # 4. çº¯ä¸“å®¶æ¨¡å¼ï¼šå®Œå…¨ä½¿ç”¨MoEç³»ç»Ÿ
        expert_output = self.moe_system(h, csi_vec, x_raw, dual_streams)
        if self.training:
            self._last_direct_output = None  # ä¸ä½¿ç”¨ç›´é€šè·¯å¾„
            self._last_expert_output = expert_output.detach().clone()
            self._last_bypass_weight = 0.0
            self._last_expert_weight = 1.0

        return expert_output

    def update_performance_ema(self, direct_loss: float, expert_loss: float, alpha: float = 0.99):
        """æ›´æ–°æ€§èƒ½EMAç”¨äºæƒé‡è°ƒæ•´

        Args:
            direct_loss: ç›´æµé€šè·¯æŸå¤±
            expert_loss: ä¸“å®¶ç³»ç»ŸæŸå¤±
            alpha: EMAè¡°å‡ç³»æ•°
        """
        if self.enable_direct_pathway and self.training:
            # å®‰å…¨çš„EMAæ›´æ–°
            if torch.isfinite(self.direct_loss_ema) and self.direct_loss_ema < float('inf'):
                self.direct_loss_ema = alpha * self.direct_loss_ema + (1 - alpha) * direct_loss
            else:
                self.direct_loss_ema = torch.tensor(direct_loss)

            if torch.isfinite(self.expert_loss_ema) and self.expert_loss_ema < float('inf'):
                self.expert_loss_ema = alpha * self.expert_loss_ema + (1 - alpha) * expert_loss
            else:
                self.expert_loss_ema = torch.tensor(expert_loss)

    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ç”¨äºç›‘æ§

        Returns:
            dict: åŒ…å«æƒé‡ã€æŸå¤±EMAã€æ€§èƒ½æ¯”ç‡ç­‰ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {}

        if self.enable_direct_pathway:
            current_bypass = float(self.bypass_weight.data)

            stats.update({
                'bypass_weight': current_bypass,
                'expert_weight': 1.0 - current_bypass,
                'direct_loss_ema': float(self.direct_loss_ema) if torch.isfinite(self.direct_loss_ema) else float('inf'),
                'expert_loss_ema': float(self.expert_loss_ema) if torch.isfinite(self.expert_loss_ema) else float('inf'),
                'training_steps': int(self.training_step_count),
            })

            # æ¶æ„çº§ç»•è¿‡çŠ¶æ€
            if current_bypass >= 0.9:
                stats['pathway_mode'] = 'architectural_bypass'
                stats['stage1_equivalent'] = True
            elif current_bypass > 0.1:
                stats['pathway_mode'] = 'mixed'
                stats['stage1_equivalent'] = False
            else:
                stats['pathway_mode'] = 'pure_expert'
                stats['stage1_equivalent'] = False

            # æ€§èƒ½æ¯”ç‡
            if (torch.isfinite(self.expert_loss_ema) and torch.isfinite(self.direct_loss_ema) and
                self.direct_loss_ema > 0):
                stats['performance_ratio'] = float(self.expert_loss_ema / self.direct_loss_ema)
            else:
                stats['performance_ratio'] = 1.0

            # è®¡ç®—å¤æ‚åº¦ä¼°ç®—ï¼ˆç›¸å¯¹äºStage1ï¼‰
            if stats['pathway_mode'] == 'architectural_bypass':
                stats['complexity_ratio'] = 1.0  # ä¸Stage1ç›¸åŒ
            elif stats['pathway_mode'] == 'mixed':
                stats['complexity_ratio'] = 1.0 + current_bypass * 2.0  # ä¼°ç®—MoEå¼€é”€
            else:
                stats['complexity_ratio'] = 3.0  # çº¯ä¸“å®¶æ¨¡å¼å¼€é”€

        # æ·»åŠ ä¸“å®¶ç³»ç»Ÿç»Ÿè®¡
        try:
            expert_util = self.moe_system.get_expert_utilization()
            stats['expert_utilization'] = [float(u) for u in expert_util]
        except:
            stats['expert_utilization'] = [0.0] * getattr(self.moe_system, 'n_experts', 3)

        return stats

    def get_separated_outputs(self):
        """è·å–åˆ†ç¦»çš„è¾“å‡ºç”¨äºæŸå¤±è®¡ç®—

        Returns:
            tuple: (direct_output, expert_output) æˆ– (None, None)
        """
        if (self.enable_direct_pathway and hasattr(self, '_last_direct_output') and
            hasattr(self, '_last_expert_output')):
            return self._last_direct_output, self._last_expert_output
        return None, None

    def get_expert_utilization(self):
        """ä»£ç†ä¸“å®¶åˆ©ç”¨ç‡è·å–"""
        return self.moe_system.get_expert_utilization()

    def get_aux_losses(self, h: torch.Tensor, csi_vec: torch.Tensor = None,
                       x_raw: torch.Tensor = None, dual_streams: dict = None) -> dict:
        """è·å–è¾…åŠ©æŸå¤±ï¼ŒåŒ…æ‹¬MoEå¹³è¡¡æŸå¤±ç­‰"""
        return self.moe_system.get_aux_losses(h, csi_vec, x_raw, dual_streams)


if __name__ == "__main__":
    pass
