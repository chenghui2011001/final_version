# -*- coding: utf-8 -*-
"""
Simplified MicroMoE: basic sample-level routing aligned with jiagou.md.

Core Features:
- Sample-level top-k routing (not token-level)
- Simple load balancing with uniform prior
- Expert dropout for regularization
- Compatible with CSI conditioning
"""

from __future__ import annotations
from typing import Dict, Tuple, Optional
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from .utils import global_pool
except ImportError:  # pragma: no cover
    from utils import global_pool


# --- å¼ºåŒ–åŽçš„ RobustLSTMï¼šå‡ºçŽ°éžæ•°ä¹Ÿä¼šå›žé€€åˆ°FP32 ---
class RobustLSTM(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.lstm = nn.LSTM(*args, **kwargs)
        self.force_fp32 = False

    def forward(self, x, *args, **kwargs):
        x = x.contiguous()
        orig = x.dtype

        def _run(inp):
            out, hid = self.lstm(inp, *args, **kwargs)
            return out, hid

        try:
            if self.force_fp32 and orig in (torch.bfloat16, torch.float16):
                out, hid = _run(x.float())
            else:
                out, hid = _run(x)
            # å…³é”®ï¼šäº§å‡ºåŽæ•°å€¼è‡ªæ£€
            if torch.isnan(out).any() or torch.isinf(out).any():
                raise RuntimeError("non-finite in LSTM output")
        except RuntimeError:
            if orig in (torch.bfloat16, torch.float16):
                self.force_fp32 = True
                out, hid = _run(x.float())
            else:
                raise

            # å†åšä¸€æ¬¡è‡ªæ£€ï¼ˆæžç«¯ä¿é™©ï¼‰
            if torch.isnan(out).any() or torch.isinf(out).any():
                out = torch.nan_to_num(out, nan=0.0, posinf=1e4, neginf=-1e4)

        # å›žåˆ°åŽŸdtype
        if isinstance(hid, tuple):
            hid = tuple(h.to(orig) for h in hid)
        else:
            hid = hid.to(orig)
        return out.to(orig), hid


# --- æ–°å¢ž RobustGRUï¼šåŒæ ·çš„ç­–ç•¥ ---
class RobustGRU(nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.gru = nn.GRU(*args, **kwargs)
        self.force_fp32 = False

    def forward(self, x, *args, **kwargs):
        x = x.contiguous()
        orig = x.dtype

        def _run(inp):
            out, hid = self.gru(inp, *args, **kwargs)
            return out, hid

        try:
            if self.force_fp32 and orig in (torch.bfloat16, torch.float16):
                out, hid = _run(x.float())
            else:
                out, hid = _run(x)
            if torch.isnan(out).any() or torch.isinf(out).any():
                raise RuntimeError("non-finite in GRU output")
        except RuntimeError:
            if orig in (torch.bfloat16, torch.float16):
                self.force_fp32 = True
                out, hid = _run(x.float())
            else:
                raise
            if torch.isnan(out).any() or torch.isinf(out).any():
                out = torch.nan_to_num(out, nan=0.0, posinf=1e4, neginf=-1e4)

        if isinstance(hid, tuple):
            hid = tuple(h.to(orig) for h in hid)
        else:
            hid = hid.to(orig)
        return out.to(orig), hid


class AcousticFeatureExtractor(nn.Module):
    """ä¸‰å±‚ç‰¹å¾æå–å™¨ - åˆ©ç”¨åŽŸå§‹å£°å­¦ã€Ribbonè¯­ä¹‰ã€Threadå¾®éŸ³æ®µç‰¹å¾"""
    def __init__(self, d_raw: int = 36, d_model: int = 128, feature_dim: int = 64, n_experts: int = 4):
        super().__init__()
        self.d_raw = d_raw
        self.d_model = d_model
        self.feature_dim = feature_dim
        self.n_experts = n_experts  # åŠ¨æ€ä¸“å®¶æ•°é‡

        # åŽŸå§‹å£°å­¦ç‰¹å¾åˆ†æžå™¨ - ç›´æŽ¥ä»Ž36ç»´ç‰¹å¾æå–å£°å­¦ä¿¡æ¯
        # E1: Harmonic - F0è½¨è¿¹åˆ†æž (å‡è®¾ç‰¹å¾ç»´åº¦åŒ…å«F0ç›¸å…³ä¿¡æ¯)
        self.raw_f0_extractor = nn.Conv1d(d_raw, 16, kernel_size=7, padding=3)  # åŽŸå§‹F0ç‰¹å¾
        self.raw_pitch_tracker = RobustLSTM(d_raw, 8, batch_first=True, bidirectional=True)  # F0è½¨è¿¹è·Ÿè¸ª

        # E2: Transient - é«˜é¢‘çž¬æ€åˆ†æž
        self.raw_transient_detector = nn.Conv1d(d_raw, 16, kernel_size=3, padding=1)  # çŸ­æ—¶çž¬æ€
        self.raw_energy_analyzer = nn.Conv1d(d_raw, 16, kernel_size=5, padding=2)  # èƒ½é‡åˆ†å¸ƒ

        # E3: Burst-Inpaint - è¿žç»­æ€§åˆ†æž
        self.raw_continuity_check = nn.Conv1d(d_raw, 16, kernel_size=9, padding=4, dilation=2)  # è¿žç»­æ€§
        self.raw_gap_detector = RobustGRU(d_raw, 8, batch_first=True, bidirectional=True) # ç¼ºå¤±æ£€æµ‹

        # E4: Low-SNR - ä¿¡å·è´¨é‡åˆ†æž
        self.raw_snr_estimator = nn.Sequential(
            nn.Linear(d_raw, d_raw//2),
            nn.ReLU(),
            nn.Linear(d_raw//2, 16)
        )
        self.raw_noise_profiler = nn.Conv1d(d_raw, 16, kernel_size=11, padding=5)  # å™ªå£°è½®å»“

        # ç¼–ç ç‰¹å¾åˆ†æžå™¨ - ä»Ž128ç»´è¯­ä¹‰ç‰¹å¾è¡¥å……é«˜å±‚ä¿¡æ¯
        # E1: Harmonic - è°æ³¢è¯­ä¹‰ç†è§£
        self.encoded_harmonic_semantic = nn.Conv1d(d_model, 8, kernel_size=7, padding=3)

        # E2: Transient - çž¬æ€è¯­ä¹‰ç†è§£
        self.encoded_transient_semantic = nn.Conv1d(d_model, 8, kernel_size=3, padding=1)

        # E3: Burst-Inpaint - ä¸Šä¸‹æ–‡è¯­ä¹‰ç†è§£
        self.encoded_context_semantic = nn.Conv1d(d_model, 8, kernel_size=11, padding=5)

        # E4: Low-SNR - è¯­ä¹‰ç¨³å®šæ€§ç†è§£
        self.encoded_stability_semantic = nn.Conv1d(d_model, 8, kernel_size=9, padding=4)

        # åŒå±‚ç‰¹å¾èžåˆï¼šåŽŸå§‹å£°å­¦(16) + ç¼–ç è¯­ä¹‰(8) = 24 per expert - åŠ¨æ€æ•°é‡
        self.expert_fusion = nn.ModuleList([
            nn.Linear(24, 16) for _ in range(n_experts)  # åŠ¨æ€ä¸“å®¶èžåˆå™¨
        ])

        # æœ€ç»ˆç‰¹å¾èžåˆ - åŠ¨æ€è¾“å…¥ç»´åº¦
        expert_feature_dim = n_experts * 16  # ä¸“å®¶æ•°é‡ * 16ç‰¹å¾
        self.feature_fusion = nn.Linear(expert_feature_dim, feature_dim)

    def forward(
        self,
        x_raw: torch.Tensor,
        ribbon_stream: torch.Tensor,
        thread_stream: torch.Tensor,
        fused_stream: torch.Tensor = None
    ) -> torch.Tensor:
        """
        ä¸‰å±‚ç‰¹å¾æå–ï¼šåŽŸå§‹å£°å­¦ + Ribbonè¯­ä¹‰å¸¦ + Threadå¾®éŸ³æ®µå¸¦

        Args:
            x_raw: [B, T, 36] åŽŸå§‹è¾“å…¥ç‰¹å¾
            ribbon_stream: [B, T, 128] Ribboné•¿è¯­ä¹‰å¸¦ (3xä¸‹é‡‡æ ·å¤„ç†)
            thread_stream: [B, T, 128] Threadå¾®éŸ³æ®µå¸¦ (åŽŸåˆ†è¾¨çŽ‡å¤„ç†)
            fused_stream: [B, T, 128] èžåˆç‰¹å¾ (å¯é€‰)
        Returns:
            features: [B, feature_dim] ä¸‰å±‚èžåˆçš„ä¸“å®¶åå¥½ç‰¹å¾
        """
        b, t, _ = x_raw.shape

        # è½¬ç½®ä¸ºconv1dæ ¼å¼
        x_raw_conv = x_raw.transpose(1, 2)  # [B, 36, T]
        ribbon_conv = ribbon_stream.transpose(1, 2)  # [B, 128, T]
        thread_conv = thread_stream.transpose(1, 2)  # [B, 128, T]

        expert_features = []

        # E1: Harmonic Expert - è°æ³¢åˆ†æž
        # å±‚1: åŽŸå§‹å£°å­¦F0è½¨è¿¹åˆ†æž
        raw_f0 = self.raw_f0_extractor(x_raw_conv)  # [B, 16, T]
        # ä½¿ç”¨æ›´å®‰å…¨çš„ç»Ÿè®¡è®¡ç®—é¿å…fp16ä¸‹çš„NaN
        with torch.no_grad():
            raw_f0_safe = torch.clamp(raw_f0.float(), min=-100.0, max=100.0)
            f0_var = raw_f0_safe.var(dim=-1, unbiased=False, keepdim=False)
            f0_stability = torch.clamp(f0_var + 1e-6, min=1e-6, max=100.0).to(raw_f0.dtype)  # [B, 16] F0ç¨³å®šæ€§

        # RobustLSTMä¼šè‡ªåŠ¨å¤„ç†tensorè¿žç»­æ€§å’Œdtypeå…¼å®¹æ€§
        pitch_out, _ = self.raw_pitch_tracker(x_raw)  # [B, T, 16]

        # æ£€æŸ¥LSTMè¾“å‡ºæ˜¯å¦æœ‰NaN
        if torch.isnan(pitch_out).any():
            print(f"âš ï¸ pitch_out has NaN! Input range: [{x_raw.min():.3f}, {x_raw.max():.3f}]")
            pitch_out = torch.where(torch.isnan(pitch_out), torch.zeros_like(pitch_out), pitch_out)

        with torch.no_grad():
            pitch_safe = torch.clamp(pitch_out.float(), min=-100.0, max=100.0)
            pitch_std = pitch_safe.std(dim=1, unbiased=False, keepdim=False)
            pitch_continuity = torch.clamp(pitch_std + 1e-6, min=1e-6, max=100.0).to(pitch_out.dtype)  # [B, 16] éŸ³è°ƒè¿žç»­æ€§

        # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - è°æ³¢çš„è¯­è¨€å­¦æ„ä¹‰(éŸ³èŠ‚/è¯æ±‡çº§åˆ«çš„F0æ¨¡å¼)
        ribbon_harmonic = self.encoded_harmonic_semantic(ribbon_conv)  # [B, 8, T]
        ribbon_harmonic_pooled = ribbon_harmonic.mean(dim=-1)  # [B, 8] é•¿æœŸè°æ³¢æ¨¡å¼

        # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - ç»†ç²’åº¦F0è°ƒåˆ¶å’Œè°æ³¢å¾®ç»“æž„
        # ä½¿ç”¨Threadæµåˆ†æžçŸ­æ—¶F0å˜åŒ–å’Œè°æ³¢ç»†èŠ‚
        thread_f0_micro = thread_stream.mean(dim=1)[:, :8]  # [B, 8] ç®€åŒ–æå–è°æ³¢å¾®ç»“æž„

        # ä¸‰å±‚èžåˆ: åŽŸå§‹F0(16) + Ribbonè¯­ä¹‰F0(8) + Threadå¾®F0(8) = 32 â†’ 16
        # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ˜¯å¦æœ‰NaN
        components = [f0_stability, pitch_continuity, ribbon_harmonic_pooled, thread_f0_micro]
        for i, comp in enumerate(components):
            if torch.isnan(comp).any():
                print(f"âš ï¸ Component {i} has NaN: {comp.shape}, range=[{comp.min():.3f}, {comp.max():.3f}]")
                components[i] = torch.where(torch.isnan(comp), torch.zeros_like(comp), comp)

        harmonic_triple = torch.cat(components, dim=-1)  # [B, 32]
        # å®‰å…¨æˆªæ–­ + åŽ»éžæ•°
        triple_safe = torch.nan_to_num(harmonic_triple[:, :24], nan=0.0, posinf=1e4, neginf=-1e4)
        if torch.isnan(triple_safe).any():
            print(f"âš ï¸ triple_safe has NaN before fusion!")
            triple_safe = torch.where(torch.isnan(triple_safe), torch.zeros_like(triple_safe), triple_safe)

        harmonic_fused = self.expert_fusion[0](triple_safe)  # [B, 16]

        # æ£€æŸ¥èžåˆç»“æžœ
        if torch.isnan(harmonic_fused).any():
            print(f"âš ï¸ harmonic_fused has NaN after fusion!")
            harmonic_fused = torch.where(torch.isnan(harmonic_fused), torch.zeros_like(harmonic_fused), harmonic_fused)

        expert_features.append(harmonic_fused)

        # E2: Transient Expert - çž¬æ€åˆ†æž
        # å±‚1: åŽŸå§‹å£°å­¦çž¬æ€æ£€æµ‹
        raw_transient = self.raw_transient_detector(x_raw_conv)  # [B, 16, T]
        with torch.no_grad():
            transient_safe = torch.clamp(raw_transient.float(), min=-100.0, max=100.0)
            transient_std = transient_safe.std(dim=-1, unbiased=False, keepdim=False)
            transient_intensity = torch.clamp(transient_std + 1e-6, min=1e-6, max=100.0).to(raw_transient.dtype)  # [B, 16] çž¬æ€å¼ºåº¦

        raw_energy = self.raw_energy_analyzer(x_raw_conv)  # [B, 16, T]
        with torch.no_grad():
            energy_safe = torch.clamp(raw_energy.float(), min=-100.0, max=100.0)
            energy_var = energy_safe.var(dim=-1, unbiased=False, keepdim=False)
            energy_variance = torch.clamp(energy_var + 1e-6, min=1e-6, max=100.0).to(raw_energy.dtype)  # [B, 16] èƒ½é‡å˜åŒ–

        # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - éŸ³ç´ çº§çž¬æ€æ¨¡å¼(çˆ†ç ´éŸ³vsæ‘©æ“¦éŸ³çš„è¯­è¨€å­¦åˆ†ç±»)
        ribbon_transient = self.encoded_transient_semantic(ribbon_conv)  # [B, 8, T]
        with torch.no_grad():
            ribbon_trans_safe = torch.clamp(ribbon_transient.float(), min=-100.0, max=100.0)
            ribbon_trans_std = ribbon_trans_safe.std(dim=-1, unbiased=False, keepdim=False)
            ribbon_transient_pooled = torch.clamp(ribbon_trans_std + 1e-6, min=1e-6, max=100.0).to(ribbon_transient.dtype)  # [B, 8] è¯­è¨€å­¦çž¬æ€æ¨¡å¼

        # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - æœ€é€‚åˆçž¬æ€ï¼ä¿æŒåŽŸåˆ†è¾¨çŽ‡æ•èŽ·çŸ­æ—¶å†²å‡»
        with torch.no_grad():
            thread_safe = torch.clamp(thread_stream.float(), min=-100.0, max=100.0)
            thread_std = thread_safe.std(dim=1, unbiased=False, keepdim=False)[:, :8]
            thread_transient_micro = torch.clamp(thread_std + 1e-6, min=1e-6, max=100.0).to(thread_stream.dtype)  # [B, 8] Threadå¤©ç„¶é€‚åˆçž¬æ€

        # ä¸‰å±‚èžåˆ: åŽŸå§‹çž¬æ€(16) + Ribbonè¯­ä¹‰çž¬æ€(8) + Threadå¾®çž¬æ€(8) = 32 â†’ 16
        transient_components = [transient_intensity, energy_variance, ribbon_transient_pooled, thread_transient_micro]
        for i, comp in enumerate(transient_components):
            if torch.isnan(comp).any():
                print(f"âš ï¸ Transient component {i} has NaN")
                transient_components[i] = torch.where(torch.isnan(comp), torch.zeros_like(comp), comp)

        transient_triple = torch.cat(transient_components, dim=-1)
        transient_safe = transient_triple[:, :24]
        transient_safe = torch.nan_to_num(transient_safe, nan=0.0, posinf=1e4, neginf=-1e4)
        if torch.isnan(transient_safe).any():
            transient_safe = torch.where(torch.isnan(transient_safe), torch.zeros_like(transient_safe), transient_safe)

        transient_fused = self.expert_fusion[1](transient_safe)  # [B, 16]
        if torch.isnan(transient_fused).any():
            print(f"âš ï¸ transient_fused has NaN after fusion!")
            transient_fused = torch.where(torch.isnan(transient_fused), torch.zeros_like(transient_fused), transient_fused)

        expert_features.append(transient_fused)

        # E3: Burst-Inpaint Expert - è¿žç»­æ€§å’Œä¿®å¤åˆ†æž
        # å±‚1: åŽŸå§‹å£°å­¦è¿žç»­æ€§æ£€æŸ¥
        raw_continuity = self.raw_continuity_check(x_raw_conv)  # [B, 16, T]
        if t > 1:
            continuity_breaks = torch.diff(raw_continuity, dim=-1).abs().mean(dim=-1)  # [B, 16]
        else:
            continuity_breaks = torch.zeros(b, 16, device=x_raw.device)

        gap_out, _ = self.raw_gap_detector(x_raw)  # [B, T, 16]
        gap_pattern = torch.clamp(gap_out.float().std(dim=1, unbiased=False) + 1e-6, min=1e-6, max=100.0).to(gap_out.dtype)  # [B, 16] ç¼ºå¤±æ¨¡å¼

        # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - é•¿ç¨‹ä¸Šä¸‹æ–‡ä¾èµ–(å¥å­çº§ä¿®å¤è¯­ä¹‰)
        ribbon_context = self.encoded_context_semantic(ribbon_conv)  # [B, 8, T]
        ribbon_context_pooled = ribbon_context.mean(dim=-1)  # [B, 8] é•¿ç¨‹è¯­ä¹‰ä¸Šä¸‹æ–‡

        # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - å±€éƒ¨é‚»è¿‘ä¿®å¤(éŸ³ç´ é—´è¿‡æ¸¡)
        thread_local_context = torch.clamp(thread_stream.float().var(dim=1, unbiased=False)[:, :8] + 1e-6, min=1e-6, max=100.0).to(thread_stream.dtype)  # [B, 8] å±€éƒ¨å˜å¼‚ç”¨äºŽæ£€æµ‹ç¼ºå¤±

        # ä¸‰å±‚èžåˆ: åŽŸå§‹è¿žç»­æ€§(16) + Ribboné•¿ç¨‹(8) + Threadå±€éƒ¨(8) = 32 â†’ 16
        inpaint_triple = torch.cat([continuity_breaks, gap_pattern, ribbon_context_pooled, thread_local_context], dim=-1)
        inpaint_safe   = torch.nan_to_num(inpaint_triple[:, :24], nan=0.0, posinf=1e4, neginf=-1e4)
        inpaint_fused  = self.expert_fusion[2](inpaint_safe)  # [B, 16]
        expert_features.append(inpaint_fused)

        # E4: Low-SNR Expert - ä¿¡å·è´¨é‡å’Œç¨³å®šæ€§åˆ†æž (ä»…å½“n_experts=4æ—¶)
        if self.n_experts >= 4:
            # å±‚1: åŽŸå§‹å£°å­¦SNRä¼°è®¡
            x_raw_mean = x_raw.mean(dim=1)  # [B, 36]
            raw_snr = self.raw_snr_estimator(x_raw_mean)  # [B, 16]

            raw_noise = self.raw_noise_profiler(x_raw_conv)  # [B, 16, T]
            noise_profile = torch.clamp(raw_noise.float().std(dim=-1, unbiased=False) + 1e-6, min=1e-6, max=100.0).to(raw_noise.dtype)  # [B, 16] å™ªå£°ç‰¹å¾

            # å±‚2: Ribboné•¿è¯­ä¹‰å¸¦ - è¯­ä¹‰ç¨³å®šæ€§(è¯æ±‡/å¥å­çº§ä¸€è‡´æ€§)
            ribbon_stability = self.encoded_stability_semantic(ribbon_conv)  # [B, 8, T]
            ribbon_stability_pooled = torch.clamp(ribbon_stability.float().var(dim=-1, unbiased=False) + 1e-6, min=1e-6, max=100.0).to(ribbon_stability.dtype)  # [B, 8] é•¿æœŸè¯­ä¹‰ç¨³å®šæ€§

            # å±‚3: Threadå¾®éŸ³æ®µå¸¦ - ä¿¡å·è´¨é‡å¾®åˆ†æž(éŸ³ç´ çº§SNR)
            thread_signal_quality = thread_stream.mean(dim=1)[:, 8:16]  # [B, 8] Threadå¾®ä¿¡å·è´¨é‡

            # ä¸‰å±‚èžåˆ: åŽŸå§‹SNR(16) + Ribbonè¯­ä¹‰ç¨³å®š(8) + Threadè´¨é‡(8) = 32 â†’ 16
            lowsnr_triple = torch.cat([raw_snr, noise_profile, ribbon_stability_pooled, thread_signal_quality], dim=-1)
            lowsnr_fused = self.expert_fusion[3](lowsnr_triple[:, :24])  # [B, 16]
            expert_features.append(lowsnr_fused)

        # ç»„åˆæ‰€æœ‰ä¸“å®¶ç‰¹å¾ (åŠ¨æ€é•¿åº¦)
        all_expert_features = torch.cat(expert_features, dim=-1)  # [B, n_experts*16]
        all_expert_features = torch.nan_to_num(all_expert_features, nan=0.0, posinf=1e4, neginf=-1e4)
        # æœ€ç»ˆç‰¹å¾èžåˆ
        acoustic_features = self.feature_fusion(all_expert_features)  # [B, feature_dim]

        return acoustic_features


class HarmonicExpert(nn.Module):
    """E1: è°æ³¢ä¸“å®¶ - ç®€åŒ–ä½†åŠŸèƒ½æ˜Žç¡®çš„è°æ³¢å¤„ç†"""
    def __init__(self, d_model: int = 128):
        super().__init__()

        # è°æ³¢å»ºæ¨¡ï¼šé•¿å·ç§¯æ•èŽ·ä½Žé¢‘è°æ³¢ç»“æž„
        self.harmonic_conv = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3)

        # F0å¹³æ»‘ï¼šé¿å…åŸºé¢‘è·³è·ƒ
        self.f0_smooth = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Tanh(),  # é™åˆ¶èŒƒå›´ï¼Œå¹³æ»‘F0
            nn.Linear(d_model, d_model)
        )

        # æµŠéŸ³è¿žç»­æ€§ï¼šGRUå»ºæ¨¡æ—¶åºä¾èµ–
        self.voicing_gru = nn.GRU(d_model, d_model//2, batch_first=True, bidirectional=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, t, d = x.shape

        # 1. è°æ³¢å·ç§¯
        x_conv = x.transpose(1, 2)  # [B, D, T]
        harmonic_feat = self.harmonic_conv(x_conv).transpose(1, 2)  # [B, T, D]

        # 2. F0å¹³æ»‘
        f0_smooth = self.f0_smooth(harmonic_feat)

        # 3. æµŠéŸ³è¿žç»­æ€§
        voiced_feat, _ = self.voicing_gru(f0_smooth)  # [B, T, D]

        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œè£å‰ª
        voiced_feat = torch.clamp(voiced_feat, min=-10.0, max=10.0)

        return voiced_feat + x


class TransientExpert(nn.Module):
    """E2: çž¬æ€ä¸“å®¶ - ç®€åŒ–ä½†æœ‰æ•ˆçš„çž¬æ€å¤„ç†"""
    def __init__(self, d_model: int = 128):
        super().__init__()

        # çž¬æ€æ£€æµ‹ï¼šçŸ­å·ç§¯ï¼ˆæ·±åº¦å¯åˆ†ç¦» + è½»åº¦ç©ºæ´žï¼‰æ•èŽ·å¿«é€Ÿå˜åŒ–ä¸”ç®—åŠ›å‹å¥½
        self.transient_dw = nn.Conv1d(
            d_model, d_model, kernel_size=3, padding=2, dilation=2, groups=d_model
        )
        self.transient_pw = nn.Conv1d(d_model, d_model, kernel_size=1)

        # çªå˜å¢žå¼ºï¼šGLUé—¨æŽ§æå–çž¬æ€è„‰å†²
        self.burst_enhance = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GLU(dim=-1),
            nn.Linear(d_model, d_model)
        )

        # è¾¹ç¼˜ä¿æŠ¤ï¼šé˜²æ­¢çž¬æ€è¢«è¿‡åº¦å¹³æ»‘
        self.edge_gate = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Sigmoid()  # é—¨æŽ§ä¿æŠ¤
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, t, d = x.shape

        # 1. çž¬æ€æ£€æµ‹
        x_conv = x.transpose(1, 2)  # [B, D, T]
        transient_feat = self.transient_pw(self.transient_dw(x_conv)).transpose(1, 2)  # [B, T, D]

        # 2. çªå˜å¢žå¼º
        burst_enhanced = self.burst_enhance(transient_feat)

        # 3. è¾¹ç¼˜ä¿æŠ¤
        gate_weights = self.edge_gate(burst_enhanced)
        protected = burst_enhanced * gate_weights

        return protected + x


class BurstInpaintExpert(nn.Module):
    """E3: ç¼ºå¤±ä¿®å¤ä¸“å®¶ - ç®€åŒ–ä½†æœ‰æ•ˆçš„ä¿®å¤å¤„ç†"""
    def __init__(self, d_model: int = 128):
        super().__init__()

        # ä¸Šä¸‹æ–‡æ•èŽ·ï¼šæ‰©å¼ å·ç§¯èŽ·å–é•¿ç¨‹ä¾èµ–
        self.context_conv = nn.Conv1d(d_model, d_model, kernel_size=5, padding=4, dilation=2)

        # åŒå‘å»ºæ¨¡ï¼šLSTMèŽ·å–å‰åŽä¸Šä¸‹æ–‡
        self.context_lstm = RobustLSTM(d_model, d_model//2, batch_first=True, bidirectional=True)

        # ä¿®å¤ç½‘ç»œï¼šGLUé€‚åˆç”Ÿæˆä»»åŠ¡
        self.inpaint_net = nn.Sequential(
            nn.Linear(d_model, d_model*2),
            nn.GLU(dim=-1),
            nn.Linear(d_model, d_model)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, t, d = x.shape

        # 1. ä¸Šä¸‹æ–‡æ•èŽ·
        x_conv = x.transpose(1, 2)  # [B, D, T]
        context_feat = self.context_conv(x_conv).transpose(1, 2)  # [B, T, D]

        # 2. åŒå‘å»ºæ¨¡
        bidirectional_feat, _ = self.context_lstm(context_feat)  # [B, T, D]

        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        bidirectional_feat = torch.clamp(bidirectional_feat, min=-10.0, max=10.0)

        # 3. ä¿®å¤å¤„ç†
        inpainted = self.inpaint_net(bidirectional_feat)

        return inpainted + x


class LowSNRExpert(nn.Module):
    """E4: ä½ŽSNRä¸“å®¶ - ç®€åŒ–ä½†æœ‰æ•ˆçš„é™å™ªå’Œè¯­ä¹‰ä¿æŠ¤"""
    def __init__(self, d_model: int = 128):
        super().__init__()

        # é™å™ªï¼šå¹³æ»‘å·ç§¯
        self.denoise_conv = nn.Conv1d(d_model, d_model, kernel_size=5, padding=2)

        # è¯­ä¹‰ä¿æŠ¤ï¼šLayerNorm + GLU
        self.semantic_protect = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model*2),
            nn.GLU(dim=-1),
            nn.Dropout(0.05),  # è½»å¾®dropoutå¢žå¼ºç¨³å®šæ€§
            nn.Linear(d_model, d_model)
        )

        # ç¨³å®šæ€§å¢žå¼ºï¼šLSTMå¹³æ»‘æ—¶åº
        self.stability_lstm = RobustLSTM(d_model, d_model//2, batch_first=True, bidirectional=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, t, d = x.shape

        # 1. é™å™ªå¤„ç†
        x_conv = x.transpose(1, 2)  # [B, D, T]
        denoised = self.denoise_conv(x_conv).transpose(1, 2)  # [B, T, D]

        # 2. è¯­ä¹‰ä¿æŠ¤
        semantic_protected = self.semantic_protect(denoised)

        # 3. ç¨³å®šæ€§å¢žå¼º
        stable_feat, _ = self.stability_lstm(semantic_protected)  # [B, T, D]

        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        stable_feat = torch.clamp(stable_feat, min=-10.0, max=10.0)

        return stable_feat + x


class SpecializedMicroMoE(nn.Module):
    """ä¸“ä¸šåŒ–MicroMoE - åŸºäºŽéŸ³é¢‘å†…å®¹çš„token-levelè·¯ç”±

    Features:
    - 4ä¸ªä¸“ä¸šåŒ–expert: Harmonic/Transient/BurstInpaint/LowSNR
    - Acoustic-aware routeræ›¿ä»£global_pool
    - Token-level routingæ”¯æŒæ—¶åºå†…çš„åŠ¨æ€experté€‰æ‹©
    - CSI integration for Stage3 ablation compatibility
    """
    def __init__(
        self,
        D: int = 128,
        d_csi: int = 10,
        n_experts: int = 4,
        topk: int = 2,
        expert_dropout: float = 0.0,
        balance_weight: float = 0.5,
        router_use_csi: bool = True,
        use_token_level: bool = True,
    ):
        super().__init__()
        self.d_model = D
        self.d_csi = d_csi
        self.n_experts = n_experts
        self.topk = topk
        self.expert_dropout = expert_dropout
        self.balance_weight = balance_weight
        self.router_use_csi = router_use_csi
        self.use_token_level = use_token_level
        self.router_jitter = 0.0  # è®­ç»ƒæ€çš„å°æŠ–åŠ¨å¼ºåº¦ï¼Œå¤–éƒ¨å¯æ³¨å…¥
        # åŒå±‚acoustic feature extractor: åŽŸå§‹å£°å­¦ç‰¹å¾ + ç¼–ç ç‰¹å¾ - ä¼ é€’ä¸“å®¶æ•°é‡
        self.acoustic_extractor = AcousticFeatureExtractor(d_raw=36, d_model=D, feature_dim=64, n_experts=n_experts)

        # Enhanced router: acoustic_features(64) + [csi(10)] -> experts(4)
        acoustic_dim = 64
        router_input_dim = acoustic_dim + (d_csi if router_use_csi else 0)

        if use_token_level:
            # Token-level router: å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥çš„è·¯ç”±
            self.token_router = nn.Sequential(
                nn.Linear(D, 64),  # æ¯ä¸ªtokençš„ç‰¹å¾
                nn.ReLU(),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, n_experts)
            )

        # Sample-level router: åŸºäºŽacoustic features
        hidden_dim = max(32, router_input_dim // 2)
        self.sample_router = nn.Sequential(
            nn.Linear(router_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, n_experts)
        )

        # Routeråˆå§‹åŒ–ï¼šç¡®ä¿å‡åŒ€çš„ä¸“å®¶é€‰æ‹©
        for router in [self.sample_router, self.token_router if use_token_level else None]:
            if router is not None:
                with torch.no_grad():
                    # æœ€åŽä¸€å±‚ï¼ˆè·¯ç”±å±‚ï¼‰çš„åç½®è®¾ä¸ºå°çš„è´Ÿå€¼ï¼Œé¿å…æžç«¯softmax
                    router[-1].bias.fill_(-1.0)  # å°è´Ÿå€¼ç¡®ä¿åˆæœŸå‡åŒ€åˆ†å¸ƒ
                    # è·¯ç”±å±‚æƒé‡ä½¿ç”¨æ›´å°çš„æ–¹å·®
                    nn.init.normal_(router[-1].weight, mean=0.0, std=0.01)

        # ä¸“ä¸šåŒ–Expert networks - æ ¹æ®n_expertsåŠ¨æ€åˆ›å»º
        expert_classes = [
            HarmonicExpert,      # E1: è°æ³¢ä¸“å®¶ (åŸºç¡€ï¼Œæ€»æ˜¯éœ€è¦)
            TransientExpert,     # E2: çž¬æ€ä¸“å®¶ (åŸºç¡€ï¼Œæ€»æ˜¯éœ€è¦)
            BurstInpaintExpert,  # E3: çªå‘ä¿®å¤ä¸“å®¶ (åŸºç¡€ï¼Œæ€»æ˜¯éœ€è¦)
            LowSNRExpert,        # E4: ä½ŽSNRä¸“å®¶ (éœ€è¦CSIï¼ŒStage3è·³è¿‡)
        ]

        self.experts = nn.ModuleList([
            expert_classes[i](D) for i in range(min(n_experts, len(expert_classes)))
        ])

        # Expert utilization tracking
        self.register_buffer('expert_counts', torch.zeros(n_experts))
        self.register_buffer('total_samples', torch.tensor(0.0))

        # ðŸ”§ å‚æ•°åˆå§‹åŒ–ç¨³å®šåŒ– - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        self._init_parameters()

    def _init_parameters(self):
        """å¹³è¡¡çš„å‚æ•°åˆå§‹åŒ–ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸"""
        # å¯¹æ‰€æœ‰ä¸“å®¶ç½‘ç»œåº”ç”¨å¹³è¡¡çš„åˆå§‹åŒ–
        for expert in self.experts:
            for module in expert.modules():
                if isinstance(module, nn.Linear):
                    # ä½¿ç”¨æ ‡å‡†çš„Xavieråˆå§‹åŒ–ï¼Œé€‚åº¦çš„gain
                    nn.init.xavier_uniform_(module.weight, gain=0.5)  # å¹³è¡¡çš„gain
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                elif isinstance(module, (nn.Conv1d, nn.Conv2d)):
                    # å·ç§¯å±‚ä½¿ç”¨Heåˆå§‹åŒ–ï¼ˆé€‚åˆReLUï¼‰
                    nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                elif isinstance(module, (nn.LSTM, nn.GRU)):
                    # RNNæƒé‡ä½¿ç”¨æ ‡å‡†æ­£äº¤åˆå§‹åŒ–
                    for name, param in module.named_parameters():
                        if 'weight' in name:
                            nn.init.orthogonal_(param, gain=1.0)  # æ ‡å‡†gain
                        elif 'bias' in name:
                            # é—å¿˜é—¨åç½®è®¾ä¸º1ï¼Œå…¶ä»–è®¾ä¸º0
                            nn.init.zeros_(param)
                            if 'bias_hh' in name:  # é—å¿˜é—¨åç½®
                                hidden_size = param.size(0) // 3  # GRUæœ‰3ä¸ªé—¨
                                param.data[hidden_size:2*hidden_size].fill_(1.0)

        # å¯¹acoustic feature extractoråº”ç”¨æ ‡å‡†åˆå§‹åŒ–
        for module in self.acoustic_extractor.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, (nn.Conv1d, nn.Conv2d)):
                nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, (nn.LSTM, nn.GRU)):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param, gain=1.0)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
                        if 'bias_hh' in name and 'GRU' in str(type(module)):
                            hidden_size = param.size(0) // 3
                            param.data[hidden_size:2*hidden_size].fill_(1.0)

    def update_expert_usage(self, assignments: torch.Tensor):
        """Update expert usage statistics with numerical safety."""
        with torch.no_grad():  # ç¡®ä¿ç»Ÿè®¡æ›´æ–°ä¸å‚ä¸Žæ¢¯åº¦å›¾
            # assignments: [B, E]
            counts = assignments.sum(dim=0)  # [E]
            total = assignments.sum()

            # EMA update with clamping
            momentum = 0.99
            self.expert_counts = momentum * self.expert_counts + (1 - momentum) * counts.detach()
            self.total_samples = momentum * self.total_samples + (1 - momentum) * total.detach()

    def load_balance_loss(self, gate_logits: torch.Tensor) -> torch.Tensor:
        """Compute load balance loss to encourage uniform expert usage with numerical stability."""
        # å¼ºåˆ¶fp32è®¡ç®—é¿å…bf16ä¸‹çš„æ•°å€¼é—®é¢˜
        gate_logits = gate_logits.float().clamp_(-20, 20)
        probs = F.softmax(gate_logits, dim=-1)  # [B, E]
        mean_probs = probs.mean(dim=0)  # [E]

        # ä½¿ç”¨æ›´ç¨³å®šçš„MSEæŸå¤±æ›¿ä»£KLæ•£åº¦
        uniform = torch.full_like(mean_probs, 1.0 / self.n_experts)
        mse_loss = F.mse_loss(mean_probs, uniform)

        # ç¡®ä¿æŸå¤±ä¸ºæ­£ä¸”æœ‰æ„ä¹‰çš„æ¢¯åº¦
        return mse_loss.clamp(min=1e-8)

    def forward(self, h: torch.Tensor, csi_vec: torch.Tensor = None, x_raw: torch.Tensor = None, dual_streams: dict = None) -> torch.Tensor:
        """
        ä¸“ä¸šåŒ–MoEå‰å‘ä¼ æ’­ - åŸºäºŽä¸‰å±‚ç‰¹å¾çš„æ™ºèƒ½è·¯ç”±

        Args:
            h: [B, T, D] èžåˆåŽç‰¹å¾ (DualStreamè¾“å‡º)
            csi_vec: [B, d_csi] optional CSI vector for Stage3 compatibility
            x_raw: [B, T, 36] åŽŸå§‹è¾“å…¥ç‰¹å¾
            dual_streams: dict containing {'ribbon_stream', 'thread_stream', 'fused_stream'}

        Returns:
            output: [B, T, D] expert-processed features
        """
        b, t, d = h.shape

        # ðŸš¨ ç´§æ€¥è°ƒè¯•æ¨¡å¼ï¼šå®Œå…¨ç»•è¿‡MoEé€»è¾‘ï¼Œä½†ä»è®°å½•ç»Ÿè®¡
        if hasattr(self, '_emergency_bypass') and self._emergency_bypass:
            # åœ¨ç»•è¿‡æ¨¡å¼ä¸‹ï¼Œæ¨¡æ‹Ÿå‡åŒ€çš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
            if self.training:
                with torch.no_grad():
                    # åˆ›å»ºå‡åŒ€åˆ†å¸ƒçš„å‡ä¸“å®¶åˆ†é…ç”¨äºŽç»Ÿè®¡
                    uniform_assignments = torch.ones(b, self.n_experts, device=h.device) / self.n_experts
                    self.update_expert_usage(uniform_assignments)
            return h  # ç›´æŽ¥è¿”å›žè¾“å…¥ï¼Œå®Œå…¨è·³è¿‡MoE

        # æ£€æŸ¥æ˜¯å¦æä¾›å®Œæ•´ç‰¹å¾
        if x_raw is None or dual_streams is None:
            # å…¼å®¹æ¨¡å¼ï¼šç›´æŽ¥è¿”å›žè¾“å…¥ç‰¹å¾ï¼Œé¿å…acoustic_extractorçš„æ½œåœ¨é—®é¢˜
            return h
        else:
            # å°è¯•ä¸‰å±‚ç‰¹å¾æå–ï¼Œä½†æ·»åŠ å¼‚å¸¸æ•èŽ·
            try:
                # 1. ä¸‰å±‚ç‰¹å¾æå–ï¼šåŽŸå§‹å£°å­¦ + Ribbonè¯­ä¹‰ + Threadå¾®éŸ³æ®µ
                ribbon_stream = dual_streams.get('ribbon_stream', h)  # [B,T,128]
                thread_stream = dual_streams.get('thread_stream', h)  # [B,T,128]
                acoustic_features = self.acoustic_extractor(x_raw, ribbon_stream, thread_stream)  # [B, 64]
                acoustic_features = torch.nan_to_num(acoustic_features, nan=0.0, posinf=1e4, neginf=-1e4)
            except Exception as e:
                print(f"Warning: acoustic_extractor failed: {e}, falling back to bypass mode")
                return h

        # 2. Prepare router input
        if self.router_use_csi and csi_vec is not None:
            router_input = torch.cat([acoustic_features, csi_vec], dim=-1)  # [B, 64+d_csi]
        else:
            router_input = acoustic_features  # [B, 64] - Stage3 ablation

        # 3. Sample-level routing based on acoustic preferences
        sample_gate_logits = self.sample_router(router_input)  # [B, E]
        # å¼ºåˆ¶æ•°å€¼ç¨³å®šæ€§ï¼šfp32è·¯ç”±è®¡ç®— + å®‰å…¨å½’ä¸€åŒ–
        sample_gate_logits = sample_gate_logits.float().clamp_(-20, 20)  # fp32 + æˆªæ–­
        # >>> ROUTER EXPLORATION JITTER <<<
        # >>> ROUTER EXPLORATION JITTER (sample-level) <<<
        if self.training and getattr(self, 'router_jitter', 0.0) > 0.0:
            j = float(self.router_jitter)
            sample_gate_logits = sample_gate_logits + j * torch.randn_like(sample_gate_logits)
        # <<< END JITTER <<<

        sample_probs = F.softmax(sample_gate_logits, dim=-1)  # fp32 softmax

        # ä¿å­˜è·¯ç”±ä¿¡æ¯ç”¨äºŽè¯Šæ–­ï¼ˆä¸å‚ä¸Žæ¢¯åº¦è®¡ç®—ï¼‰
        if self.training:
            self._last_router_logits = sample_gate_logits.detach()

        if self.use_token_level:
            # 4. Token-level routing for fine-grained control
            token_gate_logits = self.token_router(h)  # [B, T, E]
            # >>> ROUTER EXPLORATION JITTER (token-level) <<<
            if self.training and getattr(self, 'router_jitter', 0.0) > 0.0:
                j = float(self.router_jitter)
                token_gate_logits = token_gate_logits + j * torch.randn_like(token_gate_logits)
            # <<< END JITTER <<<
            # å¼ºåˆ¶æ•°å€¼ç¨³å®šæ€§ï¼šfp32è·¯ç”±è®¡ç®— + å®‰å…¨å½’ä¸€åŒ–
            token_gate_logits = token_gate_logits.float().clamp_(-20, 20)  # fp32 + æˆªæ–­
            token_probs = F.softmax(token_gate_logits, dim=-1)  # fp32 softmax

            # èžåˆsample-levelå’Œtoken-level routing
            # Sampleåå¥½ä½œä¸ºå…ˆéªŒï¼Œtokenç»†åŒ–å±€éƒ¨å†³ç­–
            sample_probs_expanded = sample_probs.unsqueeze(1).expand(b, t, self.n_experts)  # [B, T, E]
            combined_probs = 0.6 * sample_probs_expanded + 0.4 * token_probs  # [B, T, E]

            # Token-level top-k selection
            weights, indices = torch.topk(combined_probs, k=self.topk, dim=-1)  # [B, T, k]
            weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-6)  # å®‰å…¨å½’ä¸€åŒ–
            weights = weights.to(h.dtype)  # å›žåˆ°åŽŸ dtypeï¼ˆbf16/fp16ï¼‰

            # Create token-level assignment matrix
            assignments = torch.zeros_like(combined_probs)  # [B, T, E]
            assignments.scatter_(-1, indices, weights)  # [B, T, E]

            # Expert dropout
            if self.training and self.expert_dropout > 0:
                dropout_mask = (torch.rand(b, t, self.n_experts, device=h.device) > self.expert_dropout).float()
                assignments = assignments * dropout_mask
                assignments = assignments / (assignments.sum(dim=-1, keepdim=True) + 1e-6)

            # Apply experts with token-level weighting
            expert_outputs = []
            for i, expert in enumerate(self.experts):
                expert_out = expert(h)  # [B, T, D]
                expert_outputs.append(expert_out)

            expert_stack = torch.stack(expert_outputs, dim=-1)  # [B, T, D, E]

            # Token-level weighted combination
            output = torch.einsum('btde,bte->btd', expert_stack, assignments)  # [B, T, D]

            # Usage tracking (sample-level for consistency)
            if self.training:
                sample_assignments = assignments.mean(dim=1)  # [B, E] - average over time
                self.update_expert_usage(sample_assignments.detach())

        else:
            # Sample-level routing (fallback)
            weights, indices = torch.topk(sample_probs, k=self.topk, dim=-1)  # [B, k]
            weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-6)
            weights = weights.to(h.dtype)  # å›žåˆ°åŽŸ dtype

            assignments = torch.zeros_like(sample_probs)  # [B, E]
            assignments.scatter_(1, indices, weights)

            if self.training and self.expert_dropout > 0:
                dropout_mask = (torch.rand(b, self.n_experts, device=h.device) > self.expert_dropout).float()
                assignments = assignments * dropout_mask
                assignments = assignments / (assignments.sum(dim=-1, keepdim=True) + 1e-6)

            # Apply experts
            expert_outputs = []
            for i, expert in enumerate(self.experts):
                expert_out = expert(h)  # [B, T, D]
                expert_outputs.append(expert_out)

            expert_stack = torch.stack(expert_outputs, dim=-1)  # [B, T, D, E]
            output = torch.einsum('btde,be->btd', expert_stack, assignments)  # [B, T, D]

            if self.training:
                self.update_expert_usage(assignments.detach())

        return output

    @torch._dynamo.disable  # ä¸è¿›å…¥ torch.compile å›¾ï¼Œå®‰å…¨ä½¿ç”¨ .item()
    def get_expert_utilization(self) -> torch.Tensor:
        """Expert utilization for logging/metrics only (kept outside compiled graph)."""
        if self.total_samples.item() > 0:
            util = self.expert_counts / (self.total_samples + 1e-8)
        else:
            util = torch.ones_like(self.expert_counts) / self.n_experts
        return util.detach()


    def get_aux_losses(
        self,
        h: torch.Tensor,
        csi_vec: torch.Tensor = None,
        x_raw: torch.Tensor = None,
        dual_streams: Dict[str, torch.Tensor] | None = None,
    ) -> Dict[str, torch.Tensor]:
        aux_losses: Dict[str, torch.Tensor] = {}
        b = h.size(0)

        # 1) å‡†å¤‡è·¯ç”±è¾“å…¥ï¼ˆä¸Ž forward åŒæ­¥ï¼‰
        if x_raw is not None and dual_streams is not None:
            ribbon_stream = dual_streams.get('ribbon_stream', h)
            thread_stream = dual_streams.get('thread_stream', h)
            acoustic_features = self.acoustic_extractor(x_raw, ribbon_stream, thread_stream)  # [B,64]
        else:
            acoustic_features = torch.zeros(b, 64, device=h.device, dtype=h.dtype)

        router_input = torch.cat([acoustic_features, csi_vec], dim=-1) if (self.router_use_csi and csi_vec is not None) else acoustic_features

        # 2) Sample-level å¹³è¡¡æŸå¤±
        sample_gate_logits = self.sample_router(router_input).float().clamp_(-20, 20)  # æ•°å€¼ç¨³å®š
        aux_losses['moe_balance_loss'] = self.load_balance_loss(sample_gate_logits)     # ç»´æŒä¸º tensor

        # 3) Token-level å¹³è¡¡æŸå¤±ï¼ˆè‹¥å¯ç”¨ token routerï¼‰
        if self.use_token_level:
            token_gate_logits = self.token_router(h).float().clamp_(-20, 20)
            e = token_gate_logits.shape[-1]
            token_gate_flat = token_gate_logits.view(-1, e)                              # å…³é”®ï¼šé¿å… UnboundLocalError
            aux_losses['moe_token_balance_loss'] = self.load_balance_loss(token_gate_flat)

        # 4) å¯é€‰çš„åå¥½/ç¨€ç–åº¦æŒ‡æ ‡ï¼ˆä»…åšç›‘æŽ§ï¼Œä¸ç›´æŽ¥å…¥ lossï¼‰
        if x_raw is not None and dual_streams is not None:
            with torch.no_grad():
                aux_losses['moe_harmonic_pref'] = acoustic_features.mean()
        return aux_losses



class CompatibleMicroMoE(nn.Module):
    """å‘åŽå…¼å®¹çš„MicroMoEæŽ¥å£ - åŒ…è£…SpecializedMicroMoE"""
    def __init__(self, d_model: int, n_experts: int = 4, top_k: int = 2, **kwargs):
        super().__init__()
        # æ˜ å°„å‚æ•°åˆ°SpecializedMicroMoE
        specialized_kwargs = {
            'D': d_model,
            'n_experts': n_experts,
            'topk': top_k,
        }
        # ä¼ é€’å…¶ä»–kwargs
        for key, value in kwargs.items():
            if key in ['d_csi', 'expert_dropout', 'balance_weight', 'router_use_csi']:
                specialized_kwargs[key] = value

        self.specialized_moe = SpecializedMicroMoE(**specialized_kwargs)

    def forward(self, h: torch.Tensor, router_input: torch.Tensor, x_raw: torch.Tensor = None, dual_streams: dict = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """å…¼å®¹æŽ¥å£ - è¿”å›žoutputå’Œaux losses"""
        # router_inputçŽ°åœ¨æ˜¯acoustic_features(64)æˆ–acoustic_features(64)+csi(10)
        # ç”±äºŽacoustic_extractorå·²ç»åœ¨ä¸Šå±‚è°ƒç”¨è¿‡ï¼Œè¿™é‡Œåªéœ€è¦è§£æžCSI

        expected_acoustic_dim = 64  # AcousticFeatureExtractorè¾“å‡ºç»´åº¦
        if router_input.shape[-1] > expected_acoustic_dim:
            # åŒ…å«CSIçš„æƒ…å†µ: [acoustic_features(64) + csi(d_csi)]
            csi_dim = router_input.shape[-1] - expected_acoustic_dim
            if csi_dim == self.specialized_moe.d_csi:
                csi_vec = router_input[:, -csi_dim:]  # [B, d_csi]
            else:
                # CSIç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æˆªæ–­æˆ–é›¶å¡«å……
                if csi_dim > self.specialized_moe.d_csi:
                    csi_vec = router_input[:, -self.specialized_moe.d_csi:]  # æˆªæ–­
                else:
                    # é›¶å¡«å……
                    padding = torch.zeros(router_input.shape[0], self.specialized_moe.d_csi - csi_dim,
                                        device=router_input.device, dtype=router_input.dtype)
                    csi_vec = torch.cat([router_input[:, expected_acoustic_dim:], padding], dim=-1)
        elif router_input.shape[-1] == expected_acoustic_dim:
            # çº¯acoustic featuresï¼Œæ— CSI
            csi_vec = None
        else:
            # ç»´åº¦ä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯legacyæ ¼å¼
            csi_vec = None

        output = self.specialized_moe(h, csi_vec, x_raw=x_raw, dual_streams=dual_streams)
        aux_losses = self.specialized_moe.get_aux_losses(h, csi_vec, x_raw=x_raw, dual_streams=dual_streams)
        if 'balance_loss' in aux_losses:               # å…¼å®¹æ—§é”®
            aux_losses['moe_balance_loss'] = aux_losses.pop('balance_loss')
        if 'token_balance_loss' in aux_losses:         # å…¼å®¹æ—§é”®
            aux_losses['moe_token_balance_loss'] = aux_losses.pop('token_balance_loss')
        return output, aux_losses

    def get_expert_utilization(self):
        """å‘åŽå…¼å®¹æ–¹æ³•"""
        return self.specialized_moe.get_expert_utilization()


# ä¿æŒæ—§çš„MicroMoEç±»ä»¥å…¼å®¹å¯èƒ½çš„ç›´æŽ¥å¼•ç”¨
class MicroMoE(nn.Module):
    """Legacy MicroMoE - redirects to SpecializedMicroMoE for consistency"""
    def __init__(self, D: int = 128, d_csi: int = 10, n_experts: int = 4, topk: int = 2, **kwargs):
        super().__init__()
        # ç›´æŽ¥ä½¿ç”¨SpecializedMicroMoEï¼Œä½†ç¦ç”¨token-level routingä»¥ä¿æŒå…¼å®¹æ€§
        kwargs['use_token_level'] = kwargs.get('use_token_level', False)
        self.specialized_moe = SpecializedMicroMoE(
            D=D, d_csi=d_csi, n_experts=n_experts, topk=topk, **kwargs
        )

    def forward(self, h: torch.Tensor, router_input: torch.Tensor) -> torch.Tensor:
        """Legacy interface for basic MoE functionality"""
        # è§£æžrouter_inputä»¥æå–CSI
        if router_input.shape[-1] > self.specialized_moe.d_model:
            csi_dim = router_input.shape[-1] - self.specialized_moe.d_model
            if csi_dim == self.specialized_moe.d_csi:
                csi_vec = router_input[:, -csi_dim:]
            else:
                csi_vec = None
        else:
            csi_vec = None

        return self.specialized_moe(h, csi_vec)

    def get_expert_utilization(self):
        return self.specialized_moe.get_expert_utilization()

    def get_aux_losses(self, h: torch.Tensor, csi_vec: torch.Tensor = None):
        return self.specialized_moe.get_aux_losses(h, csi_vec)


if __name__ == "__main__":
    pass
